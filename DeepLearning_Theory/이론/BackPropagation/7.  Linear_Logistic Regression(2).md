

# Linear Regression
![](../../Data/14.이론_Linear_Logistic_Regression(2)/1.LLR.png)




## Jacobians
![](../../Data/14.이론_Linear_Logistic_Regression(2)/2.LLR.png)



## Backpropagation
![](../../Data/14.이론_Linear_Logistic_Regression(2)/3.LLR.png)



## Parameter Update
![](../../Data/14.이론_Linear_Logistic_Regression(2)/4.LLR.png)



## Backpropagation with Matrices
![](../../Data/14.이론_Linear_Logistic_Regression(2)/5.LLR.png)


# Implementation
``` python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm

np.random.seed(0)
plt.style.use('seaborn')

# set params
N, n_feature = 300, 5
lr = 0.03 
epochs = 30
b_size = 32 # batch size
t_W = np.random.uniform(-1, 1, (n_feature, 1))
t_b = np.random.uniform(-1, 1, (1, 1))

W = np.random.uniform(-1, 1, (n_feature, 1))
b = np.random.uniform(-1, 1, (1, 1))

n_batch = N // b_size 

# generate dataset
x_data = np.random.randn(N, n_feature) # 이미 Transpose 되어 있는 상태 
y_data = x_data @ t_W + t_b # (32 x 1)

J_track = list()
W_track, b_track = list(), list()

for epoch in range(epochs):
    for b_idx in range(n_batch):
        W_track.append(W)
        b_track.append(b)
        
        # get minibatch
        X = x_data[b_idx * b_size  : (b_idx + 1) * b_size, ...] # ... 나머지 열 전부
        Y = y_data[b_idx * b_size  : (b_idx + 1) * b_size, ...]
        
        # forward propagation
        Pred = X @ W + b
        J0 = (Y - Pred)**2
        J = np.mean(J0)
        J_track.append(J)
        
        # jacobians
        dJ_dJ0 = 1 / b_size * np.ones((1, b_size))
        dJ0_dPred = np.diag(-2 * (Y - Pred).flatten())
        dPred_dW = X 
        dPread_dB = np.ones((b_size, 1))
        
        # backpropagation
        dJ_dPred = dJ_dJ0 @ dJ0_dPred
        dJ_dW = dJ_dPred @ dPred_dW
        dJ_db = dJ_dPred @ dPread_dB
        
        # parameter update
        W = W - lr * dJ_dW.T
        b = b - lr * dJ_db
```

``` python
W_track = np.hstack(W_track)
b_track = np.concatenate(b_track)

cmap = cm.get_cmap('tab10', n_feature)
fig, axes = plt.subplots(2, 1, figsize = (20, 15))
axes[0].plot(J_track)

for w_idx, (t_w, w_track) in enumerate(zip(t_W, W_track)):
    axes[1].axhline(y = t_w, linestyle=':', color = cmap(w_idx))
    axes[1].plot(w_track, color = cmap(w_idx))
axes[1].axhline(y = t_b, linestyle = ':', color='black')
axes[1].plot(b_track, color = 'black')
```

![](../../Data/14.이론_Linear_Logistic_Regression(2)/6.LLR.png)






# Logistic Regression


## Model and Tensors
![](../../Data/14.이론_Linear_Logistic_Regression(2)/7.LLR.png)



## Jacobians
![](../../Data/14.이론_Linear_Logistic_Regression(2)/8.LLR.png)



## Backpropagation
![](../../Data/14.이론_Linear_Logistic_Regression(2)/9.LLR.png)




## Parameter Update
![](../../Data/14.이론_Linear_Logistic_Regression(2)/10.LLR.png)




## Backpropagation with Matrices
![](../../Data/14.이론_Linear_Logistic_Regression(2)/11.LLR.png)




``` python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as compile

np.random.seed(3)
plt.style.use('seaborn')

# set parameters
N, n_feature = 5000, 3
lr = 0.01
epochs = 500
b_size = 256

t_W = np.random.uniform(-1, 1, (n_feature, 1))
t_b = np.random.uniform(-1, 1, (1, 1))

W = np.random.uniform(-1, 1, (n_feature, 1))
b = np.random.uniform(-1, 1, (1, 1))

n_batch = N // b_size

# generate dataset
x_data = np.random.uniform(0, 1, (N, n_feature))
y_data = x_data @ t_W + t_b
y_data = (y_data > 0).astype(np.int)

# training
J_track = list()
acc_track = list()

for epoch in range(epochs):
    for b_idx in range(n_batch):
        # get minibatches
        X = x_data[b_idx * b_size : (b_idx + 1) * b_size, ...] # ... 나머지 열 전부
        Y = y_data[b_idx * b_size : (b_idx + 1) * b_size, ...] 
        
        # forward propagation
        # print(X.shape, W.shape, b.shape)
        
        Z = X @ W + b
        Pred = 1 / (1 + np.exp(-Z))
        J0 = -(Y * np.log(Pred) + (1 - Y)*np.log(1 - Pred))
        J = np.mean(J0)
        J_track.append(J)
        
        # calculate accuracy
        Pred_ = (Pred > 0.5).astype(np.int) # 0.5: decision boundary
        # print(Pred_.shape, Y.shape)
        n_correct = np.sum((Pred_ == Y).astype(np.int))
        acc = n_correct / b_size
        acc_track.append(acc)
        
        # jacobians
        dJ_dJ0 = 1 / N * np.ones((1, b_size))
        dJ0_dPred = np.diag(((Pred - Y) / (Pred * (1 - Pred))).flatten())
        dPred_dZ = np.diag((Pred * (1-Pred)).flatten())
        dZ_dW = X
        dZ_db = np.ones((b_size, 1))
        
        # print(dJ_dJ0.shape)
        # print(dJ0_dPred.shape)
        # print(dPred_dZ.shape)
        # print(dZ_dW.shape)
        # print(dZ_db.shape)
        
        # backpropagation
        dJ_dPred = dJ_dJ0 @ dJ0_dPred
        dJ_dZ = dJ_dPred @ dPred_dZ
        dJ_dW = dJ_dZ @ dZ_dW
        dJ_db = dJ_dZ @ dZ_db
        
        # print(dJ_dW.shape, W.shape)
        # print(dJ_db.shape, b.shape)
        
        # parameter update 
        W = W - lr * dJ_dW.T
        b = b - lr * dJ_db
```

``` python
fig, axes = plt.subplots(2, 1, figsize = (20 ,10))
axes[0].plot(J_track)
axes[1].plot(acc_track)
```

![](../../Data/14.이론_Linear_Logistic_Regression(2)/12.LLR.png)




