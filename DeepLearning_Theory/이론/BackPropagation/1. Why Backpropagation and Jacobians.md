

### Trainable Models and Params
![[backpropagation1.png]]





### Gradient-based Learning
![[backpropagation2.png]]
- + : -방향으로 움직여야 가장 작은 값을 가짐 
- - : +방향으로 움직여야 가장 큰 값을 가짐 
- 값: 얼마 만큼 움직여야 하는지 


#### Update Notation
![[backpropagation3.png]]





### Effectiveness of Gradients 
![[backpropagation4.png]]
$- {dy\over dx}$ 를 그대로 사용하면 발산 




### Learning Rate and Gradient-based Learning
![[backpropagation5.png]]

- learning rate는 Hyperparameter의 일종
### $$x := x - \alpha {dy \over dx}$$


### Descending Without a Map
![[backpropagation6.png]]
- 함수의 최솟값을 구하는 과정
# $$x := x - \alpha f^ \prime(x)$$





### Target of Gradient
![[backpropagation7.png]]
- Loss 함수를 최소화 하기 위해 x를 계속 업데이트

1. 뉴럴네트워크에 있는 값, 즉 Weight와 Bias들은 고차원의 tensor를 가지고 있다.
2. 미분을 구하는 것은 Loss함수 값을 최소로 만들기 위해 x값이 어떻게 변해야 하는지를 구하기 위해 미분을 한다





``` python
import numpy as np
import matplotlib.pyplot as plt

function_x = np.linspace(-3, 3, 100)
function_y = function_x**3

fig, ax = plt.subplots(figsize = (10, 5))
ax.plot(function_x, function_y)

x = 3
y = x**3
lr = 0.01
ax.scatter(x, y, color = 'red', s= 300)

for _ in range(30):
	dy_dx = 3*x**2
	x = x - lr*dy_dx
	y = x**3
	ax.scatter(x, y, color = 'red', s= 100)
```
![[backpropagation14.png]]








### Backpropagation

#### - Chain Rule
![[backpropagation8.png]]

![[backpropagation9.png]]

![[backpropagation10.png]]








#### - Backpropagation
![[backpropagation11.png]]
- 다같이 힘을모아 Loss를 줄이자 







### Derivatives of Scalars
![[backpropagation13.png]]




