


# Linear/Logistic Regression Models

![[1.LLR.png]]

![[2.LLR.png]]




# Linear Regression

## - Forward Propagation and Partial Derivatives 
![[3.LLR.png]]



## - Backpropagation
![[4.LLR.png]]



# Linear Regression (1 Feature)
``` python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm

np.random.seed(1)

# set params
N = 300
lr = 0.01
t_w, t_b = 5, -3
w, b = np.random.uniform(-3, 3, 2) # 최소 -3 ~ 최대 3가지의 임의의 수 2개


# generate dataset
x_data = np.random.randn(N, )
y_data = x_data * t_w + t_b
# y_data += 0.5 * np.random.randn(N) # add Noise


# visualize dataset
# cmap : color map
# lut : look-up-table 의 크기를 100개로 나눠 총 100가지의 색상을 사용
cmap = cm.get_cmap('rainbow', lut = N)
fig, ax = plt.subplots(figsize = (10, 10))
ax.scatter(x_data, y_data)
x_range = np.array([x_data.min(), x_data.max()])
J_track = list()
w_track, b_track = list(), list()


# train model and visualize updated models
for data_idx, (x, y) in enumerate(zip(x_data, y_data)):
	w_track.append(w)
	b_track.append(b)

	# visualize updated model
	y_range = w * x_range + b 
	ax.plot(x_range, y_range, color = cmap(data_idx), alpha = 0.5)

	# forward propagation
	pred = x * w + b
	J = (y - pred)**2
	J_track.append(J)

	# jacobians
	dJ_dpred = -2*(y-pred)
	dpred_dw = x
	dpred_db = 1

	# backpropagation
	dJ_dw = dJ_dpred * dpred_dw
	dJ_db = dJ_dpred * dpred_db

	# parameter update
	# w = w - lr * dJ_dw
	# b = b - lr * dJ_db
	w = w + 2*lr*x*(y-pred)
	b = b + 2*lr*(y-pred)

# visualize result
fig, axes = plt.subplots(2, 1, figsize = (20 ,10))
axes[0].plot(J_track)
axes[1].plot(w_track, color = 'darkred')
axes[1].plot(b_track, color = 'darkblue')

axes[0].set_ylabel('MSE', fontsize = 30)
axes[0].tick_params(labelsize = 20)

axes[1].axhline(y=t_w, color = 'darkred', linestyle = ':')
axes[1].axhline(y=t_b, color = 'darkblue', linestyle = ':')
axes[1].tick_params(labelsize = 20)
```
![[5.LLR.png]]

![[6.LLR.png]]




# Linear Regression(N feature)

``` python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm

np.random.seed(1)

# set params
N, n_feature = 100, 3
lr = 0.03
t_W = np.random.uniform(-3, 3, (n_feature, 1))
t_b = np.random.uniform(-3, 3, (1, ))

W = np.random.uniform(-3, 3, (n_feature, 1))
b = np.random.uniform(-3, 3, (1, 1))

  
# generate dataset
x_data = np.random.randn(N, n_feature)
# print(x_data.shape, t_W.shape, t_b.shape)
# y_data = np.matmul(X, t_W) + t_b
y_data = x_data @ t_W + t_b
# print(x_data.shape, y_data.shape)


J_track = list()
W_track, b_track = list(), list()
for data_idx, (X, y) in enumerate(zip(x_data, y_data)):
	# print(X.shape, y.shape)
	# print(b.shape)
	W_track.append(W)
	b_track.append(b)

	# forward propagation                                 vector -> matrix
	X = X.reshape(1, -1)  # 1행을 새로 만들고 나머지를 열로 만들어라 (3, ) -> (1, 3)
	# print(X.shape, W.shape, b.shape)
	pred = X @ W + b
	# print(y.shape, pred.shape)
	J = (y - pred)**2
	# print(J.squeeze().shape)
	J_track.append(J.squeeze())  
	# matrix -> scalar J.shape = (1, 1): [[89.80910404]]
	#                  J.squeeze() = (): 89.80910404104637


	# jacobians
	dJ_dpred = -2 * (y - pred)
	# dpred_dW = X.T # Transpose
	dpred_dW = X
	dpred_db = 1
	# print(dJ_dpred.shape, dpred_dW.shape, dpred_db)
	
	# backpropagation
	dJ_dW = dJ_dpred * dpred_dW
	dJ_db = dJ_dpred * dpred_db
	# print(dJ_dW.shape, dJ_db.shape)

	# parameter update
	W = W - lr*dJ_dW.T
	b = b - lr*dJ_db.T
	# print(b.shape)
	# print(len(W_track))
	# print(W_track[0].shape)

W_track = np.hstack(W_track) # W_track 내의 모든 배열을 수평으로 연결
# a = np.array([1, 2, 3]) 
# b = np.array([4, 5, 6]) 
# c = np.array([7, 8, 9]) 
# W_track = [a, b, c] result = np.hstack(W_track)
# [1, 2, 3, 4, 5, 6, 7, 8, 9]
	
# print(b_track[0].shape)
b_track = np.concatenate(b_track).flatten()
# flatten은 단일 배열을 1차원으로 변환
# hstack은 여러 배열을 수평방향으로 연결 

  
  

# visualize results
fig, axes = plt.subplots(2, 1, figsize = (20, 10))
axes[0].plot(J_track)
axes[0].set_ylabel('MSE', fontsize = 30)
axes[0].tick_params(labelsize = 20)

cmap = cm.get_cmap('rainbow', lut = n_feature)

for w_idx, (t_w, w) in enumerate(zip(t_W, W_track)):
	axes[1].axhline(y=t_w, color = cmap(w_idx), linestyle = ':')
	axes[1].plot(w, color = cmap(w_idx))
axes[1].axhline(y=t_b, color = 'black', linestyle=':')
axes[1].plot(b_track, color = 'black')
axes[1].tick_params(labelsize = 20)
```
![[7.LLR.png]]




# Logistic Regression

![[8.LLR.png]]
$W: = W + \alpha(y-\hat y) \cdot \vec{x}$
$b:= b + \alpha(y-\hat y)$


![[9.LLR.png]]
$W$ 값이 커질수록 기울기 증가 경계에서 멀어진 만큼이 에러율 
따라서 경계에 최대한 가깝게 가기 위해 기울기 커져야 함



# Sigmoid's Params

![[10.LLR.png]]

$$Z = x \cdot W + b$$

- W : 그래프의 기울기에 영향 
- b  : 그래프의 평행이동에 영향



### Weight의 값이 바뀜에 따른 그래프의 변화 (Bias = 0)

``` python
import numpy as np
import matplotlib.pyplot as plt 
import matplotlib.cm as cm

plt.style.use('seaborn')

x = np.linspace(-3, 3, 100)

n_w = 20
w_list = np.linspace(10, -0.1, n_w)

cmap = cm.get_cmap('rainbow', lut = n_w)

fig, ax = plt.subplots(figsize = (10, 5))
for w_idx, w in enumerate(w_list):
	z = x * w
	sig = 1 / 1 + np.exp(-z)

	ax.plot(x, sig, color = cmap(w_idx))
```
![[11.LLR.png]]



### Bias의 값이 바뀜에 따른 그래프의 변화 (Weight = 1)

``` python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm

x = np.linspace(-10, 10, 100)

n_b = 20
w = 1
b_list = np.linspace(-5, 0, n_b)
cmap = cm.get_cmap('rainbow', lut = n_b)

fig, ax = plt.subplots(figsize = (10 ,5))

for n_idx, b in enumerate(b_list):
	z = x * w + b
	sig = 1 / 1 + np.exp(-z)

	ax.plot(x, sig, color = cmap(b_idx))
```
![[12.LLR.png]]






# Sigmoid Decision Boundary

![[13.LLR.png]]

$-{b \over W}$ = t_b, t_W 를 학습하는 것이 아닌 Decision Boundary를 학습화도록 조절

$\star$ 즉 $-{b \over W}$ = Decision Boundary

문제점 
if $-{b \over W} = 2$ 라면 $-2W = b$ 를 유지해야 함 

$W$ 의 값이 커질 수록 기울기가 급격히 증가하며 오차를 줄이기 위해선 기울기가 급격히 증가해야 만 한다. 

$W$ 는 오차를 줄이기 위해 $\infty$로 발산할 것이고 $b$ 또한 $W$ 의 -2배를 유지해야 하므로 $-\infty$
로 발산한다.

![[14.LLR.png]]



# Logistic Regression(1 Feature)

``` python
import numpy as np
import matplotlib.pyploy as plt 
import mapplotlib.cm as cm 

np.random.seed(1)
plt.style.ues('seaborn')

# set params
N = 2000
lr = 0.03 

t_w = np.random.normal(-3, 3, (1, ))
t_b = np.random.normal(-3, 3, (1, ))
# t_w = 5
# t_b = -3 

w = np.random.normal(-3, 3, (1, ))
b = np.random.normal(-3, 3, (1, ))

# generate dataset 
# x_data = np.random.randn(N, )
# y_data = x_data * t_w + t_b 
# y_data = 1 / (1 + np.exp(-y_data))
# y_data = (y_data > 5).astype(np.int)


# generate dataset 
t_decision_boundary = -t_b / t_w 
x_data = np.random.normal(t_decision_boundary, 1, size = (N, ))
y_data = x_data * t_w + t_b
y_data = (y_data > t_decision_boundary).astype(np.int)

fig, ax = plt.subplots(figsize = (20, 10))
ax.scatter(x_data, y_data)

J_track = list()
w_track, b_track = list(), list()

x_range = np.linspace(x_data.min(), x_data.max(), 100)
cmap = cm.get_cmap('rainbow', lut = N)

for data_idx, (x, y) in enumerate(zip(x_data, y_data)):
	w_track.append(w)
	b_track.append(b)

	# visualize updated model 
	y_range = w * x_range + b
	y_range = 1 / (1 + np.exp(-y_range))
	ax.plot(x_range, y_range, color = cmap(data_idx), 
													alpha = 0.3)

	# forward propagation
	z = x * w + b
	pred = 1 / (1 + np.exp(-z))
	J = - (y * np.log(pred) + (1 - y) * np.log(1 - pred))
	J_track.append(J)


	# jacobians
	dJ_dpred = (pred - y) / (pred * (1 - pred))
	dpred_dz = pred * (1 - pred)
	dz_dw = x
	dz_db = 1


	# backpropagation
	dJ_dz = dJ_dpred * dpred_dz
	dJ_dw = dJ_dz * dz_dw
	dJ_db = dJ_dz * dz_db

	# parameter update 
	w = w - lr * dJ_dw
	b = b - lr * dJ_db


# visualize loss
fig, axes = plt.subplots(2, 1 figsize = (20, 10))
axes[0].plot(J_track)
axes[0].set_ylabel('BCCE', fontsize = 30)
axes[0].tick_params(labelsoze = 30)

axes[1].axhline(y = t_w, color = 'darkred', linestyle = ':')
axes[1].plot(w_track, color = 'darkred')
axes[1].axhline(y = t_b, color = 'darkblue', linestyle = ':')
axes[1].plot(b_track, color = 'darkblue')

axes[1].tick_params(labelsize = 30)
```

![[15.LLR.png]]

![[16.LLR.png]]



``` python
w_track = np.array(w_track)
b_track = np.array(b_track)

db_track = -b_track / w_track
db = -t_b / t_w 

fig, ac = plt.subplots(figsize = (10, 5))
ax.axhline(y = db, color = 'black', linestyle = ':')
ax.plot(db_track, color = 'black')
```

![[17.LLR.png]]







# Logistic Regression (N Feature)

``` python
import numpy as np
import matplotlib.pyplot as plt 
import matplotlib.cm as cm 

plt.style.use('seaborn')
np.random.seed(1)

# set param
N, n_feature = 1000, 3
lr = 0.03
t_W = np.random.uniform(-1, 1, (n_feature, 1))
t_b = np.random.uniform(-1, 1, (1, ))
w = np.random.uniform(-1, 1, (n_feature, 1))
b = np.random.uniform(-1, 1, (1, 1))

# generate dataset
x_data = np.random.randn(N, n_feature)
y_data = x_data @ t_w + t_b
y_data = 1 / (1 + np.exp(-y_data))
y_data = (y_data > 0.5).astype(np.int)

J_track, acc_track = list(), list()
n_correct = 0

for data_idx, (X, y) in enumerate(zip(x_data, y_data)):

	# forward propagation
	z = X @ W + b
	pred = 1 / (1 + np.exp(-z))
	J = - (y * np.log(pred) + (1 - y) * np.log(1 - pred))
	J_track.append(J.squeeze())

	# calculate accumulated accuracy
	pred_ = (pred > 0.5).astype(np.int).squeeze()
	if pred_ == y:
		n_correct += 1
	acc_track.append(n_correct / (data_idx + 1))


	# jacobians
	dJ_dpred = (pred - y) / (pred * (1 - pred))
	dpred_dz = pred * (1 - pred)
	dz_dW = X.reshape(1, -1)
	dz_db = 1

	# backpropagation
	dJ_dz = dJ_dpred * dpred_dz
	dJ_dW = dJ_dz * dz_dW
	dJ_db = dJ_dz * dz_db

	# parameter update 
	W = W - lr * dJ_dW.T
	b = b - lr * dJ_db

# visualize result 
fig, axes = plt.subplots(2, 1, figsize = (20, 10))
axes[0].plot(J_track)
axes[1].plot(acc_track)

axes[0].set_ylabel('BCEE', fontsize = 30)
axes[0].tick_params(labelsize = 20)

axes[1].set_ylabel('Accumulated Accuracy', fontsize = 30)
axes[1].tick_params(labelsize = 20)
```

![[18.LLR.png]]


![[19.LLR.png]]

![[20.LLR.mp4]]










