

# 준비 

- Regression $\in$ R         (대소관계 O)

- Classification $\in$ C     (대소관계 X)


Dataset

|   $x$    |   $y$    |
|:--------:|:--------:|
| 공부시간 | 수학점수 |
|    5     |   100    |
|    4     |    80    |
|   ...    |   ...    |


  차이
$y$   $\leftrightarrow$    $\hat{y}$ (predict)   



실제값과 예측값의 차이를 수치화 하는 도구
- Regression 
	- $\hat{y} \in R$   $\rightarrow$   MSE

- Classification
	- $\hat{y} \in C$   $\rightarrow$   $\begin{cases} binomial\\ multi \end{cases}$




# Cartesian Product for Predictions/Labels

### - R : 실수

### - B : {0, 1}                       ex) {강아지, 고양이}


k개의 object를 구분할 때
### - C = {$c_1$, $c_2$,... , $c_k$ }      ex) {강아지, 고양이, .... 호랑이}  


probability (확률집합)
### - P = {$x|0 \le x \le 1$}


one-hot-vector                                                    전체 합이 1 즉 하나만 1인 집합
### - $B^{n}_1$ = $\lbrace (b_1, b_2, ..., b_n)^T \; | \; \forall b_i \in B, \displaystyle \sum_{i=1}^n b_i = 1\rbrace$ 
one-hot-vector encoding           모든원소는 0 또는 1
강아지 (0)  $\rightarrow$  $001$
고양이 (1)   $\rightarrow$  $010$
토끼    (2)   $\rightarrow$  $100$




# Mean Squared Error

![](1.LossFunction.png)

sample1       공부시간   용돈       ....     거주지역       실제 성적 데이터      실수에 포함
 ## $(\vec{x}^{(1)})$     =    ( $x_1^{(1)}$        $x_2^{(1)}$       ....        $x_{l_I}^{(1)}$ )                  $y^{(1)}$             $\in$         $R$
   $\vdots$                               x data set                               y data set





            squared error
# $J = (y - \hat{y})^2$      $\rightarrow$       두 실수값의 차이

![](2.LossFunction.png)





# Mean Squared Error in Minibatch

# $J = {1 \over N} \displaystyle \sum_{i=1}^N (y^{(i)} - \hat{y}^{(i)})^2$      $\rightarrow$       오차평균

$\hat{y}^{(i)}$ = $wx + b$     최종적으로     $w$,  $b$ 에 대한 2차식  (2차함수에서 $a \gt 0$ 이면 아래로 볼록)

$(y^{(i)} - \hat{y}^{(i)})^2$     $\rightarrow$      $y$ 와 $\hat{y}$ 이 서로 비슷할수록 0에 가까워지고 차이가 커질수록 값이 커진다 

$\therefore$ $loss$ $\geq$ $0$
 
![](3.LossFunction.png)
! 여러개의 data sample이 들어간다고 해서 weight 의 갯수가 바뀌는건 아니다





# Binary Cross Entropy

![](4.LossFunction.png)

samples       공부시간   용돈       ....     거주지역           합/불 (1 / 0)
 ## $(\vec{x}^{(1)})$     =    ( $x_1^{(1)}$        $x_2^{(1)}$       ....        $x_{l_I}^{(1)}$ )                  $y^{(1)}$             $\in$         $B$
 ## $(\vec{x}^{(2)})$     =    ( $x_1^{(2)}$        $x_2^{(2)}$       ....        $x_{l_I}^{(2)}$ )                  $y^{(1)}$             $\in$         $B$
   $\vdots$                               x data set                               y data set

$R$ ~ $R$   $\rightarrow$   MSE
$\hat{y}$  ~ $y$

$B$ ~ $B$   $\rightarrow$   BCE
$\hat{y}$  ~ $y$





# $H_b(y, \hat{y}) = -[ylog(\hat{y}) \; + (1 \; - \; y)log(1 \; - \; \hat{y})]$

![](5.LossFunction.png)

if 
$\begin{cases} y = 1 \Rightarrow -log(\hat{y})\\ y = 0 \Rightarrow -log(1-\hat{y}) \end{cases}$

$y = 0$  일 때  예측값 또한 $\hat{y} \approx 0$ 이면 $L \approx 0$
$y = 0$  일 때  예측값이 $\hat{y} \approx 1$ 이면 $L \rightarrow \infty$




# Binary Cross Entropy in Minibatch

![](6.LossFunction.png)

오차 평균
# $\displaystyle - {1 \over N} \sum_{i=1}^N [y^{(i)}log(\hat{y}^{(i)}) + (1 - y^{(i)})log(1-\hat{y}^{(1)})]$





# Categorical Cross Entropy

![](7.LossFunction.png)

## $C \; = \; \lbrace c_1, \; c_2, \; \dots ,\; c_K \rbrace$
$K$ : 구하고자 하는 클래스 갯수
$C \; = \; \lbrace 0, \; 1, \; \dots ,\; k \rbrace$

 강아지
 $(\vec{x}^{(1)})$     =    ( $x_1^{(1)}$        $x_2^{(1)}$       ....        $x_{l_I}^{(1)}$ )                  $y^{(1)}$             $\in$         $C$
 
 고양이
 $(\vec{x}^{(2)})$     =    ( $x_1^{(2)}$        $x_2^{(2)}$       ....        $x_{l_I}^{(2)}$ )                  $y^{(1)}$             $\in$         $C$
   $\vdots$                               
 
 토끼 
$(\vec{x}^{(N)})$     =    ( $x_1^{(N)}$        $x_2^{(N)}$       ....        $x_{l_I}^{(N)}$ )                $y^{(N)}$             $\in$        $C$




### Dataset for Multi-class Classification+ One-hot Encoding

![](8.LossFunction.png)


![](9.LossFunction.png)

![](10.LossFunction.png)




# Categorical Cross Entropy in Minibatch

![](11.LossFunction.png)









---
# 실습


## python에서 차원의 이해

0.  0차원 스칼라 tf.Tensor(42, shape=(), dtype=int32)
1.  1차원 배열 (벡터): `[1, 2, 3]` - 시작과 끝에 1개의 `[`와 `]`가 있습니다.
2.  2차원 배열 (행렬): `[[1, 2, 3], [4, 5, 6)` - 시작과 끝에 2개의 `[`와 `]`가 있습니다.
3.  3차원 배열 (텐서): `[[[1, 2], [3, 4), [[5, 6], [7, 8)]` - 시작과 끝에 3개의 `[`와 `]`가 있습니다.


차원 배열 생성
``` python
import tensorflow as tf

"""0차원 스칼라"""
scalar = tf.constant(42)

"""1차원 벡터"""
vector_tensor = tf.constant([3, 5, 7, 9])

"""2차원 배열(행렬)"""
matrix = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=tf.int32)

"""3차원 텐서"""
tensor_3d = tf.constant([[[1, 2], [3, 4], [[5, 6], [7, 8]]], dtype=tf.int32)
```


가변 배열 생성
``` python
import tensorflow as tf 

""" 1차원 가변 벡터 """
# (0, ) 는 1차원 벡터 ex) [1, 2, 3 ,4 ] = 크기가 4인 1차원 벡터
empty_vector = tf.zeros(shape=(0,)) # 1차원 벡터(0개의 원소) (차원은 변하지 않음)

# 1차원 가변 벡터에 데이터 추가
new_data = tf.constnat([1, 2, 3])
concatenated_vector = tf.concat([empty_vector, new_data], axis = 0)

""" 2차원 가변 배열 """
# 빈 2차원 가변 배열 생성
empty_tensor = tf.zeros(shape=(0, 3)) # 열의 갯수는 3개 고정

# 2차원 가변 배열에 데이터 추가 
new_data = tf.constant([[1, 2, 3], [4, 5, 6), dtype=tf.float32) concatenated_tensor = tf.concat([empty_tensor, new_data], axis=0)


""" 3차원 가변 배열 """
# 빈 3차원 텐서를 생성 
empty_3d_tensor = tf.zeros(shape=(0, 2, 2), dtype=tf.int32)
# 새로운 데이터 추가 
new_data = tf.constant([ [ [1, 2], [3, 4] ], [ [5, 6], [7, 8] ] ], dtype=tf.int32) 
# 첫 번째 차원에 데이터 추가 
concatenated_3d_tensor = tf.concat([empty_3d_tensor, new_data], axis=0)
"""
즉, `shape=(0, 2, 2)`에서 두 번째 차원(행)과 세 번째 차원(열)의 크기는 고정
첫 번째 차원(레이어)의 크기는 가변적
이를 통해 첫 번째 차원에 새로운 데이터를 추가할 수 있으며, 나머지 차원은 고정된 크기를 유지
"""
```




### Code.4-1-1: Dataset for Regression
``` python
import tensorflow as tf
from tensorflow.keras.layers import Dense

N, n_feature = 8, 5
t_weights = tf.constant([1, 2, 3, 4 ,5], dtype = tf.float32)
t_bias = tf.constant([10], dtype = tf.float32)

x = tf.random.normal(mean = 0, stddev = 1, shape = (N, n_feature))
y = tf.reduce_sum(t_weights * x, axis = 1) + t_bias
print(x)
print(y.shape)

""" 출력
[[-1.3544159 0.7045493 0.03666191 0.86918795 0.43842277] [-0.53439844 -0.07710292 1.5658046 -0.1012345 -0.2744975 ] [ 1.420466 1.2609465 -0.4364091 -1.963399 -0.06452482] [-1.056841 1.0019135 0.6735137 0.06987705 -1.4077919 ] [ 1.0278524 0.2797411 -0.01347954 1.8451811 0.9706112 ] [-1.0242516 -0.6544423 -0.29738778 -1.3240397 0.28785658] [-0.87579006 -0.08856997 0.6921164 0.842157 -0.06378508] [ 0.9280078 -0.6039788 -0.17669262 0.04221032 0.29037958)
"""
```




### Code.4-1-2: Dataset for Binary Classification
``` python
import tensorflow as tf

N, n_feature = 8, 5
t_weights = tf.constant([1, 2, 3 ,4 ,5], dtype = tf.float32)
t_bias = tf.constant([10], dtype = tf.float32)

x = tf.random.normal(mean = 0, stddev = 1, shape = (N, n_feature))
y = tf.reduce_sum(t_weights * x, axis = 1) + t_bais

print(y > 5)
y = tf.cast(y > 5, tf.int32)
print(y)


""" 출력
[ True True False True True False True True]

[1 1 0 1 1 0 1 1]
"""
```




#### Code.4-1-3: Dataset for Multi-class Classification
``` python
import tensorflow as tf
import matplotlib.pyplot as plt

plt.style.use('seaborn-v0_8')
tf.random.set_seed(3)

N, n_feature = 8, 2
n_class = 3

# 0은 가변으로 만드는 것 즉 가변행, 열은 n_feature 고정
X = tf.zeros(shape = (0, n_feature))          # 2차원 배열 ()

# (0, ) 는 1차원 벡터 ex) [1, 2, 3 ,4 ] = 크기가 4인 1차원 벡터
Y = tf.zeros(shape = (0, ), dtype = tf.int32) # 벡터 (0개의 원소)

fig, ax = plt.subplots(figsize = (5, 5))
for class_idx in range(n_class):
	center = tf.random.uniform(minval=-15, maxval=15, shape=(2,))
	
	x1 = center[0] + tf.random.normal(shape = (N, 1)) # x축으로 사용
	x2 = center[1] + tf.random.normal(shape = (N, 1)) # y축으로 사용

	x = tf.concat((x1, x2), axis = 1) # x축과 y축으로 사용하기 위해 concat
	
	# 각 클래스가 몇번쨰 클래스인지 나타내는 변수
	y = class_idx * tf.ones(shape = (N, ), dtype =tf.int32) 

				# x축          # y축
	ax.scatter(x[:,0].numpy(), x[:,1].numpy(), aplha = 0.3)

	X = tf.concat((X,x), axis = 0) # 생성된 x데이터 묶음 (24, 2)
	Y = tf.concat((Y,y), axis = 0) # 생성된 y데이터 묶음 (24, )

print("X(shape/dtype/data): {} \ {}\n{}\n".format(X.shape, X.dtype, X.numpy()))

print("Y(shape/dtype/data): {} \ {}\n{}\n".format(Y.shape, Y.dtype, Y.numpy()))
```





### Code.4-1-4: Dataset for Multi-class Classification with One-hot Encoding

``` python
import tensorflow as tf
import matplotlib.pyplot as plt

plt.style.use('seaborn-v0_8')
tf.random.set_seed(3)

N, n_feature = 8, 2
n_class = 3

X = tf.zeros(shape = (0, n_feature))
Y = tf.zeros(shape = (0, ), dtype=tf.int32)

fix, ax = plt.subplots(figsizse = (5,5))

for class_idx in range(n_class):
	center = tf.random.uniform(minval = -15, maxval =15, shape = (2,))

	x1 = center[0] + tf.random.normal(shape = (N, 1))
	x2 = center[1] + tf.random.normal(shape = (N, 1))

	x = tf.concat((x1, x2), axis = 1)
	y = class_idx * tf.ones(shape = (N, ), dtype = tf.int32)

	ax.scatter(x[:,0].numpy(), ax[:,1].numpy(), alpha = 0.3)

	X = tf.concat((X, x), axis = 0)
	Y = tf.concat((Y, y), axis = 0)


Y = tf.one_hot(Y, depth = n_class, dtype = tf.int32)

print("X(shape/dtype/data): {} \ {}\n{}\n".format(X.shape, X.dtype, X.numpy()))

print("Y(shape/dtype/data): {} \ {}\n{}\n".format(Y.shape, Y.dtype, Y.numpy()))
```




### Code.4-1-5: Dataset Objects
``` python
import tensorflow as tf

N, n_feature = 100, 5
batchsize = 32 # 32개씩 꺼내오겠다 (여기서는 3번 봅아옴)

t_weights = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)
t_bias = tf.constant([10], dtype=tf.float32)
					#평균       표준편차(standard deviation)
x = tf.random.normal(mean = 0, stddev=1, shape=(N, n_feature))
y = tf.reduce_sum(t_weights * x, axis=1) + t_bais

dataset = tf.data.Dataset.from_tensor_slices((x, y))
dataset = dataset.batch(batch_size).shuffle(100)

for x, y in dataset:
	print(x.shape, y.shape)
```





# 4-2: Mean Squared Error


### Code.4-2-1: MSE Calculation
``` python
import tensorflow as tf
from tensorflow.keras.loss import MeanSquaredError

loss_object = MeanSquareError()

batch_size = 32

predictions = tf.random.normal(shape = (batch_size, 1))
labels = tf.random.normal(shape = (batch_size, 1))

mse = loss_object(labels, predictions)

mse_man = tf.reduce_mean(tf.math.pow(labels-predictions, 2))

print("tensorflow: ", mse.numpy())
print("manual: ", mse_man.numpy())
```



### Code.4-2-2: MSE with Model/Dataset
``` python
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import mean_square_error

N, n_feature = 100, 5
batch_size = 32

x = tf.random.normal(shape = (N, n_feature))
y = tf.random.normal(shape = (N, 1))

dataset = tf.dataset.Dataset.from_tensor_slices((x, y))
dataset = dataset.batch(batch_size)

model = Dense(units = 1, activation = 'linear')
loss_object = MeanSquaredError()

for x, y in dataset:
	predictions = model(x)
	loss = loss_object(y, predictions)
	print(loss.numpy())
```




# 4-3: Binary Cross Entropy

### Code. 4-3-1: BCE Calculation
``` python
import tensorflow as tf
from tensorflow.keras.losses import BinaryCrossentropy

batch_size = 4
n_class = 2

predictions = tf.random.uniform(shape = (batch_size, 1),
							   minval = 0, maxval = 1,
							   dtype = tf.float32)

labels = tf.random.uniform(shape = (batch_size, 1),
						  minval = 0, maxval = 1,
						  dtype = tf.int32)

loss_object = BinaryCrossentropy()
loss_tf = loss_object(labels, predictions)

labels = tf.cast(labels, tf.float32)
bce_man = -(labels * tf.math.log(predictions) + (1 - labels) * tf.math.log(1 - predictions))

loss_man = tf.reduce_mean(bce_man)

print("BCE(TF)", loss_tf.numpy())
print("BCE(man)", loss_man.numpy())
```


### Code.4-3-2: BCE with Model/Dataset
``` python
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import BinaryCrossentropy

N, n_feature = 100, 5
batch_size = 30

t_weights = tf.constant([1, 2, 3, 4, 5], dtype = tf.float32)
t_bias = tf.constant([10], dtype = tf.float32)

X = tf.random.normal(mean = 0, stddev = 1, shape = (N, n_feature))
Y = tf.reduce_sum(t_weights * X, axis = 1) + t_bias
Y = tf.cast(Y > 5, tf.int32)

dataset = tf.data.Dataset.from_tensor_slices((X, Y))
dataset = dataset.batch(batch_size)

model = Dense(units = 1, activation = 'sigmoid')
loss_object = BinaryCrossentropy()

for x, y in dataset:
	predictions = model(x)
	loss = loss_object(y, predictions)
	print(loss.numpy())
```




# 4-4: Sparse Categorical Cross Entropy
(one-hot encoding이 안되어 있을 때)

### Code.4-4-1: SCCE Calculation
``` python
import tensorflow as tf
from tensorflow.keras.losses import SparseCategoricalCrossentropy

batch_size, n_class = 16, 5

predictions = tf.random.uniform(shape = (batch_size, n_class),
							   minval = 0, maxval = 1,
							   dtype = tf.float32)

# predictions 만들기 : 각 행의 합은 1이 되도록 만들어야 함
# 따라서 각 행의 합을 구하고 데이터를 행벡터로 만들어 준다
# 그래야 브로트캐스팅을 통해 자동으로 predictions를 나누어 줄 수 있다
# 그럼 최종적으로 합헀을 때 1이 되는 확률 값으로 들어가게 된다
# -1은 뒤에 열을 1로 하고 나머지를 다 행으로 보낸다는 의미 (반대도 가능)
pred_sum = tf.reshape(tf.reduce_sum(predictions, axis=1), (-1, 1)) 
predictions = predictions / pred_sum
print(tf.reduce_sum(predictions, axis = 1)) # 1 다 1나옴

labels = tf.random.uniform(shape = (batch_size, ),
						   minval = 0, maxval = 5,
						   dtype = tf.int32)
				
loss_object = SparseCategoricalCrossentropy()
loss = loss_object(labels, predictions)

ce = 0
for label, predictions in zip(labels, predictions):
	print(label, predictions) 
	#label이 2라면 predictions의 2번째(0부터 시작) 만 log씌워줌
	ce += -tf.math.log(predictions[label]) # 평균낼거라 굳이 앞에 label안붙혀도됨

ce /= batch_size # 평균 구하기

print('loss: ', loss)
print('ce: ', ce.numpy())
```





### Code.4-4-2: SCCE with Model/Dataset
``` python
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import SparseCategoricalCrossentropy

N, n_feature = 100, 2
n_class = 5

X = tf.zeros(shape = (0, n_feature))
Y = tf.zeros(shape = (0, 1), dtype = tf.int32)

for class_idx in range(n_class):
	center = tf.random.uniform(minval = -15, maxval = 15, shape = (2, ))

	x1 = center[0] + tf.random.normal(shape = (N, 1))
	x2 = center[1] + tf.random.normal(shape = (N, 1))

	x = tf.concat((x1, x2), axis = 1)
	y = class_idx * tf.ones(shape = (N, 1), dtype = tf.int32)

	X = tf.concat((X, x), axis = 0)
	Y = tf.concat((Y, y), axis = 0)

dataset = tf.data.Dataset.from_tensor_slices((X, Y))
dataset = dataset.batch(batch_size)

model = Dense(units = n_class, activation = 'softmax')
loss_object = SparseCategoricalCrossentropy()

for x, y in dataset:
	predictions = model(x)
	loss = loss_object(y, predictions)
	print(loss.numpy())
```





# 4-5: Categorical Cross Entropy
one-hot encoding이 되있을 때


### Code.4-5-1: CCE Calculation
``` python
import tensorflow as tf
from tensorflow.keras.losses import CategoricalCrossentropy

batch_size, n_class = 16, 5

predictions = tf.random.uniform(shape = (batch_size, n_class),
							   minval = 0, maxval = 1,
							   dtype = tf.float32)

pred_sum = tf.reshape(tf.reduce_sum(predictions, axis = 1), (-1,1))
predictions = predictions / pred_sum

labels = tf.random.uniform(shape = (batch_size, ),
						  minval = 0, maxval = n_class,
						  dtype = tf.int32)

labels = tf.one_hot(labels, n_class)

loss_object = CategoricalCrossentropy()
loss = loss_object(labels, predictions)

print('CCE(tf): ', loss.numpy())

cce_man = tf.reduce_mean(tf.reduce_sum(-labels * tf.math.log(predictions), axis = 1))

print('CCE(man): ', loss.numpy())
```




### Code.4-5-2: CCE with Model/Dataset
``` python
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import CategoricalCrossentropy

N, n_feature = 8, 2
n_class = 5

X = tf.zeros(shape = (0, n_feature))
Y = tf.zeros(shape = (0, ), dtype = tf.int32)

for class_idx in range(n_class):
	center = tf.random.uniform(minval = -15, maxval = 15, shape = (2,))

	x1 = center[0] + tf.random.normal(shape = (N, 1))
	x2 = center[1] + tf.random.normal(shape = (N, 1))

	x = tf.concat((x1, x2), axis = 1)
	y = class_idx * tf.ones(shape = (N, ), dtype = tf.int32)

	X = tf.concat((X, x), axis = 0)
	Y = tf.concat((Y, y), axis = 0)

Y = tf.one_hot(Y, depth = n_class, dtype = tf.int32)

dataset = tf.data.Dataset.from_tensor_slices((X, Y))
dataset = dataset.batch(batch_size)

model = Dense(units = n_class, activation = 'softmax')
loss_object = CategoricalCrossentropy()

for x, y in dataset:
	predictions = model(x)
	loss = loss_object(y, predictions)
	print(loss.numpy())
```