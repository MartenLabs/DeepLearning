
## 이론
-----

### Odds 

- 확률을 표현하는 또다른 방법 
- 확률이 1.0에 갈수록 발산 
![[1.Sigmoid_Softmax.png]]
### $$O = {p \over 1-p}$$



### Logit

![[2.Sigmoid_Softmax.png]]
### $$-\infty < l = log({p \over 1 - p}) < \infty$$$$= -\infty < l = log(O) < \infty$$

## $l = log({p \over 1-p})$    

## $\to e^l = {p \over 1-p}$ 

## $\to {1 \over e^l} = {1 - p \over p} = {1 \over p} -1$

## $\to {1 \over e^l} + 1 = {1 \over p}$

## $\to {1 + e^l \over e^l} = {1\over p}$

## $\to p = {e^l \over 1 + e^l}$

## $\to p = {1 \over 1 + e^{-l}}$

# $p = \sigma(l) = {1 \over {1 + e^{-l}}}$ 

- Simgoid = 확률을 계산하는 함수

![[3.Sigmoid_Softmax.png]]

- ### Deep Learning은 아무렇게나 만든게 아니다
1. output layer에 affine function의 값을 logit으로 해석해보자
2. Sigmoid를 붙여주면 확률이 되겠네?
3. 그럼 이 확률이 실제 확률에 맞도록 학습을 시켜주면 되겠다! 
4. 라고 Network를 설계

즉 affine function의 값을 logit으로써 해석 => 확률로 바뀜

- 딥러닝 네트워크는 
![[4.Sigmoid_Softmax.png]]
- 확률로 나타내주는 시스템 (classification 에서)





### Sotfmax Layer

- multiclassification 을 위한 layer

#### softmax 특징 
![[5.Sigmoid_Softmax.png]]

### ex) $p_1 = {e^{l_1} \over \sum_{k=1}^{K} [e^{l_k}]}$         $p_2 = {e^{l_2} \over \sum_{k=1}^{K} [e^{l_k}]}$

- input은 logit 즉 k개의 logit vector를 넣어주게 되면 k개의 Probability vector로 바꿔주는 것이 softmax

- 즉 각각의 클래스에  대한 확률을 계산 
	- 마지막 출력 뉴런의 갯수는 구분하는 클래스의 갯수
		- ex) 4개를 구분 => 뉴런 4개






## 실습
----


# 3-1 : The Graphs of Odds, Logit and Sigmoid

### Code. 3-1-1: The Graphs of Odds and Logit

Odds $O = {p \over 1-p}$

Logit   $l = log({p \over 1 - p})$

``` python
"""
Odds 그래프와 Logit 그래프를 pyplot을 이용하여 그리시오
"""
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt 

plt.style.use('seaborn')

# 시작값 0.01 ~ 끝값 0.99 까지 100개의 배열 생성 
# 시작값과 끝값 사이를 동일한 간격으로 나눈 값을 가진 배열 반환
p_np = np.linspace(0.01, 0.99, 100)
p_tf = tf.linspace(0.01, 0.99, 100)

odds_np = p_np / (1 - p_np)
odds_tf = p_tf / (1 - p_tf)

logit_np = np.log(odds_np)
logit_tf = tf.math.log(odds_tf)

fig, axes = plt.subplots(2, 1, figsize = (15, 10),
						sharex = True)

axes[0].plot(p_np, odds_np)
axes[1].plot(p_np, logit_tf.numpy())

xticks = np.arange(0, 1.1, 0.1)
axes[0].tick_params(labelsize = 15)
axes[0].set_xticks(xticks)
axes[0].set_ylabel('Odds', fontsize = 20, color = 'darkblue')

axes[1].tick_params(labelsize = 15)
axes[1].set_xticks(xticks)
axes[1].set_ylabel('Logit', fontsize = 20, color = 'darkblue')

axes[1].set_xlabel('Probability', fontsize = 20, color = 'darkblue')
```
![[6.Sigmoid_Softmax.png]]





### Code. 3-1-2: The Graphs of Sigmoid

``` python
"""
sigmoid 그래프를 pyplot을 사용하여 그리시오
"""
import tensorflow as tf
from tensorflow.keras.layers import Activation

X = tf.linspace(-10, 10, 100)
sigmoid = Activation('sigmoid')(X)

fig, ax = plt.subplots(figsize = (10, 5))
ax.plot(X.numpy(), sigmoid.numpy())
```
![[7.Sigmoid_Softmax.png]]



# 3-2: Logistic Regression Models

### Code.3-2-1: Single-variate Logistic Regression Models

``` python
"""
100개의 normal distribution을 따르는 랜덤 number를 sigmoid함수를 통과 한 뒤의 그래프를 pyplot을 사용하여 그리시오
"""
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Dense

plt.style.use('seaborn')

X = tf.random.normal(shape = (100, 1))
dense = Dense(units = 1, activation = 'sigmoid')
Y = dense(X)
print(Y.shape)

fig, ax = plt.subplots(figsize = (15, 10))
ax.scatter(X.numpy().flatten(), Y.numpy().flatten())
```
![[8.Sigmoid_Softmax.png]]






### Code. 3-2-2: Multi-variate Logistic Regression Models

``` python
"""
Multi Variate input이 sigmoid dense layer를 통과 한뒤의 shape을 확인하시오
"""
import tensorflow as tf
from tensorflow.keras.layers import Dense

plt.style.use('seaborn')

N = 100 # 사람 수
# 여러가지 종류(공부시간, 용돈, 교육비, 거주지역, 부모님) = Multi variate
n_feature = 5 

X = tf.random.normal(shape = (N, n_feature))
dense = Dense(units = 1, activation = 'sigmoid')

Y = dense(X)
print(Y.shape)
```




# 3-3: Binary Classifier with Dense Layers

### Code.3-3-1: Binary Classifier with Dense Layers

``` python
"""
Binary Classifier를 수행하는 3-Dense Layer를 구현하시오
"""
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

model = Sequential()
model.add(Dense(units = 10, activation = 'relu'))
model.add(Dense(units = 5, activation = 'relu'))
model.add(Dense(units = 3, activation = 'sigmoid'))
```



# 3-4: Softmax Layers


### Code.3-4-1: IO of Softmax
``` python
"""
uniform distribution을 따르는 8, 5 행렬의 input을 softmax Activation layer를 통과시킨 뒤 Probabilities와 softmax의 합의 결과를 확인하시오
"""
import tensorflow as tf
from tensorflow.keras.layers import Activation

logit = tf.random.uniform(shape=(8, 5), minval = -10, maxval=10)

softmax_value = Activation('softmax')(logit)
# axis0 = 세로축, axis1 = 가로축
softmax_sum = tf.reduce_sum(softmax_value, axis = 1)

print('Logits: ', logit.numpy())
print('Probabilities: ', softmax_value.numpy()) # P의 합은 무조건 1
print('Sum of softmax values: ', softmax_sum.numpy())
```



### Code.3-4-2: Softmax in Dense Layers

``` python
"""
8개의 class를 분류 할 수 있는 1 dense layer를 구현하시오
input = (8, 5)
"""
import tensorflow as tf
from tensorflow.keras.layers import Dense

logit = tf.random.uniform(shape = (8, 5), minval=-10, maxval=10)
				
dense = Dense(units=8, activation = 'softmax') # 8개의 클래스 분류

Y = dense(logit)
print(tf.reduce_sum(Y, axis = 1))
```




# 3-5: Multi-class Classifiers


### Code.3-5-1 : Multi-calss Classifiers

``` python
"""
Multi-calss Classifiers 3-dense layer를 class로 구현하시오
"""
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model

class TestModel(Model):
	def __init__(self):
		super(TestModel, self).__init__()
		
		self.dense1 = Dense(units=8, activation = 'relu')
		self.dense2 = Dense(units=5, activation = 'relu')
		self.dense3 = Dense(units=3, activation = 'softmax')

	def __call__(self, x):
		print("X: {}\n{}\n".format(X.shape, X.numpy()))

		x = self.dense1(x)
		print("A1 {}\n{}\n".format(x.shape, x.numpy()))

		x = self.dense2(x)
		print("A2 {}\n{}\n".format(x.shape, x.numpy()))

		x = self.dense3(x)
		print('Y: {}\n{}\n\n'.format(x.shape, x.numpy()))
		print('Sum of vectors: {}\n'.format(tf.reduce_sum(x, axis = 1)))



model = TestModel()

X = tf.random.uniform(shape = (8, 5), minval = -10, maxval = 10)
Y = model(X)
```

