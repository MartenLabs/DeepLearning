

## 이론
-----

### Neuron Vectors and Layers


![[DenseLayer1.png]]

### $\vec \nu$  : neuron vector       $\nu _1$ : 1번 neuron (parameter = $w$, $b$)

### Layer : 필터들의 묶음 (Fillter Bank)

- 각 neuron 들은 서로다른 파라미터를 가지고 있음
- 따라서 각 뉴런들은 같은 입력을 받아도 서로 다른 출력을 내보냄 


- layer가 계속 연결된 구조 = cascaded 구조
- deep learning도 cascaded filtter bank의 특징을 따르고 있다

#### 딥러닝 Architecture 
- correlation filtter들을 묶어서 fillter bank를 만들고 그 fillter들의cascaded 구조




### Dense Layers

![[DenseLayer2.png]]

- Dense : 뉴런마다 input들이 전부 다 연결돼 있는것 (Fully Connected)
- edge == connection
- node == neuron




### Notation

![[DenseLayer3.png]]
- $L^{[i]}$ : 몇번째 Layer            $l_i$ : 뉴런 갯수        $\vec \nu ^{[i]}$ : 몇번째 Layer의 뉴런 벡터




### Params of Dense Layer

![[DenseLayer4.png]]
- $\nu ^{[1]}_{l_1}$  : 첫번째 레이어의 $l_1$ 번째 뉴런
- $w^{[1]}_{1, 2}$ : 첫번째 레이어중 첫번째 input($x_1$) 과 곱해질 두번째 뉴런의 가중치
- $b^{[1]}_{l_1}$   : bias의 갯수는 뉴런의 갯수와 동일



### Weight Matrix and Bias Vector

![[DenseLayer5.png]]

$W^{[1]}$ = $l_I \times l_1$         $(\vec b^{[1]})^T$ = $1 \times l_1$ 

$l_I$ : input 갯수           $l_1$ : 뉴런 갯수



### Forward Propagation of Dense Layer

![[DenseLayer6.png]]

$(\vec a^{[1]})^T$ : $1 \times l_1$           $W^{[1]}$ = $l_I \times l_1$            $(\vec b^{[1]})^T$ = $1 \times l_1$ 




### The Second Dense Layer

![[DenseLayer7.png]]




### Generalized Dense Layer

![[DenseLayer8.png]]

W를 행으로 묶으면 이는 첫번째 input이 가는 모든 neuron들의 묶음 
W를 열로 묶으면 이는 첫번째 뉴런으로 가는 모든 input들의 묶음 




### Minibatch in Dense Layers

![[DenseLayer9.png]]

Minibatch는 excel을 생각하면 쉽다
Ex) 이름  |  국어    영어    ....    수학 => $l_I$
                                           .
                                           .
                                           .
                                           N

- $X^T$ = $(N \times l_I)$     N = 데이터 sample의 갯수

- $X^T$ 가 행렬이 됐다고 가중치 갯수의 변화가 생기지는 않는다 $W = (l_I, l_1)$ 

- 그럼 당연히 출력 $A^T = X^T \cdot W$  이므로 $(N \times l_1)$


![[DenseLayer10.png]]
출력 $(A^{[i]})^T$ 를 행으로 묶으면 서로 다른 $W, \;\; b$ 
- 하나의 input data가 다른 Neuron에 곱해졌기 때문

출력 $(A^{[i]})^T$ 를 열로 묶으면 동일한 $W, \;\; b$ 
- 각각의 input data 가 같은 Neuron에 곱해졌기 때문

![[DenseLayer11.png]]



### Cascaded Dense Layers

![[DenseLayer12.png]]







## 실습
----

# 2-1 : Dense Layers

### Code 2-1-1 : Shapes of Dense Layers

``` python
"""
sigmoid activation function과 3개의 neuron을 가지는 Dense Layer를 구현하시오 또한 w와 ouput을 예상해 보고 맞는지 확인하시오.
input은 (8, 10)
"""
import tensorflow as tf
from tensorflow.keras.layers import Dense

N, n_feature = 8, 10

X = tf.random.normal(shape = (N, n_feature))

n_neurons = 3
dense = Dense(units = n_neuron, activation = 'sigmoid')
Y = dense(X)


W, B = dense.get_weights()


print("------ Input / Weight / Bias ------")
print("X: ", X.shape) # (8, 10)
print("W: ", W.shape) # (10, 3)
print("B: ", B.shape) # (1, 3)
print("Y: ", Y.shape) # (8, 3)
```




### Code 2-1-2 : Output Calculations

``` python
"""
3개의 뉴런을 가지고 sigmoid activation function을 가지는 dense layer를 구현하시오. 단 dot product까지 메뉴얼로 구현하시오 
input = (4, 10)
"""
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.linalg import matmul
form tensorflow.math import exp

N, n_feature = 4, 10
X = tf.random.normal(shape = (N, n_feature))

n_neuron = 3

dense = Dense(units = n_neuron, activation = 'sigmoid')
y_tf = dense(X)


W, B = dense.get_weights()


""" manual """
Z = matmul(X, W) + B
y_man_matmul = 1 / (1 + exp(-Z))



""" only manual """
y_man_vec = np.zeros(shape = (N, n_neuron))

for x_idx in range(N):
	x = X[x_idx]
	for nu_idx in range(n_neuron):
	w, b = W[:, nu_idx], B[nu_idx]

	z = tf.reduce_sum(x * w) + b 
	a = 1 / (1 + exp(-z))
	Y_man_vec[x_idx, nu_idx] = a
  
print("Y(with dot products: ) \n", Y_man_vec)
```





# 2-2 : Cascaded Dense Layers


### Code 2-2-1 : Shape of Cascaded Dense Layers

``` python
"""
3, 5개의 뉴런으로 구성된 casecaded dense layer를 구현하고 각 레이어의 w와 output을 예상하여 맞는지 확인하시오
activation function = sigmoid
input (4, 10)
"""
import tensorflow as tf
from tensorflow.keras.layers import Dense

N, n_feature = 4, 10
X = tf.random.normal(shape = (N, n_feature))

n_neurons = [3, 5]
dense1 = Dense(units = n_neurons[0], activation = 'sigmoid')
dense2 = Dense(units = n_neurons[1], activation = 'sigmoid')

A1 = dense1(X) # (4, 3)
Y = dense2(A1) # (4, 5)


W1, B1 = dense1.get_weights() # W1 = (10 ,3)
W2, B2 = dense2.get_weights() # W2 = (3, 5)
 
print("X: {}\n".format(X.shape))
print("W1: ", W1.shape)
print("B1: ", B1.shape)

print("A1: {}\n".format(A1.shape))


print("W2: ", W2.shape)
print("B2: ", B2.shape)

print("Y: {}\n".format(Y.shape))

```




### Code 2-2-2 : Dense Layers with Python List 

``` python
"""
10+=10 ~ 90 개의 뉴런을 가지는 9개의 레이어로 구성된 cascaded dense layer를 구현하고 각 레이어에서의 w와 output을 예상하여 맞는지 확인하시오
activation function = relu
input (4, 10)
"""
import tensorflow as tf
from tensorflow.keras.layers import Dense

N, n_feature = 4, 10
X = tf.random.normal(shape = (N, n_feature))

n_neurons = [10, 20, 30, 40, 50, 60, 70, 80, 90]

dense_layers = list()
for n_neuron in n_neurons:
	dense = Dense(units = n_neuron, activation = 'relu')
	dense_layers.append(dense)

for dense_idx, dense in enumerate(dense_layers):
	X = dense(X)
	print("After dense layer ", dense_idx+1)
	print(X.shape, '\n')

```




### Code 2-2-3 : Output Calculators

``` python
"""
3 ~ 5개의 뉴런을 가지는 3개의 cascaded dense layer를 리스트로 구현하고 각각의 레이어의 w와 b또한 레이어로 받아와 메뉴얼로 직접 구현하시오

activation function = sigmoid
input (4, 10)
"""
import tensorflow as tf
from tensorflow.math import exp
from tensorflow.linalg import matmul
from tensorflow.keras.layers import Dense

N, n_feature = 4, 10
X = tf.random.normal(shape = (N, n_feature))
X_cp = tf.identity(X)

n_neurons = [3, 4, 5]

dense_layers = list()
for n_neuron in n_neurons:
	dense = Dense(units = n_neuron, activation = 'sigmoid')
	dense_layers.append(dense)


W, B = list(), list()
for dense_idx, dense in enumerate(dense_layers):
	X = dense(X)
	w, b = dense.get_weights()
	W.append(w)
	B.append(b)
print("Y(Tensorflow): \n", X.numpy())


for layer_idx in range(len(n_neurons)):
	w, b = W[layer_idx], B[layer_idx]

	X_cp = matmul(X_cp, w) + b
	X_cp = 1 / (1+ exp(-X_cp))

print("Y(Manual): \n", X_cp.numpy())
```





# 2-3 : Model Implementation

### Code.2-3-1: Model Implementation with Sequential Method

``` python
"""
Sequential module 을 사용하여 2-dense layer를 구성하시오
첫번째 레이어는 10개 neuron, 두번째 레이어는 20개 neuron
activation function = sigmoid
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

model = Sequential()
model.add(Dense(units=10, activation='sigmoid'))
model.add(Dense(units=20, activation='sigmoid'))
```


### Code.2-3-2: Model Implementation with Model-subclassing

``` python
"""
TestModel 클래스를 만들고 클래스 내부에서 2-dense layer를 구성하시오
첫번째 레이어는 10개 neuron, 두번째 레이어는 20개 neuron
activation function = sigmoid

또한 외부에서 dense1과 dense2 를 각각 호출해 보시오
"""

from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model

class TestModel(Model):
	def __init__(self):
		# 상속받은 Model의 생성자 초기화
		super(TestModel, self).__init__() 
		self.dense1 = Dense(units=10, activation='sigmoid')
		self.dense2 = Dense(units=20, activation='sigmoid')

  
model = TestModel()
print(model.dense1)
print(model.dense2)
```


### Code.2-3-3: Forward Propagation or Models

``` python
"""
2-dense layer를 Sequential method 와 model-subclassing을 사용하여 각각 만들어보고 값을 비교헤 보시오

input = (4, 10)
activation = sigmoid
"""
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import Model

X = tf.random.normal(shape=(4, 10))

# Sequential method
model = Sequential()
model.add(Dense(units=10, activation = 'sigmoid'))
model.add(Dense(units=20, activation = 'sigmoid'))

Y = model(X)
print(Y.numpy())
#######################################


# Model-subclassing
class TestModel(Model):
	def __init__(self):
		super(TestModel, self).__init__()
		self.dense1 = Dense(units=10, activation='sigmoid')
		self.dense2 = Dense(units=20, activation='sigmoid')

	def __call__(self, x):
		x = self.dense1(x)
		x = self.dense2(x)
		return x
		
model = TestModel()
Y = model(X)
```


### Code. 2-3-3-1 

```python
"""
가변 nueron을 가지는 n-dense layer 클래스를 설계하고 call 함수를 호출하여 출력을 확인하시오
"""
class TestModel(Model):
	def __init__(self, n_neurons):
		super(TestModel, self).__init__()
		self.n_neurons = n_neurons
		self.dense_layers = list()

		for n_neuron in self.n_neurons:
			self.dense_layers.append(Dense(units=n_neuron, 
									activation = 'sigmoid'))

	def __call__(self, x):
		for dense in self.dense_layers:
			x = dense(x)
		return x

n_neurons = [3, 4, 5]
model = TestModel(n_neurons)

x = tf.random.normal(shape = (1, 10))
y = model(x)
print(y)
```


### Code. 2-3-4: Layers in Models

``` python
"""
각각 10, 20 개의 neuron을 가지는 2-dense layer를 만들고 각 layer의 Weight와 Bias및 기타 학습 대상이 되는 var들을 확인하시오
"""
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

X = tf.random.normal(shape = (4, 10))

model = Sequential()
model.add(Dense(units = 10, activation = 'sigmoid'))
model.add(Dense(units = 20, activation = 'sigmoid'))

Y = model(X)

print(type(model.layers))
print(model.layers)

dense1 = model.layers[0]
print(dense1)

for layer in model.layers:
	w, b = layer.get_weights()
	print(w.shape, b.shape)

print(type(model.trainable_variables))
print(len(model.trainable_variables))

for train_var in model.trainable_variables:
	print(train_var.shape)
```



















