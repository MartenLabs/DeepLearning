  

## 이론
-----

### Dataset (X Data)

- $\vec x^T \; = \; (x1\;\;\; x2\;\;\; ...\;\;\; x_{l_I})$ 
- $(\vec x^{(1)})^T \; = \; (x^{(1)}_1 \;\;\; x^{(1)}_2 \;\;\; ... \;\;\; x^{(1)}_{l_I})$
- $(\vec x^{(2)})^T \; = \; (x^{(2)}_1 \;\;\; x^{(2)}_2 \;\;\; ... \;\;\; x^{(2)}_{l_I})$
-     ...
- $(\vec x^{(N)})^T \; = \; (x^{(N)}_1 \;\;\; x^{(N)}_2 \;\;\; ... \;\;\; x^{(N)}_{l_I})$



$$\vec x^T \; = \; (x_1\;\;\; x_2\;\;\; ...\;\;\; x_{l_I})$$
![[ForwarPropagation1.png]]





### Affine Function with One Feature

Weight Sum
$z = xw$

Affine Transformation
$z = xw + b$

![[ForwarPropagation2.png]]





### Affine Function with n Features

Weithted Sum
$z = w_1x_1 + w_2x_2+ ... + w_nx_n = (\vec w)^T\vec x = (\vec x)^T\vec w$


Affine Transformation
$z = w_1x_1 + w_2x_2 + ... + w_n x_n +b = (\vec x)^T \vec w + b$

![[ForwarPropagation3.png]]




### Activation Functions

# - Sigmoid       $g(x) = \sigma(x) = {1 \over 1+e^{-x}}$


# - Tanh               $g(x) = tanh(x) = {e^x - e^{-x} \over e^x + e^{-x}}$


# - ReLU       $g(x) = ReLU(x) = max(0, x)$
 




### Affine Neurons

![[ForwarPropagation4.png]]





### Minibatch in Artificial Neurons

![[ForwarPropagation5.png]]


![[ForwarPropagation6.png]]
![[ForwarPropagation7.png]]




## 실습
----
# 1-1: Affine Functions with 1 Feature

### Code 1-1-1 : Affine Function
``` python
"""
input 1개, neuron 1개 인 Affine Function을 작성하고 x, y, w, b의 shape과 값을 출력하시오
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense


x = tf.constant([[10.]]) 
dense = Dense(uints = 1, activation = 'linear')

#------------- tensorflow --------------
y_tf = dense(x) # forward propagation
W, B = dense.get_weights()

#-------------- manual ------------------
y_man = tf.linagl.matmul(x, W) + B # forward propagation
# linagl = linear algebra
# matmul = matrix multiplication

print('x_shape: {}\n'.format(x.shape, x.numpy()))
print('W_shape: {}\n'.format(W.shape, W))
print('B_shape: {}\n'.format(B.shape, B))
```



### Code 1-1-2 : Params Initialization
``` python
"""
w, b를 각각 10, 20으로 직접 지정해서 입력1, 출력1인 Affine Function을 작성하고 각 값을 확인하시오
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.initializers import Constant

w, b = tf.constant(10.), tf.constant(20.) # 사용자 지정 (아직 사용할 순 없음)
w_init, b_init = tf.keras.initializers.Constant(w), Constant(b) #이제 사용할 수 있음

dense = Dense(units = 1, 
			  activation = 'linear'
			  kernel_initializer = w_init,
			  bias_initializer = b_init)

x = tf.constant([[10.]])
y_tf = dense(x)

W, B = dense.get_weights()

print('W: {}\n{}'.format(W.shape, W))
print('B: {}\n{}'.format(B.shape, B))
```



# 1-2 : Affine Functions with N Features

### Code 1-2-1 : Affine Functions with N Features

``` python
"""
input: (1x10) feature, output 1인 Affine Function을 작성하시오
*input의 shape은 random.uniform을 사용하여 출력*
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.initializers import Constant

x = tf.random.uniform(shape=(1, 10), minval=0, maxval=10)

dense = Dense(units = 1)

y_tf = dense(x)
W, B = dense.get_weights()

y_man = tf.linalg.matmul(x, W) + B

print('x_shape: {}\n{}'.format(x.shape, x.numpy()))
print('W_shape: {}\n{}'.format(W.shape, W))
print('B_shape: {}\n{}'.format(B.shape, B))


print('y_tf: {}\n{}'.format(y_tf.shape, y_tf.numy()))
```



# 1-3 : Activation Functions

### Code 1-3-1 : Activation Layers

``` python
"""
sigmoid, tanh, relu Activation function을 tensorflow로 부터 불러오고 (1, 5) 인풋을 각각의 Activation function에 넣어보시오.
또한 각각의 Activation function을 직접 구현하고 인풋을 넣어 tensorflow와 값 차이가 있는지 확인하시오
"""


import tensorflow as tf
from tensorflow.keras.layers import Activation
from tensorflow.math import exp, maximum


x = tf.random.normal(shape = (1, 5))

sigmoid = Activation('sigmoid')
tanh = Activation('tanh')
relu = Activation('relu')

y_sigmoid_tf = sigmoid(x)
y_tanh_tf = tanh(x)
y_relu = relu(x)


y_sigmoid_man = 1 / (1 + exp(-x))
y_tanh_man = ((exp(x) - exp(-x)) / (exp(x) + exp(-x)))
y_relu_man = maximum(0, x)

print("x: {}\n{}".format(x.shape, x.numpy()))

print("Sigmoid(Tensorflow): {}\n{}\n".format(y_sigmoid_tf.shape, y_sigmoid_tf.numpy()))
print("Sigmoid(man): {}\n{}\n".format(y_sigmoid_man.shape, y_sigmoid_man.numpy()))
  
print("Tanh(Tensorflow): {}\n{}\n".format(y_tanh_tf.shape, y_tanh_tf.numpy()))
print("Tanh(man): {}\n{}\n".format(y_tanh_man.shape, y_tanh_man.numpy()))

print("ReLu(Tensorflow): {}\n{}\n".format(y_relu_tf.shape, y_relu_tf.numpy()))
print("ReLu(man): {}\n{}\n".format(y_relu_man.shape, y_relu_man.numpy()))

```




### Code 1-3-2 : Activation in Dense Layer

``` python
import tensorflow as tf
from tensorflow.keras.layers import Dense

x = tf.random.normal(shape = (1, 5))

dense_sigmoid = Dense(units = 1, activation = 'sigmoid')
dense_tanh = Dense(units = 1, activation = 'tanh')
dense_relu = Dense(units = 1, activation = 'relu')

y_sigmoid = dense_sigmoid(x)
y_tanh = dense_tanh(x)
y_relu = dense_relu(x)

# forward propagation(tensorflow)
print("AN with Sigmoid: {}\n{}".format(y_sigmoid.shape, y_sigmoid.numpy()))
print("AN with Tanh: {}\n{}".format(y_tanh.shape, y_tanh.numpy()))
print("AN with ReLU: {}\n{}".format(y_relu.shape, y_relu.numpy()))


# forward propagation(manual)
W, B = dense_sigmoid.get_weights()
z = tf.linalg.matmul(x, W) + b
y_man = 1 / (1 + exp(-z))

print("Activation value(Tensorflow): {}\n{}".format(y_sigmoid.shape, y_sigmoid.numpy()))
print("Activation value(manual): {}\n{}".format(a.shape, a.numpy()))

```




###  Code 1-4-1 : Artificial Neurons

``` python
"""
sigmoid, tanh, relu를 Activation function으로 가지는 각각의 Dense Layer를 만들고 (1, 10)짜리 인풋을 통과시켜 값을 확인하시오.

또한 직접 affine function 및 activation function을 구현하여 인풋을 넣었을 때 tensorflow와 차이가 있는지 확인하시오
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.math import exp, maximum

activation = 'relu'

x = tf.random.normal(shape = (1, 10))

dense = Dense(units = 1, activation = activation)

y_tf = dense(x)
W, B = dense.get_weights()

y_man = tf.linalg.matmul(x, W) + B

if activation == 'sigmoid':
	y_man = 1 / (1 + exp(-y_man))

elif activation == 'tanh':
	y_man = (exp(y_man) - exp(-y_man)) / (exp(y_man) + exp(-y_man))

elif activation == 'relu':
	y_man = maximum(0, y_man)

print('Activation: ', activation)
print('y_tf: {}\n{}\n'.format(y_tf.shape, y_tf.numpy()))
print('y_man: {}\n{}\n'.format(y_man.shape, y_man.numpy()))
```




# 1-5 : Minibatches

### Code 1-5-1 : Shapes of Dense Layers

``` python
import tensorflow as tf
from tensorflow.keras.layers import Dense

N, n_feature = 8, 10
x = tf.random.normal(N, n_feature)

dense = Dense(units = 1, activation = 'relu')

y_tf = dense(x)

W, B = dense.get_weights()

print("Shape of x: {}".format(x.shape))
print("Shape of W: {}".format(W.shape))
print("Shape of B: {}".format(B.shape))
```



### Code 1-5-2 : Output Calculations

``` python
"""
(8, 10)짜리 input을 sigmoid activation을 가지는 dense 1 자리 네트워크에 넣었을 때 W와 ,B를 예측해보고 그 값을 출력해 보시오 

또한 affine function과 sigmoid 함수를 직접 구현해보고 tensorflow와 값 차이가 있는지 확인하시오
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.math import exp

N, n_feature = 3, 4

x = tf.random.normal(shape = (N, n_feature))

dense = Dense(units = 1, activation = 'sigmoid')

y_tf = dense(x)

W, B = dense.get_weights()

y_man = tf.linalg.matmul(x, W) + B
y_man = 1 / (1 + exp(-y_man))

print("W shape: \n", W.shape)
print("Output(TF): \n",y_tf.numpy())
print("Output(MAN): \n",y_man.numpy())
```