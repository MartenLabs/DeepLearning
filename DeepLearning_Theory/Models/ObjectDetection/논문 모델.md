
``` python
import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam

AUTOTUNE = tf.data.AUTOTUNE

RES_HEIGHT = 96
RES_WIDTH = 128
NUM_CLASS = 1
N_BATCH = 3
N_EPOCH = 200
LR = 0.0002
```


``` python
import numpy as np

datasets = np.load('dataset/ObjectDetection.npz', allow_pickle=True)
images, numbers, bboxes = datasets['images'], datasets['numbers'], datasets['bboxes']

max_label_length = 4
labels = []
for num in numbers:
    cls = [1] * num if num != 0 else [0]
    cls += [0] * (max_label_length - len(cls))
    labels.append(cls)

# labels = np.array(labels)

# non_zero_indices = np.where(numbers != 0)[0]
non_zero_indices = np.where(numbers > 3)[0]

# numbers가 0이 아닌 항목만 유지
images_filtered = images[non_zero_indices]
bboxes_filtered = bboxes[non_zero_indices]
labels_filtered = np.array(labels)[non_zero_indices]
numbers_filters = numbers[non_zero_indices]
print(images.shape, numbers.shape, bboxes.shape, len(labels))

print(images.max(), images.min())

dataset = {
    'images' : images_filtered,
    'bboxes' : bboxes_filtered,
    'class' : labels_filtered,
    'numbers': numbers_filters
}

print(dataset['images'].shape)
print(dataset['bboxes'].shape)
print(len(dataset['class']))
print(dataset['class'])

```

``` python
import numpy as np
import cv2

def resize_and_pad_image(image, target_size):
    # 이미지 크기를 가로, 세로 방향으로 4배 늘림
    # cv2.INTER_NEAREST 보간법을 사용하여 픽셀 값을 유지
    resized_image = cv2.resize(image, (image.shape[1] * 4, image.shape[0] * 4), interpolation=cv2.INTER_NEAREST)
    
    # 이미지에 채널 차원 추가
    resized_image = np.expand_dims(resized_image, axis=-1)
    
    # 크기가 조정된 이미지를 대상 크기에 맞게 패딩
    # 패딩된 영역은 0으로 채움
    padded_image = np.zeros((target_size[1], target_size[0], resized_image.shape[2]), dtype=resized_image.dtype)
    padded_image[:resized_image.shape[0], :resized_image.shape[1]] = resized_image
    
    return padded_image

def adjust_bboxes(bboxes, original_size, target_size):
    # 바운딩 박스 좌표 조정
    # 좌표 값을 4배로 늘림
    adjusted_bboxes = bboxes.copy()
    adjusted_bboxes[:, [0, 2]] = adjusted_bboxes[:, [0, 2]] * 4
    adjusted_bboxes[:, [1, 3]] = adjusted_bboxes[:, [1, 3]] * 4
    
    return adjusted_bboxes

def resize_and_adjust_dataset(dataset, target_size):
    # 데이터셋에서 이미지, 바운딩 박스, 레이블, 숫자 정보를 추출
    images = dataset['images']
    bboxes = dataset['bboxes']
    labels = dataset['class']
    numbers = dataset['numbers']

    resized_images = []
    adjusted_bboxes = []

    # 각 이미지와 해당 바운딩 박스에 대해 반복
    for image, bbox in zip(images, bboxes):
        # 이미지의 원래 크기 저장
        original_size = image.shape[:2]
        
        # 이미지 크기를 조정하고 패딩 추가
        resized_image = resize_and_pad_image(image, target_size)
        resized_images.append(resized_image)
        
        # 바운딩 박스 좌표 조정
        adjusted_bbox = adjust_bboxes(bbox, original_size, target_size)
        adjusted_bboxes.append(adjusted_bbox)

    # 크기 조정된 이미지와 조정된 바운딩 박스를 NumPy 배열로 변환
    resized_images = np.array(resized_images)
    adjusted_bboxes = np.array(adjusted_bboxes)

    # 조정된 데이터셋을 딕셔너리 형태로 생성
    adjusted_dataset = {
        'images': resized_images,
        'bboxes': adjusted_bboxes,
        'class': labels,
        'numbers': numbers
    }

    return adjusted_dataset

target_size = (128, 96)  # 원하는 이미지 크기로 설정 (width, height)
dataset = resize_and_adjust_dataset(dataset, target_size)
```

``` python
import numpy as np
import random

def calculate_iou(box1, box2):
    # 두 개의 바운딩 박스 간의 IoU(Intersection over Union) 계산
    # 바운딩 박스의 좌표를 이용하여 겹치는 영역의 면적과 전체 영역의 면적을 계산하고,
    # 겹치는 영역의 면적을 전체 영역의 면적으로 나누어 IoU 값을 구함
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)
    
    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)
    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)
    
    union_area = box1_area + box2_area - intersection_area
    
    iou = intersection_area / union_area
    
    return iou

def apply_cutmix(dataset, max_people=10, iou_threshold=0.2, mode='rand', target_size=None, new_image_size=(96, 128)):
    # CutMix 기법을 적용하여 데이터셋을 확장
    # 입력으로 받은 데이터셋에서 이미지, 바운딩 박스, 레이블, 사람 수 정보 추출
    images = dataset['images']
    bboxes = dataset['bboxes']
    labels = dataset['class']
    numbers = dataset['numbers']

    # 최대 레이블 길이(사람 수)와 새로운 이미지 크기 설정
    max_label_length = max_people
    new_image_width, new_image_height = new_image_size

    # CutMix가 적용된 이미지, 바운딩 박스, 레이블, 사람 수를 저장할 리스트 초기화
    cutmix_images = []
    cutmix_bboxes = []
    cutmix_labels = []
    cutmix_numbers = []

    # 목표 데이터셋 크기가 지정되지 않은 경우, 원본 데이터셋의 크기 사용
    if target_size is None:
        target_size = len(images)

    # 목표 데이터셋 크기만큼 반복하여 CutMix 적용
    while len(cutmix_images) < target_size:
        # 원본 데이터셋에서 무작위로 이미지 선택
        i = random.randint(0, len(images) - 1)
        image = images[i].copy()
        bbox = bboxes[i].copy()
        label = labels[i].copy()
        number = numbers[i]

        # 모드에 따라 목표 사람 수 설정
        if mode == 'all':
            target_number = max_people
        elif mode == 'rand':
            target_number = random.randint(number, max_people)
        else:
            raise ValueError("Invalid mode. Choose 'all' or 'rand'.")

        # 목표 사람 수만큼 반복하여 다른 이미지에서 사람을 선택하여 현재 이미지에 추가
        while number < target_number:
            # 사람이 있는 이미지의 인덱스 찾음
            donor_indices = np.where(numbers > 0)[0]
            if len(donor_indices) > 0:
                # 무작위로 donor 이미지 선택
                donor_idx = random.choice(donor_indices)
                donor_image = images[donor_idx]
                donor_bbox = bboxes[donor_idx]
                donor_label = labels[donor_idx]
                donor_number = numbers[donor_idx]

                # donor 이미지에서 사람이 있는 바운딩 박스의 인덱스 찾음
                non_zero_indices = np.where(donor_label != 0)[0]
                if len(non_zero_indices) > 0:
                    # 무작위로 바운딩 박스 선택
                    bbox_idx = random.choice(non_zero_indices)
                    new_bbox = donor_bbox[bbox_idx]

                    # 선택한 바운딩 박스와 현재 이미지의 바운딩 박스 간의 IoU 계산
                    iou_values = [calculate_iou(bbox[j], new_bbox) for j in range(number)]
                    # IoU가 임계값 미만인 경우에만 이미지에 추가
                    if max(iou_values) < iou_threshold:
                        x1, y1, x2, y2 = map(int, new_bbox)
                        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(new_image_width, x2), min(new_image_height, y2)

                        # donor 이미지에서 선택한 바운딩 박스 영역을 현재 이미지에 복사
                        image[y1:y2, x1:x2] = donor_image[y1:y2, x1:x2]

                        # 바운딩 박스와 레이블 업데이트
                        bbox = np.pad(bbox, ((0, max_label_length - bbox.shape[0]), (0, 0)), mode='constant')
                        bbox[number] = new_bbox
                        label = np.pad(label, (0, max_label_length - label.shape[0]), mode='constant')
                        label[number] = 1
                        number += donor_number

        # 바운딩 박스와 레이블의 길이를 최대 레이블 길이에 맞게 패딩
        bbox = np.pad(bbox, ((0, max_label_length - bbox.shape[0]), (0, 0)), mode='constant')
        label = np.pad(label, (0, max_label_length - label.shape[0]), mode='constant')

        # CutMix가 적용된 이미지, 바운딩 박스, 레이블, 사람 수를 리스트에 추가
        cutmix_images.append(image)
        cutmix_bboxes.append(bbox)
        cutmix_labels.append(label)
        cutmix_numbers.append(number)

    # CutMix가 적용된 데이터를 NumPy 배열로 변환
    cutmix_images = np.array(cutmix_images[:target_size])
    cutmix_bboxes = np.array(cutmix_bboxes[:target_size])
    cutmix_labels = np.array(cutmix_labels[:target_size])
    cutmix_numbers = np.array(cutmix_numbers[:target_size])

    # 원본 데이터셋의 바운딩 박스와 레이블도 최대 레이블 길이에 맞게 패딩
    padded_bboxes = []
    padded_labels = []
    for bbox, label in zip(bboxes, labels):
        padded_bbox = np.pad(bbox, ((0, max_label_length - bbox.shape[0]), (0, 0)), mode='constant')
        padded_label = np.pad(label, (0, max_label_length - label.shape[0]), mode='constant')
        padded_bboxes.append(padded_bbox)
        padded_labels.append(padded_label)

    # 원본 데이터셋과 CutMix가 적용된 데이터셋을 합침
    merged_images = np.concatenate((images, cutmix_images), axis=0)
    merged_bboxes = np.concatenate((np.array(padded_bboxes), cutmix_bboxes), axis=0)
    merged_labels = np.concatenate((np.array(padded_labels), cutmix_labels), axis=0)
    merged_numbers = np.concatenate((numbers, cutmix_numbers), axis=0)

    # 합쳐진 데이터셋을 딕셔너리 형태로 반환
    merged_dataset = {
        'images': merged_images,
        'bboxes': merged_bboxes,
        'class': merged_labels,
        'numbers': merged_numbers
    }

    return merged_dataset


dataset = apply_cutmix(dataset, max_people=10, iou_threshold=0.1, mode='all', new_image_size=(128, 96), target_size=100000)
```


``` python
import os
import random
import tensorflow as tf

IMG_SIZE_WIDTH = images.shape[2]
IMG_SIZE_HEIGHT = images.shape[1]
N_DATA = images.shape[0]
N_VAL = int(images.shape[0] * 0.2)
N_TRAIN = int(images.shape[0] - N_VAL)
# N_TRAIN = int(images.shape[0])

cur_dir = os.getcwd()
tfr_dir = os.path.join(cur_dir, 'test/tfrecord/')
os.makedirs(tfr_dir, exist_ok=True)

tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')
tfr_val_dir = os.path.join(tfr_dir, 'od_val.tfr')

print("IMG_SIZE_WIDTH:  ", IMG_SIZE_WIDTH)
print("IMG_SIZE_HEIGHT: ", IMG_SIZE_HEIGHT)
print("N_DATA:          ", N_DATA)
print("N_TRAIN:         ", N_TRAIN)
print("N_VAL:           ", N_VAL)

shuffle_list = list(range(N_DATA))
random.shuffle(shuffle_list)

train_idx_list = shuffle_list[:N_TRAIN]
val_idx_list = shuffle_list[N_TRAIN:]

tfr_train_dir = os.path.join(tfr_dir, 'od_train.tfr')
tfr_val_dir = os.path.join(tfr_dir, 'od_val.tfr')

writer_train = tf.io.TFRecordWriter(tfr_train_dir)
writer_val = tf.io.TFRecordWriter(tfr_val_dir)
```



``` python
def _bytes_feature(value):
    if isinstance(value, type(tf.constant(0))):
        value = value.numpy()
    return tf.train.Feature(bytes_list = tf.train.BytesList(value = [value]))

def _float_feature(value):
    return tf.train.Feature(float_list = tf.train.FloatList(value = value))

def _int64_feature(value):
    return tf.train.Feature(int32_list = tf.train.Int64List(value = [value]))


def _bytes_feature_list(value_list):
    """value_list가 리스트일 때, 이를 serialize하여 bytes list로 변환하는 함수."""
    value_list = [tf.io.serialize_tensor(tf.constant(v)).numpy() for v in value_list]
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value_list))
```


``` python
for idx in train_idx_list:
    bbox = bboxes[idx]
    xmin, ymin, xmax, ymax = bbox[:, 0] / RES_WIDTH, bbox[:, 1] / RES_HEIGHT, bbox[:, 2] / RES_WIDTH, bbox[:, 3] / RES_HEIGHT
    bbox = np.stack([xmin, ymin, xmax, ymax], axis=-1).flatten()

    image = images[idx]
    bimage = image.tobytes()

    number = numbers[idx]
    class_id = cls[idx]
    # print(len(cls))
    serialized_cls = tf.io.serialize_tensor(tf.constant(class_id)).numpy()

    example = tf.train.Example(features=tf.train.Features(feature={
        'image': _bytes_feature(bimage),
        'bbox': _float_feature(bbox),
        'label': _bytes_feature(serialized_cls),
        # 'number': _int64_feature(number)
    }))
    
    writer_train.write(example.SerializeToString())
writer_train.close()
```

``` python
for idx in val_idx_list:
    bbox = bboxes[idx]
    xmin, ymin, xmax, ymax = bbox[:, 0] / RES_WIDTH, bbox[:, 1] / RES_HEIGHT, bbox[:, 2] / RES_WIDTH, bbox[:, 3] / RES_HEIGHT
    bbox = np.stack([xmin, ymin, xmax, ymax], axis=-1).flatten()

    image = images[idx]
    bimage = image.tobytes()

    number = numbers[idx]
    class_id = cls[idx]
    # print(len(cls))
    serialized_cls = tf.io.serialize_tensor(tf.constant(class_id)).numpy()

    example = tf.train.Example(features=tf.train.Features(feature={
        'image': _bytes_feature(bimage),
        'bbox': _float_feature(bbox),
        'label': _bytes_feature(serialized_cls),
        # 'number': _int64_feature(number)
    }))
    
    writer_val.write(example.SerializeToString())
writer_val.close()
```

``` python
AUTOTUNE = tf.data.AUTOTUNE

N_BATCH = 1
# LR = 0.0005


def _parse_function(tfrecord_serialized):
    features = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'bbox': tf.io.VarLenFeature(tf.float32),  
        'label': tf.io.FixedLenFeature([], tf.string),
        # 'number': tf.io.FixedLenFeature([], tf.int64)
    }

    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)

    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)
    image = tf.reshape(image, [RES_HEIGHT, RES_WIDTH, 1])
    image = tf.cast(image, tf.float32) 
    image = image / tf.reduce_max(image)
    image = (image - 0.5) * 2.0  # -1에서 1 사이의 범위로 변경

    bbox = tf.sparse.to_dense(parsed_features['bbox']) 
    bbox = tf.cast(bbox, tf.float32)
    # num_boxes = tf.shape(bbox)[0] // 4
    bbox = tf.reshape(bbox, [-1, 4])

    serialized_cls = parsed_features['label']
    label = tf.io.parse_tensor(serialized_cls, out_type=tf.int64)
    
    # number = tf.cast(parsed_features['number'], tf.int64)
    return image, bbox, label



train_dataset = tf.data.TFRecordDataset(tfr_train_dir)
train_dataset = train_dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)
train_dataset = train_dataset.shuffle(buffer_size=N_TRAIN).prefetch(AUTOTUNE).batch(N_BATCH, drop_remainder=True)
```

``` python
AUTOTUNE = tf.data.AUTOTUNE

N_BATCH = 1
# LR = 0.0005


def _parse_function(tfrecord_serialized):
    features = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'bbox': tf.io.VarLenFeature(tf.float32),  
        'label': tf.io.FixedLenFeature([], tf.string),
        # 'number': tf.io.FixedLenFeature([], tf.int64)
    }

    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)

    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)
    image = tf.reshape(image, [RES_HEIGHT, RES_WIDTH, 1])
    image = tf.cast(image, tf.float32) 
    image = image / tf.reduce_max(image)
    image = (image - 0.5) * 2.0  # -1에서 1 사이의 범위로 변경

    bbox = tf.sparse.to_dense(parsed_features['bbox']) 
    bbox = tf.cast(bbox, tf.float32)
    # num_boxes = tf.shape(bbox)[0] // 4
    bbox = tf.reshape(bbox, [-1, 4])

    serialized_cls = parsed_features['label']
    label = tf.io.parse_tensor(serialized_cls, out_type=tf.int64)
    
    # number = tf.cast(parsed_features['number'], tf.int64)
    return image, bbox, label




val_dataset = tf.data.TFRecordDataset(tfr_val_dir)
val_dataset = val_dataset.map(_parse_function, num_parallel_calls=AUTOTUNE)
val_dataset = val_dataset.shuffle(buffer_size=N_VAL).prefetch(AUTOTUNE).batch(N_BATCH, drop_remainder=True)
```

``` python
def convert_to_xywh(boxes):
    return tf.concat(
        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],
        axis=-1
    )

def convert_to_corners(boxes):
    return tf.concat(
        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],
        axis=-1
    )

def preprocess_data(image, gt_boxes, cls_ids):
    bbox = convert_to_xywh(gt_boxes)
    return image, bbox, cls_ids
```

``` python
idx = 0
for image, bbox, label in train_dataset.take(1):
    anchor_img = np.zeros((*image.shape[:3], 3), dtype=np.uint8)
    anchor_img = anchor_img[idx]

    strides = [4, 8, 16]
    colors = {
        4: [0, 255, 0],  # 초록색
        8: [0, 0, 255],  # 파란색
        16: [255, 0, 0], # 빨간색
    }

    for stride in strides:
        color = colors[stride]
        for y in range(0, anchor_img.shape[0], stride):
            for x in range(0, anchor_img.shape[1], stride):
                anchor_img[y, x, :] = color

    # 이미지 표시
    plt.imshow(image[idx], alpha=1)  
    plt.imshow(anchor_img, alpha=0.5) 
    plt.axis('off')
    plt.show()
    print(tf.reduce_max(image), tf.reduce_min(image))
```


``` python
idx = 0
for image, bbox, label in train_dataset.take(1):
    image, bbox, label = preprocess_data(image, bbox, label)
    img = image[idx]
    box = bbox[idx]
    label = label[idx]
    print(img.shape)
    print(box)
    print(label)
    print(tf.reduce_max(image), tf.reduce_min(image))
    # 이미지 시각화
    plt.imshow(img)
    ax = plt.gca()
    width = img.shape[1]
    height = img.shape[0]
    print("width: ", width)
    print("height: ", height)
    boxes = tf.stack(
        [
            (box[:, 0] - 0.5 * box[:, 2]),  # xmin = x_center - width/2
            (box[:, 1] - 0.5 * box[:, 3]),  # ymin = y_center - height/2
            box[:, 2],
            box[:, 3],
            
        ], axis=-1
    )
    print("bbox: ", boxes)
    # 각 바운딩 박스에 대해 반복하여 그리기
    for box in boxes:
        xmin, ymin, w, h = box
        print(box)
        patch = plt.Rectangle(
            [xmin * RES_WIDTH, ymin * RES_HEIGHT], w * RES_WIDTH, h * RES_HEIGHT, fill=False, edgecolor=[1, 0, 0], linewidth=2
        )
        ax.add_patch(patch)
    plt.show()
    print(label)
 
```


``` python
idx = 0
for image, bbox, label in train_dataset.take(1):
    image, bbox, label = preprocess_data(image, bbox, label)
    img = image[idx]
    box = bbox[idx]
    label = label[idx]
    print(img.shape)
    print(box)
    print(label)
    print(tf.reduce_max(image), tf.reduce_min(image))
    # 이미지 시각화
    plt.imshow(img)
    ax = plt.gca()
    width = img.shape[1]
    height = img.shape[0]
    print("width: ", width)
    print("height: ", height)
    boxes = tf.stack(
        [
            (box[:, 0] - 0.5 * box[:, 2]),  # xmin = x_center - width/2
            (box[:, 1] - 0.5 * box[:, 3]),  # ymin = y_center - height/2
            box[:, 2],
            box[:, 3],
            
        ], axis=-1
    )
    print("bbox: ", boxes)
    # 각 바운딩 박스에 대해 반복하여 그리기
    for box in boxes:
        xmin, ymin, w, h = box
        print(box)
        patch = plt.Rectangle(
            [xmin * RES_WIDTH, ymin * RES_HEIGHT], w * RES_WIDTH, h * RES_HEIGHT, fill=False, edgecolor=[1, 0, 0], linewidth=2
        )
        ax.add_patch(patch)
    plt.show()
    print(label)
 
```

``` python
class AnchorBox:
    def __init__(self):
        # 앵커 박스의 가로 세로 비율 설정
        self.aspect_ratios = [0.5, 1.0, 1.5]         
        # 앵커 박스의 크기 스케일 설정 (2^(2/3))
        self.scales = [2** x for x in [2/3]] 
        # 앵커 박스의 총 개수 계산 (가로 세로 비율 개수 * 크기 스케일 개수)
        self._num_anchors = len(self.aspect_ratios) * len(self.scales)
        # 피쳐 맵의 스트라이드 설정 (2^2, 2^3, 2^4)
        self._strides = [2 ** i for i in range(2, 5)]
        # 앵커 박스의 면적 설정 (15^2, 20^2, 25^2)
        self._areas = [x ** 2 for x in [15.0, 20.0, 25.0]]
        # 앵커 박스의 크기 계산
        self._anchor_dims = self._compute_dims()

    def _compute_dims(self):
        anchor_dims_all = []

        for area in self._areas:
            anchor_dims = []
            for ratio in self.aspect_ratios: 
                # 앵커 박스의 높이 계산 (면적 / 가로 세로 비율의 제곱근)
                anchor_height = tf.math.sqrt(area / ratio)
                # 앵커 박스의 너비 계산 (면적 / 높이)
                anchor_width = area / anchor_height
                # 앵커 박스의 크기를 [너비, 높이] 형태로 변환하여 dims에 저장
                dims = tf.reshape(
                    tf.stack([anchor_width, anchor_height], axis = -1), [1, 1, 2]
                )
                for scale in self.scales: 
                    # 크기 스케일을 곱하여 앵커 박스의 크기 조정
                    anchor_dims.append(scale * dims) 
            # 각 면적에 대한 앵커 박스 크기를 스택으로 쌓음
            anchor_dims_all.append(tf.stack(anchor_dims, axis = -2))
        return anchor_dims_all 
    
    def _get_anchors(self, feature_height, feature_width, level):
        # 피쳐 맵의 가로 방향 좌표 생성 (0.5를 더하여 중앙 좌표로 만듦)
        rx = tf.range(feature_width, dtype = tf.float32) + 0.5
        # 피쳐 맵의 세로 방향 좌표 생성 (0.5를 더하여 중앙 좌표로 만듦)
        ry = tf.range(feature_height, dtype = tf.float32) + 0.5

        # 피쳐 맵의 각 셀에 대한 중앙 좌표 생성
        centers = tf.stack(tf.meshgrid(rx, ry), axis = -1) * self._strides[level - 2] # stride시작점에 따라 바꿔야함 
        centers = tf.expand_dims(centers, axis = -2)
        # 중앙 좌표를 앵커 박스 개수만큼 반복하여 복사
        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])

        # 앵커 박스의 크기를 피쳐 맵의 각 셀에 대해 반복하여 복사
        dims = tf.tile(
            self._anchor_dims[level - 2], [feature_height, feature_width, 1, 1] 
        )

        # 중앙 좌표와 앵커 박스의 크기를 연결하여 앵커 박스 정보 생성
        anchors = tf.concat([centers, dims], axis=-1) 

        # 앵커 박스 정보를 1차원 텐서로 변환하여 반환
        return tf.reshape(
            anchors, [feature_height * feature_width * self._num_anchors, 4]
        )

    def get_anchors(self, image_height, image_width):
        anchors = [
            self._get_anchors(
                tf.math.ceil(image_height / 2 ** i), # 올림
                tf.math.ceil(image_width / 2 ** i),
                i
            )
            for i in range(2, 5)
        ]

        # 각 레벨의 앵커 박스를 연결하여 반환
        return tf.concat(anchors, axis=0)
```

``` python
anchors = AnchorBox()
anchor = anchors.get_anchors(96, 128)

# 앵커 박스 정규화
xmin = anchor[:, 0] / RES_WIDTH
ymin = anchor[:, 1] / RES_HEIGHT
xmax = anchor[:, 2] / RES_WIDTH
ymax = anchor[:, 3] / RES_HEIGHT

# 정규화된 좌표를 스택으로 결합
normalized_anchor = tf.stack([xmin, ymin, xmax, ymax], axis=-1)

has_negative_values = tf.reduce_any(tf.less(anchor, 0))
print("Anchor 음수 값:", has_negative_values.numpy())

print(anchor)
print(anchor.shape)
import matplotlib.pyplot as plt
import matplotlib.patches as patches

def draw_bounding_boxes(data, num_samples):
    fig, ax = plt.subplots()

    plt.imshow(img)
    print(img.shape)
    data_np = data.numpy()

    if len(data) > num_samples:
        sampled_indices = np.random.choice(len(data), num_samples, replace=False)
        sample_data = data_np[sampled_indices]
    else : 
        sample_data = data_np
    print(sample_data)
    for center_x, center_y, width, height in sample_data:
        top_left_x = center_x - width / 2
        top_left_y = center_y - height / 2

        rect = patches.Rectangle((top_left_x * RES_WIDTH, top_left_y * RES_HEIGHT), width * RES_WIDTH, height * RES_HEIGHT, linewidth=0.8, edgecolor='white', facecolor='none')
        ax.add_patch(rect)
    
    plt.show()

draw_bounding_boxes(normalized_anchor, 15)

```

``` python
import tensorflow as tf

class LabelEncoder:
    def __init__(self):
        # AnchorBox 인스턴스 생성
        self._anchor_box = AnchorBox()
        # 바운딩 박스 분산 값 설정
        self._box_variance = tf.convert_to_tensor([0.1, 0.1, 0.2, 0.2], dtype=tf.float32)

    def _match_anchor_boxes(self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.5):
        # 앵커 박스와 실제 바운딩 박스 간의 IoU 계산
        iou_matrix = compute_iou(anchor_boxes, gt_boxes)
        # 각 앵커 박스에 대해 가장 높은 IoU 값 계산
        max_iou = tf.reduce_max(iou_matrix, axis=1)
        # 각 앵커 박스에 대해 가장 높은 IoU를 가진 실제 바운딩 박스의 인덱스 계산
        matched_gt_idx = tf.argmax(iou_matrix, axis=1)
        # 양성 샘플 마스크 계산 (IoU가 match_iou 이상인 앵커 박스)
        positive_mask = tf.greater_equal(max_iou, match_iou)
        # 부정 샘플 마스크 계산 (IoU가 ignore_iou 미만인 앵커 박스)
        negative_mask = tf.less(max_iou, ignore_iou)
        # 무시할 샘플 마스크 계산 (양성도 부정도 아닌 앵커 박스)
        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))
        return matched_gt_idx, max_iou, positive_mask, ignore_mask, negative_mask

    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):
        # 앵커 박스와 매칭된 실제 바운딩 박스 간의 오프셋 계산
        box_target = tf.concat([
            (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],
            tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:])
        ], axis=-1)
        # 바운딩 박스 분산 값으로 나누어 정규화
        box_target = box_target / self._box_variance
        return box_target

    def _encode_sample(self, image_shape, gt_boxes, cls_ids):
        # 이미지 크기에 맞는 앵커 박스 생성
        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])
        # 앵커 박스 좌표를 이미지 크기로 정규화
        xmin = anchor_boxes[:, 0] / RES_WIDTH
        ymin = anchor_boxes[:, 1] / RES_HEIGHT
        xmax = anchor_boxes[:, 2] / RES_WIDTH
        ymax = anchor_boxes[:, 3] / RES_HEIGHT
        normalized_anchor = tf.stack([xmin, ymin, xmax, ymax], axis=-1)
        # 클래스 ID를 실수형으로 변환
        cls_ids = tf.cast(cls_ids, dtype=tf.float32)
        # 앵커 박스와 실제 바운딩 박스 간의 매칭 정보 계산
        matched_gt_idx, iou_values, positive_mask, ignore_mask, negative_mask = self._match_anchor_boxes(normalized_anchor, gt_boxes)
        # 매칭된 실제 바운딩 박스 가져오기
        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)
        # 바운딩 박스 타깃 값 계산
        box_target = self._compute_box_target(normalized_anchor, matched_gt_boxes)
        # 매칭된 실제 클래스 ID 가져오기
        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)
        # 클래스 타깃 값 계산 (양성 샘플은 실제 클래스 ID, 그 외는 0)
        cls_target = tf.where(positive_mask, matched_gt_cls_ids, tf.zeros_like(matched_gt_cls_ids))
        # 무시할 샘플은 클래스 타깃 값을 0.5로 설정
        cls_target = tf.where(ignore_mask, 0.5 * tf.ones_like(cls_target), cls_target)
        # 클래스 타깃 값을 차원 확장
        cls_target = tf.expand_dims(cls_target, axis=-1)
        # IoU 타깃 값 계산 (양성 샘플은 실제 IoU 값, 무시할 샘플은 0.5, 그 외는 0)
        iou_target = tf.expand_dims(tf.where(tf.cast(positive_mask, tf.bool), iou_values, 
                                             tf.where(tf.cast(ignore_mask, tf.bool), iou_values * 0.5, 
                                                      tf.zeros_like(iou_values))), axis=-1)
        # 바운딩 박스 타깃, 클래스 타깃, IoU 타깃을 연결하여 최종 레이블 생성
        label = tf.concat([box_target, cls_target, iou_target], axis=-1)
        return label

    def encode_batch(self, batch_images, gt_boxes, cls_ids):
        # 배치 이미지의 모양 가져오기
        images_shape = tf.shape(batch_images)
        # 배치 크기 가져오기
        batch_size = images_shape[0]
        # 레이블을 저장할 TensorArray 생성
        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)
        # 배치의 각 샘플에 대해 레이블 인코딩 수행
        for i in range(batch_size):
            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])
            labels = labels.write(i, label)
        # 인코딩된 배치 이미지와 레이블 반환
        return batch_images, labels.stack()
```

``` python
label_encoder = LabelEncoder()

autotune = tf.data.AUTOTUNE
train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)


autotune = tf.data.AUTOTUNE
val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)

train_dataset = train_dataset.map(
    label_encoder.encode_batch, num_parallel_calls=autotune
)

val_dataset = val_dataset.map(
    label_encoder.encode_batch, num_parallel_calls=autotune
)
```

``` python
positive_count = []
negative_count = []
ignore_count = []
for batch in train_dataset.take(10):
    images, labels = batch
    print(np.array(images).max(), np.array(images).min())
    print(labels.shape)

    # labels 텐서에서 positive, negative, ignore 값의 개수를 계산
    positive_mask = tf.equal(labels[0, :, 4], 1.0)
    positive_count = tf.reduce_sum(tf.cast(positive_mask, tf.int32))
    ignore_count = tf.reduce_sum(tf.cast(tf.equal(labels[0, :, 4], 0.5), tf.int32))
    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[0, :, 4], 0.0), tf.int32))

    print("Positive 개수:", positive_count.numpy())
    print("Negative 개수:", negative_count.numpy())
    print("Ignore 개수:", ignore_count.numpy())

    if positive_count > 0:
        positive_labels = tf.boolean_mask(labels[0], positive_mask)
        positive_iou_scores = positive_labels[:, 5]
        print("Positive IOU Scores:", positive_iou_scores.numpy())

positive_count = []
negative_count = []
ignore_count = []
for batch in val_dataset.take(10):
    images, labels = batch
    print(np.array(images).max(), np.array(images).min())
    print(labels.shape)

    # labels 텐서에서 positive, negative, ignore 값의 개수를 계산
    positive_mask = tf.equal(labels[0, :, 4], 1.0)
    positive_count = tf.reduce_sum(tf.cast(positive_mask, tf.int32))
    ignore_count = tf.reduce_sum(tf.cast(tf.equal(labels[0, :, 4], 0.5), tf.int32))
    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[0, :, 4], 0.0), tf.int32))

    print("Positive 개수:", positive_count.numpy())
    print("Negative 개수:", negative_count.numpy())
    print("Ignore 개수:", ignore_count.numpy())

    if positive_count > 0:
        positive_labels = tf.boolean_mask(labels[0], positive_mask)
        positive_iou_scores = positive_labels[:, 5]
        print("Positive IOU Scores:", positive_iou_scores.numpy())
```

``` python
def decode_predictions(labels, anchors, box_variance=[0.1, 0.1, 0.2, 0.2]):
    decoded_boxes = []
    label_idx = 0
    for label in labels:
        # 라벨에서 바운딩 박스 오프셋 추출
        dx, dy, dw, dh = label[:4]
        # 해당 라벨에 대응하는 앵커 박스 추출
        anchor = anchors[label_idx]
        anchor_x, anchor_y, anchor_w, anchor_h = anchor
        # 바운딩 박스 오프셋을 이용하여 실제 바운딩 박스 좌표 계산
        cx = dx * box_variance[0] * anchor_w + anchor_x
        cy = dy * box_variance[1] * anchor_h + anchor_y
        width = np.exp(dw * box_variance[2]) * anchor_w
        height = np.exp(dh * box_variance[3]) * anchor_h
        x_min = cx - width / 2
        y_min = cy - height / 2
        # 디코딩된 바운딩 박스 정보 저장 (좌표, 너비, 높이, Positive/Ignore/Negative 여부)
        decoded_box = [x_min, y_min, width, height, label[4]]  # label[4]는 해당 앵커 박스가 Positive, Ignore, Negative 중 어떤 것인지를 나타냅니다.
        decoded_boxes.append(decoded_box)
        label_idx += 1
    return decoded_boxes

def draw_bounding_boxes(image, decoded_boxes):
    plt.figure(figsize=(5, 5))
    plt.imshow(image)
    ax = plt.gca()
    i = 0
    for box in decoded_boxes:
        if box[4] >= 0.5:  # Positive인 경우에만 그립니다.
            i += 1
            x_min, y_min, width, height = box[:4]
            # 바운딩 박스 그리기
            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=1, edgecolor='r', facecolor='none')
            ax.add_patch(rect)
    print(i)
    plt.show()

# 앵커 박스 생성
anchor_box = AnchorBox()
anchors = anchor_box.get_anchors(96, 128)

# 학습 데이터셋에서 첫 번째 배치 추출
for batch in train_dataset.take(1):
    image = batch[0][0].numpy()
    labels = batch[1][0].numpy()
    print("Labels shape:", labels.shape)
    # Positive, Negative, Ignore 개수 계산
    positive_count = tf.reduce_sum(tf.cast(tf.greater_equal(labels[:, 4], 0.5), tf.int32))
    ignore_count = tf.reduce_sum(tf.cast(tf.logical_and(tf.less(labels[:, 4], 0.5), tf.greater(labels[:, 4], 0.0)), tf.int32))
    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, 4], 0.0), tf.int32))
    print("Positive 개수:", positive_count.numpy())
    print("Negative 개수:", negative_count.numpy())
    print("Ignore 개수:", ignore_count.numpy())
    # 바운딩 박스 디코딩
    decoded_boxes = decode_predictions(labels, anchors)
    # 이미지에 바운딩 박스 그리기
    draw_bounding_boxes(image, decoded_boxes)
```

``` python
def decode_predictions(labels, anchors, box_variance=[0.1, 0.1, 0.2, 0.2]):
    decoded_boxes = []
    label_idx = 0
    for label in labels:
        dx, dy, dw, dh = label[:4]
        anchor = anchors[label_idx]
        anchor_x, anchor_y, anchor_w, anchor_h = anchor
        cx = dx * box_variance[0] * anchor_w + anchor_x
        cy = dy * box_variance[1] * anchor_h + anchor_y
        width = np.exp(dw * box_variance[2]) * anchor_w
        height = np.exp(dh * box_variance[3]) * anchor_h
        x_min = cx - width / 2
        y_min = cy - height / 2
        decoded_box = [x_min, y_min, width, height, label[4]]  # label[4]는 해당 앵커 박스가 Positive, Ignore, Negative 중 어떤 것인지를 나타냅니다.
        decoded_boxes.append(decoded_box)
        label_idx += 1
    return decoded_boxes

def draw_bounding_boxes(image, decoded_boxes):
    plt.figure(figsize=(5, 5))
    plt.imshow(image)
    ax = plt.gca()
    i = 0
    for box in decoded_boxes:
        if box[4] >= 0.5:  # Positive인 경우에만 그립니다.
            i += 1
            x_min, y_min, width, height = box[:4]
            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=1, edgecolor='r', facecolor='none')
            ax.add_patch(rect)
    print(i)
    plt.show()

anchor_box = AnchorBox()
anchors = anchor_box.get_anchors(96, 128)

for batch in val_dataset.take(1):
    image = batch[0][0].numpy()
    labels = batch[1][0].numpy()
    print("Labels shape:", labels.shape)
    positive_count = tf.reduce_sum(tf.cast(tf.greater_equal(labels[:, 4], 0.5), tf.int32))
    ignore_count = tf.reduce_sum(tf.cast(tf.logical_and(tf.less(labels[:, 4], 0.5), tf.greater(labels[:, 4], 0.0)), tf.int32))
    negative_count = tf.reduce_sum(tf.cast(tf.equal(labels[:, 4], 0.0), tf.int32))
    print("Positive 개수:", positive_count.numpy())
    print("Negative 개수:", negative_count.numpy())
    print("Ignore 개수:", ignore_count.numpy())
    decoded_boxes = decode_predictions(labels, anchors)
    draw_bounding_boxes(image, decoded_boxes)
```

``` python
from tensorflow.keras import regularizers
class DepthwiseSeparableConv(layers.Layer):
    def __init__(self, out_channels, kernel_size, stride=1, padding=0):
        super(DepthwiseSeparableConv, self).__init__()
        self.depthwise = layers.DepthwiseConv2D(kernel_size=kernel_size, padding='same' if padding else 'valid', depth_multiplier=1, strides=stride, kernel_regularizer=regularizers.l2(0.005))
        self.pointwise = layers.Conv2D(out_channels, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.005))

    def call(self, x):
        out = self.depthwise(x)
        out = self.pointwise(out)
        return out
    
    def get_prunable_weights(self):
        return self.trainable_weights

class DepthwiseConv(layers.Layer):
    def __init__(self, out_channels, kernel_size=3, stride=1, padding=0):
        super(DepthwiseConv, self).__init__()
        self.depthwise = DepthwiseSeparableConv(out_channels, kernel_size, stride, padding)
        self.batch_norm = layers.BatchNormalization()
        self.relu = layers.Activation('silu')

    def call(self, x):
        out = self.depthwise(x)
        out = self.batch_norm(out)
        out = self.relu(out)
        return out
    
    def get_prunable_weights(self):
        return self.trainable_weights

class Conv(layers.Layer):
    def __init__(self, out_channels, kernel_size=3, stride=2, padding='SAME', kernel_regularizer=regularizers.l2(0.005)):
        super(Conv, self).__init__()
        self.conv = layers.Conv2D(out_channels, kernel_size, strides=stride, padding=padding, kernel_regularizer=regularizers.l2(0.005))
        self.batch_norm = layers.BatchNormalization()
        self.relu = layers.Activation('silu')

    def call(self, x):
        x = self.conv(x)
        x = self.batch_norm(x)
        return self.relu(x)
    
    def get_prunable_weights(self):
        return self.trainable_weights

class BackBone(tf.keras.layers.Layer):
    def __init__(self):
        super(BackBone, self).__init__()
        self.L0padding = layers.ZeroPadding2D(padding = ((2, 2),(2, 2)))
        self.L0conv1 = Conv(out_channels=8, kernel_size=3, stride=1, padding='valid')   # 128

        self.L1conv2 = Conv(out_channels=16, kernel_size=1, stride=2, padding='valid')
        self.L1conv3 = Conv(out_channels=32, kernel_size=3, stride=1, padding='valid')
        self.L1_maxpool = layers.MaxPool2D((1, 1))
        self.L1dconv4 = Conv(16, kernel_size=1, stride=1, padding='valid', kernel_regularizer=regularizers.l2(0.005))           
        self.L1dconv5 = DepthwiseConv(out_channels=16, kernel_size=3, stride=1, padding='valid') # 16       
    

        self.L3conv1 = Conv(out_channels=64, kernel_size=1, stride=2, padding='valid')  # 32
        self.L3conv2 = DepthwiseConv(out_channels=64, kernel_size=3, stride=1, padding='valid')
        self.L3_maxpool = layers.MaxPool2D((1, 1))
        self.L3dconv4 = Conv(32, kernel_size=1, stride=1, padding='valid', kernel_regularizer=regularizers.l2(0.005))             
        self.L3dconv5 = DepthwiseConv(out_channels=32, kernel_size=3, stride=1, padding='valid')


        self.L4conv1 = Conv(out_channels=64, kernel_size=1, stride=2, padding='valid')  # 16
        self.L4conv2 = DepthwiseConv(out_channels=64, kernel_size=3, stride=1, padding='valid')
        self.L4_maxpool = layers.MaxPool2D((1, 1))
        self.L4dconv4 = Conv(32, kernel_size=1, stride=1, padding='valid', kernel_regularizer=regularizers.l2(0.005))             
        self.L4dconv5 = DepthwiseConv(out_channels=32, kernel_size=3, stride=1, padding='valid')


        self.L5conv1 = Conv(out_channels=64, kernel_size=1, stride=2, padding='valid')  # 8
        self.L5conv2 = DepthwiseConv(out_channels=64, kernel_size=3, stride=1, padding='valid')
        self.L5_maxpool = layers.MaxPool2D((1, 1))
        self.L5dconv4 = Conv(32, kernel_size=1, stride=1, padding='valid', kernel_regularizer=regularizers.l2(0.005))             
        self.L5dconv5 = DepthwiseConv(out_channels=32, kernel_size=3, stride=1, padding='valid')

        


    def call(self, inputs):
        L0_padding = self.L0padding(inputs)
        L0_conv1 = self.L0conv1(L0_padding)


        L1_conv2 = self.L1conv2(L0_conv1)
        L1_conv3 = self.L1conv3(L1_conv2)
        L1_x1, L1_x2 = tf.split(L1_conv3, num_or_size_splits=2, axis=-1)
        
        L1_reduce_noise = self.L1_maxpool(L1_x1)
        L1_dconv4 = self.L1dconv4(L1_reduce_noise) 
        L1_dconv5 = self.L1dconv5(L1_dconv4)

        
        
        L3_conv1 = self.L3conv1((L1_dconv5 * L1_reduce_noise) + L1_x2)
        L3_conv2 = self.L3conv2(L3_conv1)
        L3_x1, L3_x2 = tf.split(L3_conv2, num_or_size_splits=2, axis=-1)

        L3_reduce_noise = self.L3_maxpool(L3_x1)
        L3_dconv4 = self.L3dconv4(L3_reduce_noise)
        L3_dconv5 = self.L3dconv5(L3_dconv4)

        out1 = (L3_dconv5 * L3_reduce_noise) + L3_x2



        L4_conv1 = self.L4conv1(out1)
        L4_conv2 = self.L4conv2(L4_conv1)
        L4_x1, L4_x2 = tf.split(L4_conv2, num_or_size_splits=2, axis=-1)

        L4_reduce_noise = self.L4_maxpool(L4_x1)
        L4_dconv4 = self.L4dconv4(L4_reduce_noise)
        L4_dconv5 = self.L4dconv5(L4_dconv4)

        out2 = (L4_dconv5 * L4_reduce_noise) + L4_x2



        L5_conv1 = self.L5conv1(out2)
        L5_conv2 = self.L5conv2(L5_conv1)
        L5_x1, L5_x2 = tf.split(L5_conv2, num_or_size_splits=2, axis=-1)

        L5_reduce_noise = self.L5_maxpool(L5_x1)
        L5_dconv4 = self.L5dconv4(L5_reduce_noise)
        L5_dconv5 = self.L5dconv5(L5_dconv4)

        out3 = (L5_dconv5 * L5_reduce_noise) + L5_x2

        return out1, out2, out3

class FPN(tf.keras.layers.Layer):
    def __init__(self, out_channels=72):
        super(FPN, self).__init__()
        self.out_channels = out_channels
        
        self.conv1x1_c5 = Conv(out_channels, kernel_size=1, stride=1, padding='same')
        self.conv1x1_c4 = Conv(out_channels, kernel_size=1, stride=1, padding='same')
        self.conv1x1_c3 = Conv(out_channels, kernel_size=1, stride=1, padding='same')
        
        self.conv3x3_p5 = DepthwiseConv(out_channels, kernel_size=3, stride=1, padding='same')
        self.conv3x3_p4 = DepthwiseConv(out_channels, kernel_size=3, stride=1, padding='same')
        self.conv3x3_p3 = DepthwiseConv(out_channels, kernel_size=3, stride=1, padding='same')
        
        self.upsample_p5 = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')
        self.upsample_p4 = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')
        
    def call(self, c3, c4, c5):
        p5 = self.conv1x1_c5(c5)
        p5_upsampled = self.upsample_p5(p5)
        
        p4 = self.conv1x1_c4(c4)
        p4 = layers.Add()([p5_upsampled, p4])
        p4_upsampled = self.upsample_p4(p4)
        
        p3 = self.conv1x1_c3(c3)
        p3 = layers.Add()([p4_upsampled, p3])
        
        p5 = self.conv3x3_p5(p5)
        p4 = self.conv3x3_p4(p4)
        p3 = self.conv3x3_p3(p3)
        
        return p3, p4, p5

num_anchors_per_location = 3

def DetectionModel(num_classes=1, num_anchors_per_location=num_anchors_per_location):
    inputs = tf.keras.Input(shape=(96, 128, 1))
    iou_score = num_classes
    # Backbone
    backbone = BackBone()
    p1, p2, p3 = backbone(inputs)

    # FPN
    fpn = FPN()
    p1_out, p2_out, p3_out = fpn(p1, p2, p3)

    # Detection Head
    detection_head = tf.keras.Sequential([
        DepthwiseSeparableConv(128, 3, stride=1, padding='same'),
        layers.Conv2D(num_anchors_per_location * (4 + num_classes), kernel_size=1, strides=1, padding='same') 
    ])

    outputs = []
    for feature in [p1_out, p2_out, p3_out]:
        output = detection_head(feature)
        H, W = tf.shape(feature)[1], tf.shape(feature)[2]
        num_anchors = H * W * num_anchors_per_location
        output = tf.keras.layers.Reshape((-1, 4 + num_classes))(output)
        outputs.append(output)

    final_output = tf.keras.layers.Concatenate(axis=1)(outputs)
    print(final_output.shape)
    
    model = tf.keras.Model(inputs=inputs, outputs=final_output, name='DetectionModel')
    return model
```

``` python
model = DetectionModel(num_classes=1, num_anchors_per_location=num_anchors_per_location)
model.trainable = True
model.build(input_shape=(None, 96, 128, 1))
model.summary()
```

``` python
class BoxLoss(tf.losses.Loss):
    def __init__(self, delta):
        super(BoxLoss, self).__init__(reduction="none", name="BoxLoss")
        self._delta = delta

    def call(self, y_true, y_pred):
        difference = y_true - y_pred
        absolute_difference = tf.abs(difference)
        squared_difference = difference ** 2
        loss = tf.where(
            tf.less(absolute_difference, self._delta),
            0.5 * squared_difference,
            absolute_difference - 0.5 * self._delta
        )
        loss = tf.reduce_sum(loss, axis=-1)
        return loss

    
# alpha : Recall에 영향 Beta: Precision에 영향
class ConfidenceLoss(tf.losses.Loss):
    def __init__(self, alpha=3.0, beta=1.0):
        super(ConfidenceLoss, self).__init__(reduction="none", name="ConfidenceLoss")
        self._alpha = alpha
        self._beta = beta

    def call(self, y_true, y_pred):
        confidence = tf.nn.sigmoid(y_pred)
        positive_mask = tf.cast(tf.greater(y_true, 0.0), dtype=tf.float32)
        negative_mask = 1.0 - positive_mask
        positive_loss = -self._alpha * positive_mask * tf.math.log(confidence + 1e-15)
        negative_loss = -self._beta * negative_mask * tf.math.log(1.0 - confidence + 1e-15)
        loss = positive_loss + negative_loss
        loss = tf.reduce_sum(loss, axis=-1)
        return loss
    
    
#  Alpha 값을 높이면 Recall 증가
#  Alpha 값을 낮추면 Precision 증가
# Gamma 값이 커질수록 잘못 분류된 예제에 더 큰 가중치 부여
class ClassificationLoss(tf.losses.Loss):
    def __init__(self, alpha, gamma):
        super(ClassificationLoss, self).__init__(reduction="none", name="ClassificationLoss")
        self._alpha = alpha
        self._gamma = gamma

    def call(self, y_true, y_pred):
        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)
        probs = tf.nn.sigmoid(y_pred)
        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))
        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)
        loss = alpha * tf.math.pow(1.0 - pt + 1e-6, self._gamma) * cross_entropy
        loss = tf.reduce_sum(loss, axis=-1)
        return loss

    
class Loss(tf.losses.Loss):
    def __init__(self, num_classes=1, delta=1.0, neg_pos_ratio=3.0, hard_pos_ratio = 3.0):
        super(Loss, self).__init__(reduction="auto", name="Loss")
        self._cls_loss = ClassificationLoss(0.5, 4)
        self._box_loss = BoxLoss(delta)
        self._confidence_loss = ConfidenceLoss()
        self._num_classes = num_classes
        self._neg_pos_ratio = neg_pos_ratio
        self._hard_pos_ratio = hard_pos_ratio

    def call(self, y_true, y_pred):
        y_pred = tf.cast(y_pred, dtype=tf.float32)
        box_labels = y_true[:, :, :4]
        box_predictions = y_pred[:, :, :4]
        cls_labels = y_true[:, :, 4:]
        cls_predictions = y_pred[:, :, 4:]

        positive_mask = tf.cast(tf.math.equal(y_true[:, :, 4], 1.0), dtype=tf.float32)
        ignore_mask = tf.cast(tf.math.equal(y_true[:, :, 4], 0.5), dtype=tf.float32)
        negative_mask =  tf.cast(tf.math.equal(y_true[:, :, 4], 0.0), dtype=tf.float32)

        cls_loss = self._cls_loss(cls_labels, cls_predictions)
        confidence_loss = self._confidence_loss(cls_labels, cls_predictions)

        box_loss = self._box_loss(box_labels, box_predictions)

        num_positives = tf.reduce_sum(positive_mask)

        # Hard Negative Mining
        negative_cls_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, confidence_loss * negative_mask)
        num_positives = tf.reduce_sum(positive_mask)
        num_hard_negatives = tf.cast(self._neg_pos_ratio * num_positives, dtype=tf.int32)
        negative_cls_loss = tf.reshape(negative_cls_loss, shape=(-1,))
        top_k_negative_cls_loss, _ = tf.math.top_k(negative_cls_loss, k=num_hard_negatives)
        hard_negative_mask = tf.cast(negative_cls_loss >= top_k_negative_cls_loss[-1], dtype=tf.float32)
        hard_negative_mask = tf.reshape(hard_negative_mask, shape=tf.shape(positive_mask))


        # Hard Positive Mining
        positive_cls_loss = tf.where(tf.equal(positive_mask, 1.0), confidence_loss, 0.0)
        num_hard_positives = tf.cast(self._hard_pos_ratio * num_positives, dtype=tf.int32)
        positive_cls_loss = tf.reshape(positive_cls_loss, shape=(-1,))
        top_k_positive_cls_loss, _ = tf.math.top_k(positive_cls_loss, k=num_hard_positives)
        hard_positive_mask = tf.cast(positive_cls_loss >= top_k_positive_cls_loss[-1], dtype=tf.float32)
        hard_positive_mask = tf.reshape(hard_positive_mask, shape=tf.shape(positive_mask)) # Precision 증가


        cls_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, cls_loss)
        confidence_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, confidence_loss)
        
        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)
        
        normalizer_positive = tf.reduce_sum(positive_mask, axis=-1) + 1e-6
        normalizer_negative = tf.reduce_sum(negative_mask, axis=-1) + 1e-6
        normalizer_hard_positive = tf.reduce_sum(hard_positive_mask, axis=-1) + 1e-6
        normalizer_hard_negative = tf.reduce_sum(hard_negative_mask, axis=-1) + 1e-6
        
        cls_loss_positive = tf.math.divide_no_nan(tf.reduce_sum(cls_loss * (positive_mask + negative_mask), axis=-1), normalizer_positive + normalizer_negative) 
        confidence_loss_positive = tf.math.divide_no_nan(tf.reduce_sum(confidence_loss * (hard_positive_mask + hard_negative_mask), axis=-1), normalizer_hard_positive + normalizer_hard_negative)
        
        box_loss_positive = tf.math.divide_no_nan(tf.reduce_sum(box_loss * positive_mask, axis=-1), normalizer_positive)

        loss = box_loss_positive + cls_loss_positive + confidence_loss_positive
        return loss
```

``` python
import tensorflow as tf
from tensorflow.keras import backend as K
class CustomRecall(tf.keras.metrics.Metric):
    def __init__(self, name='Recall', threshold=0.5, **kwargs):
        super(CustomRecall, self).__init__(name=name, **kwargs)
        self.threshold = threshold
        self.true_positives = self.add_weight(name='tp', initializer='zeros')
        self.actual_positives = self.add_weight(name='actual_positives', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        # IoU 기준으로 레이블 결정 (0.5 기준)
        labels = tf.cast(y_true[:, :, 4:5] >= self.threshold, tf.int32)
        probabilities = tf.nn.sigmoid(y_pred[:, :, 4:5])
        pred_positives = tf.cast(probabilities > self.threshold, self.dtype)

        true_positives = tf.logical_and(labels == 1, tf.cast(pred_positives, tf.bool))
        true_positives = tf.cast(true_positives, self.dtype)
        self.true_positives.assign_add(tf.reduce_sum(true_positives))
        
        actual_positives = tf.cast(labels, self.dtype)
        self.actual_positives.assign_add(tf.reduce_sum(actual_positives))

    def result(self):
        return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())


class CustomPrecision(tf.keras.metrics.Metric):
    def __init__(self, name='Precision', threshold=0.5, **kwargs):
        super(CustomPrecision, self).__init__(name=name, **kwargs)
        self.threshold = threshold
        self.true_positives = self.add_weight(name='tp', initializer='zeros')
        self.predicted_positives = self.add_weight(name='predicted_positives', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        # IoU 기준으로 레이블 결정 (0.5 기준)
        labels = tf.cast(y_true[:, :, 4:5] >= self.threshold, tf.int32)
        probabilities = tf.nn.sigmoid(y_pred[:, :, 4:5])
        pred_positives = tf.cast(probabilities > self.threshold, self.dtype)

        true_positives = tf.logical_and(labels == 1, tf.cast(pred_positives, tf.bool))
        true_positives = tf.cast(true_positives, self.dtype)
        self.true_positives.assign_add(tf.reduce_sum(true_positives))
        self.predicted_positives.assign_add(tf.reduce_sum(pred_positives))

    def result(self):
        return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())


class F1Score(tf.keras.metrics.Metric):
    def __init__(self, name='F1Score', threshold=0.5, **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.threshold = threshold
        self.precision = CustomPrecision(threshold=threshold)
        self.recall = CustomRecall(threshold=threshold)

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)

    def result(self):
        precision = self.precision.result()
        recall = self.recall.result()
        f1_score = 2 * ((precision * recall) / (precision + recall + K.epsilon()))
        return f1_score

    def reset_state(self):
        self.precision.reset_state()
        self.recall.reset_state()


class CustomClassAccuracy(tf.keras.metrics.Metric):
    def __init__(self, name='Accuracy', threshold=0.5, **kwargs):
        super(CustomClassAccuracy, self).__init__(name=name, **kwargs)
        self.threshold = threshold
        self.true_positives = self.add_weight(name='tp', initializer='zeros')
        self.true_negatives = self.add_weight(name='tn', initializer='zeros')
        self.total_samples = self.add_weight(name='total_samples', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        # 클래스에 대한 레이블 결정 (0.5 기준)
        labels = tf.cast(y_true[:, :, 4:5] > 0.5, tf.int32)
        probabilities = tf.cast(tf.nn.sigmoid(y_pred[:, :, 4:5]) > self.threshold, tf.int32)

        true_positives = tf.logical_and(labels == 1, probabilities == 1)
        true_positives = tf.cast(true_positives, self.dtype)
        self.true_positives.assign_add(tf.reduce_sum(true_positives))

        true_negatives = tf.logical_and(labels == 0, probabilities == 0)
        true_negatives = tf.cast(true_negatives, self.dtype)
        self.true_negatives.assign_add(tf.reduce_sum(true_negatives))

        total_samples = tf.size(labels)
        self.total_samples.assign_add(tf.cast(total_samples, self.dtype))

    def result(self):
        return (self.true_positives + self.true_negatives) / (self.total_samples + tf.keras.backend.epsilon())
```

``` python
new_batch_size = 200

# 기존 데이터셋에서 배치 사이즈를 새로운 값으로 변경
train_dataset = train_dataset.unbatch()  # 먼저, 기존 배치를 해제
train_dataset = train_dataset.shuffle(buffer_size=1000)  # 데이터셋 shuffle
train_dataset = train_dataset.batch(new_batch_size, drop_remainder=True)  # 새로운 배치 사이즈로 재배치


train_data_count = 0
for images, labels in train_dataset:
    train_data_count += images.shape[0]

print(f"Batch Images shape: {images.shape}")
print(f"Batch Labels shape: {labels.shape}")
print(f"Images max: {tf.reduce_max(images)}")
print(f"Images min: {tf.reduce_min(images)}")
print("Labels shape:", labels[:, :, :4].shape)

print(f"Total data count: {train_data_count}")
```

``` python
new_batch_size = 200

# 기존 데이터셋에서 배치 사이즈를 새로운 값으로 변경
val_dataset = val_dataset.unbatch()  # 먼저, 기존 배치를 해제
val_dataset = val_dataset.shuffle(buffer_size=1000)  # 데이터셋 shuffle
val_dataset = val_dataset.batch(new_batch_size, drop_remainder=True)  # 새로운 배치 사이즈로 재배치


val_data_count = 0
for images, labels in val_dataset:
    val_data_count += images.shape[0]

print(f"Batch Images shape: {images.shape}")
print(f"Batch Labels shape: {labels.shape}")
print(f"Images max: {tf.reduce_max(images)}")
print(f"Images min: {tf.reduce_min(images)}")
print("Labels shape:", labels[:, :, :4].shape)    

print(f"Total data count: {val_data_count}")
```

``` python
from tensorflow.keras.callbacks import TensorBoard

log_dir = 'Log/ObjectDetectionV4/'
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)
```

``` python
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

initial_learning_rate = 0.0003

early_stopping_callback = EarlyStopping(
    monitor='F1Score',  # 모니터링할 메트릭
    min_delta=0,         # 개선으로 간주할 메트릭 변화의 최소 폭
    patience=10,             # 성능 향상이 없더라도 몇 에포크를 기다릴지
    verbose=1,              # 메시지 출력
    mode='max',             # F1 스코어를 최대화하는 것이 목표
    baseline=0.95,          # 목표로 하는 성능 수치 (95% 이상)
    restore_best_weights=True  # 가장 좋은 가중치를 저장했다가 복원
)

reduce_lr_callback = ReduceLROnPlateau(
    monitor='val_F1Score',  # 모니터링할 메트릭 (e.g., val_loss, val_accuracy)
    factor=0.5,          # learning rate를 얼마나 줄일지 (e.g., 0.5는 half)
    patience=3,          # 성능 향상이 없더라도 몇 에포크를 기다릴지
    verbose=1,           # 메시지 출력
    mode='max',          # 메트릭을 최소화하는 것이 목표임을 명시 (또는 acc라면 max)
    min_delta=1e-3,      # 개선으로 간주할 메트릭 변화의 최소 폭
    cooldown=0,          # learning rate 감소 후 정상 학습 재개까지 걸리는 에포크 수
    min_lr=1e-8          # learning rate의 하한선
)

num_classes = 1
loss_fn = Loss(num_classes = 1)
anchor_box = AnchorBox()
anchors = anchor_box.get_anchors(96, 128)

optimizer = Adam(learning_rate=initial_learning_rate, clipnorm=1.0) #0.25
model.compile(optimizer=optimizer, loss=[loss_fn], metrics=[F1Score(), CustomPrecision(), CustomRecall()])

epochs = 100
model.fit(
    train_dataset,
    validation_data=val_dataset,  
    epochs=epochs,
    callbacks=[reduce_lr_callback, early_stopping_callback, tensorboard_callback],
    verbose=1,
)
```

``` python
import numpy as np

datasets = np.load('dataset/ObjectDetection.npz', allow_pickle=True)
images, numbers, bboxes = datasets['images'], datasets['numbers'], datasets['bboxes']

max_label_length = 4
labels = []
for num in numbers:
    cls = [1] * num if num != 0 else [0]
    cls += [0] * (max_label_length - len(cls))
    labels.append(cls)

# labels = np.array(labels)

# non_zero_indices = np.where(numbers != 0)[0]
non_zero_indices = np.where(numbers > 0)[0]

# numbers가 0이 아닌 항목만 유지
images_filtered = images[non_zero_indices]
bboxes_filtered = bboxes[non_zero_indices]
labels_filtered = np.array(labels)[non_zero_indices]
numbers_filters = numbers[non_zero_indices]
print(images.shape, numbers.shape, bboxes.shape, len(labels))

print(images.max(), images.min())

dataset = {
    'images' : images_filtered,
    'bboxes' : bboxes_filtered,
    'class' : labels_filtered,
    'numbers': numbers_filters
}

target_size = (128, 96)  # 원하는 이미지 크기로 설정 (width, height)
dataset = resize_and_adjust_dataset(dataset, target_size)

print(dataset['images'][0].shape)
print(dataset['bboxes'].shape)
print(len(dataset['class']))
print(dataset['class'])
```

``` python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import tensorflow as tf

def iou(box1, box2):
    # 두 바운딩 박스의 IoU 계산
    x1, y1, x2, y2 = box1
    x3, y3, x4, y4 = box2

    x_overlap = max(0, min(x2, x4) - max(x1, x3))
    y_overlap = max(0, min(y2, y4) - max(y1, y3))

    intersection = x_overlap * y_overlap
    area1 = (x2 - x1) * (y2 - y1)
    area2 = (x4 - x3) * (y4 - y3)
    union = area1 + area2 - intersection

    return intersection / union

def decode_predictions(predictions, anchors, box_variance=[0.1, 0.1, 0.2, 0.2], iou_threshold=0.5, score_threshold=0.5, top_n=30):
    decoded_boxes = []
    scores = []
    for i, prediction in enumerate(predictions):
        score = tf.nn.sigmoid(prediction[-1])
        if score > score_threshold:
            scores.append((score, i))

    # 점수에 따라 내림차순 정렬
    scores.sort(reverse=True)

    # 상위 N개 선택
    scores = scores[:top_n]

    # NMS 적용
    while scores:
        score, i = scores.pop(0)
        prediction = predictions[i]
        dx, dy, dw, dh = prediction[:4]
        anchor = anchors[i]
        anchor_x, anchor_y, anchor_w, anchor_h = anchor
        cx = dx * box_variance[0] * anchor_w + anchor_x
        cy = dy * box_variance[1] * anchor_h + anchor_y
        width = np.exp(dw * box_variance[2]) * anchor_w
        height = np.exp(dh * box_variance[3]) * anchor_h
        x_min = cx - width / 2
        y_min = cy - height / 2
        decoded_box = [x_min, y_min, x_min + width, y_min + height, score]
        keep = True
        for other_box in decoded_boxes:
            if iou(decoded_box[:4], other_box[:4]) >= iou_threshold:
                keep = False
                break
        if keep:
            decoded_boxes.append(decoded_box)

    return decoded_boxes

def draw_bounding_boxes(image, boxes, class_names):
    plt.figure(figsize=(10, 10))
    plt.imshow(image, cmap='gray')  # 이미지가 grayscale인 경우 cmap='gray'를 추가합니다.
    ax = plt.gca()
    for box in boxes:
        x_min, y_min, x_max, y_max, score = box
        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

        # 박스 위에 클래스 이름과 확률 표시
        class_name = class_names[0]  # 예시로 'person' 클래스를 사용합니다.
        text = f'{class_name}'
        ax.text(x_min, y_min, text, fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))
        print(score)
    plt.axis('off')
    plt.show()

# AnchorBox 클래스와 get_anchors 함수가 필요합니다.
anchor_box = AnchorBox()
anchors = anchor_box.get_anchors(96, 128)  # 앵커 박스 생성 예시

class_names = ['person']  # 클래스 이름 리스트

img = dataset['images'][9545]  # 이미지 선택 (예시로 9545번째 이미지 사용)
print(img.shape)
img = tf.expand_dims(img, axis=0)  # 배치 차원 추가
print(img.shape)

# 입력 이미지의 데이터 타입을 float32로 변경
img = tf.cast(img, tf.float32)

img = img / np.array(img).max()  # 이미지 정규화 (0에서 1 사이의 범위로 변경)
img = (img - 0.5) * 2.0  # -1에서 1 사이의 범위로 변경
print(print(np.array(img).max()))
print(print(np.array(img).min()))
predictions = model.predict(img)[0]  # 첫 번째 이미지에 대한 예측 결과
print(predictions)
decoded_boxes = decode_predictions(predictions, anchors)  # 예측된 바운딩 박스 디코딩
draw_bounding_boxes(img[0].numpy(), decoded_boxes, class_names)  # 디코딩된 바운딩 박스를 이미지에 그리기
```

``` python
class DecodePredictions(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    def call(self, inputs):
        predictions = inputs
        
        # 앵커 박스 생성
        anchor_box = AnchorBox()
        anchors = anchor_box.get_anchors(96, 128)
        
        # 바운딩 박스 디코딩
        boxes = self.decode_boxes(predictions, anchors)
        
        # 스코어 추출
        scores = predictions[..., -1]
        
        # 바운딩 박스와 스코어 합치기
        decoded_predictions = tf.concat([boxes, tf.expand_dims(scores, axis=-1)], axis=-1)
        return decoded_predictions
    
    def decode_boxes(self, predictions, anchors):
        box_variance = [0.1, 0.1, 0.2, 0.2]
        
        dx = predictions[..., 0]
        dy = predictions[..., 1]
        dw = predictions[..., 2]
        dh = predictions[..., 3]
        
        anchor_x = anchors[..., 0]
        anchor_y = anchors[..., 1]
        anchor_w = anchors[..., 2]
        anchor_h = anchors[..., 3]
        
        cx = dx * box_variance[0] * anchor_w + anchor_x
        cy = dy * box_variance[1] * anchor_h + anchor_y
        width = tf.exp(dw * box_variance[2]) * anchor_w
        height = tf.exp(dh * box_variance[3]) * anchor_h
        
        x_min = cx - width / 2
        y_min = cy - height / 2
        x_max = x_min + width
        y_max = y_min + height
        
        boxes = tf.stack([x_min, y_min, x_max, y_max], axis=-1)
        return boxes
```

``` python
# # 추론 모델 생성
image = tf.keras.Input(shape=[96, 128, 1], name="image")
predictions = model(image, training=False)
detections = DecodePredictions()(predictions)
inference_model = tf.keras.Model(inputs=image, outputs=detections)

export_path = 'ObjectDetection/model_v10/model_v10.pb'
inference_model.save(export_path, save_format="tf")
```

``` python
import tensorflow as tf
import numpy as np


# 저장된 모델의 디렉토리 경로
saved_model_dir = 'ObjectDetection/model_v10/model_v10.pb'

# 대표 데이터셋 (양자화에 사용될 데이터셋)
representative_dataset = val_dataset

# 대표 데이터셋 생성기 함수
def representative_dataset_gen():
    for images, _ in representative_dataset:
        yield [np.array(images, dtype=np.float32)]


# TensorFlow Lite 변환기 생성
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

# 최적화 옵션 설정 (기본 최적화 사용)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# 대표 데이터셋 설정 (양자화에 사용될 데이터셋)
converter.representative_dataset = representative_dataset_gen

# 지원되는 연산 집합 설정 (TFLITE_BUILTINS_INT8: 8비트 정수 연산 사용)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

# 입력 텐서의 데이터 타입 설정 (float32)
converter.inference_input_type = tf.float32

# 출력 텐서의 데이터 타입 설정 (float32)
converter.inference_output_type = tf.float32

# 모델 변환 (양자화 적용)
tflite_model_quant = converter.convert()

# 변환된 모델을 파일로 저장
with open('ObjectDetection/model_v10/tflite/model_v10_quant.tflite', 'wb') as f:
    f.write(tflite_model_quant)

! python -m tflite_flops ObjectDetection/model_v10/tflite/model_v10_quant.tflite
```

``` python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import tensorflow as tf

def iou(box1, box2):
    x1, y1, x2, y2 = box1
    x3, y3, x4, y4 = box2

    x_overlap = max(0, min(x2, x4) - max(x1, x3))
    y_overlap = max(0, min(y2, y4) - max(y1, y3))

    intersection = x_overlap * y_overlap
    area1 = (x2 - x1) * (y2 - y1)
    area2 = (x4 - x3) * (y4 - y3)
    union = area1 + area2 - intersection

    return intersection / union

def decode_predictions(predictions, anchors, box_variance=[0.1, 0.1, 0.2, 0.2], iou_threshold=0.5, score_threshold=0.5, top_n=30):
    decoded_boxes = []
    scores = []
    for i, prediction in enumerate(predictions):
        score = tf.nn.sigmoid(prediction[-1])
        if score > score_threshold:
            scores.append((score, i))

    # 점수에 따라 내림차순 정렬
    scores.sort(reverse=True)

    # 상위 N개 선택
    scores = scores[:top_n]

    # NMS 적용
    while scores:
        score, i = scores.pop(0)
        prediction = predictions[i]
        dx, dy, dw, dh = prediction[:4]
        anchor = anchors[i]
        anchor_x, anchor_y, anchor_w, anchor_h = anchor
        cx = dx * box_variance[0] * anchor_w + anchor_x
        cy = dy * box_variance[1] * anchor_h + anchor_y
        width = np.exp(dw * box_variance[2]) * anchor_w
        height = np.exp(dh * box_variance[3]) * anchor_h
        x_min = cx - width / 2
        y_min = cy - height / 2
        decoded_box = [x_min, y_min, x_min + width, y_min + height, score]
        keep = True
        for other_box in decoded_boxes:
            if iou(decoded_box[:4], other_box[:4]) >= iou_threshold:
                keep = False
                break
        if keep:
            decoded_boxes.append(decoded_box)

    return decoded_boxes

def draw_bounding_boxes(image, boxes, class_names):
    plt.figure(figsize=(10, 10))
    plt.imshow(image, cmap='gray')  # 이미지가 grayscale인 경우 cmap='gray'를 추가합니다.
    ax = plt.gca()
    for box in boxes:
        x_min, y_min, x_max, y_max, score = box
        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

        # 박스 위에 클래스 이름과 확률 표시
        class_name = class_names[0]  # 예시로 'person' 클래스를 사용합니다.
        # text = f'{class_name}: {score / 4:.2f}'
        text = f'{class_name}'
        ax.text(x_min, y_min, text, fontsize=10, bbox=dict(facecolor='yellow', alpha=0.5))
        print(score)
    plt.axis('off')
    plt.show()

# AnchorBox 클래스와 get_anchors 함수가 필요합니다.
anchor_box = AnchorBox()
anchors = anchor_box.get_anchors(96, 128)  # 앵커 박스 생성 예시

class_names = ['person']  # 클래스 이름 리스트

img = dataset['images'][9545] # 9145 # 9545
print(img.shape)
img = tf.expand_dims(img, axis=0)
print(img.shape)

# 입력 이미지의 데이터 타입을 float32로 변경
img = tf.cast(img, tf.float32)

img = img / np.array(img).max()
img = (img - 0.5) * 2.0  # -1에서 1 사이의 범위로 변경
print(print(np.array(img).max()))
print(print(np.array(img).min()))
predictions = model.predict(img)[0]  # 첫 번째 이미지에 대한 예측 결과
# print(predictions[0].shape)
print(predictions)
decoded_boxes = decode_predictions(predictions, anchors)  # 예측된 바운딩 박스 디코딩
draw_bounding_boxes(img[0].numpy(), decoded_boxes, class_names)  # 디코딩된 바운딩 박스를 이미지에 그리기

```

```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import tensorflow as tf
import cv2


# Load dataset
datasets = np.load('dataset/ObjectDetection.npz', allow_pickle=True)
images, numbers, bboxes = datasets['images'], datasets['numbers'], datasets['bboxes']

# Filter dataset for numbers 4
max_label_length = 4
labels = []
for num in numbers:
    cls = [1] * num if num != 0 else [0]
    cls += [0] * (max_label_length - len(cls))
    labels.append(cls)

non_zero_indices = np.where(numbers == 4)[0]

images_filtered = images[non_zero_indices]
bboxes_filtered = bboxes[non_zero_indices]
labels_filtered = np.array(labels)[non_zero_indices]
numbers_filters = numbers[non_zero_indices]



# images_filtered = images_filtered[0:500]
# bboxes_filtered = bboxes_filtered[0:500]
# labels_filtered = labels_filtered[0:500]
# numbers_filters = numbers_filters[0:500]

print(len(numbers_filters))

dataset = {
    'images': images_filtered,
    'bboxes': bboxes_filtered,
    'class': labels_filtered,
    'numbers': numbers_filters
}
```

```python
target_size = (128, 96)  # Set desired image size (width, height)

def resize_and_pad_image(image, target_size):
    # Resize image (scale by 4x)
    resized_image = cv2.resize(image, (image.shape[1] * 4, image.shape[0] * 4), interpolation=cv2.INTER_NEAREST)
    # Add channel dimension if it does not exist
    if len(resized_image.shape) == 2:
        resized_image = np.expand_dims(resized_image, axis=-1)
    # Add padding to match target size
    padded_image = np.zeros((target_size[1], target_size[0], resized_image.shape[2]), dtype=resized_image.dtype)
    padded_image[:resized_image.shape[0], :resized_image.shape[1]] = resized_image
    return padded_image

def adjust_bboxes(bboxes, original_size, scale_factor):
    adjusted_bboxes = bboxes.copy()
    adjusted_bboxes[:, [0, 2]] = adjusted_bboxes[:, [0, 2]] * scale_factor
    adjusted_bboxes[:, [1, 3]] = adjusted_bboxes[:, [1, 3]] * scale_factor
    return adjusted_bboxes

def resize_and_adjust_dataset(dataset, target_size):
    images = dataset['images']
    bboxes = dataset['bboxes']
    labels = dataset['class']
    numbers = dataset['numbers']

    resized_images = []
    adjusted_bboxes = []

    scale_factor = 4  # Factor by which the image is scaled

    for image, bbox in zip(images, bboxes):
        original_size = image.shape[:2]
        
        # Resize and pad image
        resized_image = resize_and_pad_image(image, target_size)
        resized_images.append(resized_image)
        
        # Adjust bounding boxes
        adjusted_bbox = adjust_bboxes(bbox, original_size, scale_factor)
        adjusted_bboxes.append(adjusted_bbox)

    resized_images = np.array(resized_images)
    adjusted_bboxes = np.array(adjusted_bboxes)

    adjusted_dataset = {
        'images': resized_images,
        'bboxes': adjusted_bboxes,
        'class': labels,
        'numbers': numbers
    }

    return adjusted_dataset

dataset = resize_and_adjust_dataset(dataset, target_size)
```

```python
import numpy as np
import random

def calculate_iou(box1, box2):
    # 두 개의 바운딩 박스 간의 IoU(Intersection over Union) 계산
    # 바운딩 박스의 좌표를 이용하여 겹치는 영역의 면적과 전체 영역의 면적을 계산하고,
    # 겹치는 영역의 면적을 전체 영역의 면적으로 나누어 IoU 값을 구함
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    intersection_area = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)
    
    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)
    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)
    
    union_area = box1_area + box2_area - intersection_area
    
    iou = intersection_area / union_area
    
    return iou

def apply_cutmix(dataset, max_people=10, iou_threshold=0.2, mode='rand', target_size=None, new_image_size=(96, 128)):
    # CutMix 기법을 적용하여 데이터셋을 확장
    # 입력으로 받은 데이터셋에서 이미지, 바운딩 박스, 레이블, 사람 수 정보 추출
    images = dataset['images']
    bboxes = dataset['bboxes']
    labels = dataset['class']
    numbers = dataset['numbers']

    # 최대 레이블 길이(사람 수)와 새로운 이미지 크기 설정
    max_label_length = max_people
    new_image_width, new_image_height = new_image_size

    # CutMix가 적용된 이미지, 바운딩 박스, 레이블, 사람 수를 저장할 리스트 초기화
    cutmix_images = []
    cutmix_bboxes = []
    cutmix_labels = []
    cutmix_numbers = []

    # 목표 데이터셋 크기가 지정되지 않은 경우, 원본 데이터셋의 크기 사용
    if target_size is None:
        target_size = len(images)

    # 목표 데이터셋 크기만큼 반복하여 CutMix 적용
    while len(cutmix_images) < target_size:
        # 원본 데이터셋에서 무작위로 이미지 선택
        i = random.randint(0, len(images) - 1)
        image = images[i].copy()
        bbox = bboxes[i].copy()
        label = labels[i].copy()
        number = numbers[i]

        # 모드에 따라 목표 사람 수 설정
        if mode == 'all':
            target_number = max_people
        elif mode == 'rand':
            target_number = random.randint(number, max_people)
        else:
            raise ValueError("Invalid mode. Choose 'all' or 'rand'.")

        # 목표 사람 수만큼 반복하여 다른 이미지에서 사람을 선택하여 현재 이미지에 추가
        while number < target_number:
            # 사람이 있는 이미지의 인덱스 찾음
            donor_indices = np.where(numbers > 0)[0]
            if len(donor_indices) > 0:
                # 무작위로 donor 이미지 선택
                donor_idx = random.choice(donor_indices)
                donor_image = images[donor_idx]
                donor_bbox = bboxes[donor_idx]
                donor_label = labels[donor_idx]
                donor_number = numbers[donor_idx]

                # donor 이미지에서 사람이 있는 바운딩 박스의 인덱스 찾음
                non_zero_indices = np.where(donor_label != 0)[0]
                if len(non_zero_indices) > 0:
                    # 무작위로 바운딩 박스 선택
                    bbox_idx = random.choice(non_zero_indices)
                    new_bbox = donor_bbox[bbox_idx]

                    # 선택한 바운딩 박스와 현재 이미지의 바운딩 박스 간의 IoU 계산
                    iou_values = [calculate_iou(bbox[j], new_bbox) for j in range(number)]
                    # IoU가 임계값 미만인 경우에만 이미지에 추가
                    if max(iou_values) < iou_threshold:
                        x1, y1, x2, y2 = map(int, new_bbox)
                        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(new_image_width, x2), min(new_image_height, y2)

                        # donor 이미지에서 선택한 바운딩 박스 영역을 현재 이미지에 복사
                        image[y1:y2, x1:x2] = donor_image[y1:y2, x1:x2]

                        # 바운딩 박스와 레이블 업데이트
                        bbox = np.pad(bbox, ((0, max_label_length - bbox.shape[0]), (0, 0)), mode='constant')
                        bbox[number] = new_bbox
                        label = np.pad(label, (0, max_label_length - label.shape[0]), mode='constant')
                        label[number] = 1
                        number += donor_number

        # 바운딩 박스와 레이블의 길이를 최대 레이블 길이에 맞게 패딩
        bbox = np.pad(bbox, ((0, max_label_length - bbox.shape[0]), (0, 0)), mode='constant')
        label = np.pad(label, (0, max_label_length - label.shape[0]), mode='constant')

        # CutMix가 적용된 이미지, 바운딩 박스, 레이블, 사람 수를 리스트에 추가
        cutmix_images.append(image)
        cutmix_bboxes.append(bbox)
        cutmix_labels.append(label)
        cutmix_numbers.append(number)

    # CutMix가 적용된 데이터를 NumPy 배열로 변환
    cutmix_images = np.array(cutmix_images[:target_size])
    cutmix_bboxes = np.array(cutmix_bboxes[:target_size])
    cutmix_labels = np.array(cutmix_labels[:target_size])
    cutmix_numbers = np.array(cutmix_numbers[:target_size])

    # 원본 데이터셋의 바운딩 박스와 레이블도 최대 레이블 길이에 맞게 패딩
    padded_bboxes = []
    padded_labels = []
    for bbox, label in zip(bboxes, labels):
        padded_bbox = np.pad(bbox, ((0, max_label_length - bbox.shape[0]), (0, 0)), mode='constant')
        padded_label = np.pad(label, (0, max_label_length - label.shape[0]), mode='constant')
        padded_bboxes.append(padded_bbox)
        padded_labels.append(padded_label)

    # 원본 데이터셋과 CutMix가 적용된 데이터셋을 합침
    merged_images = np.concatenate((images, cutmix_images), axis=0)
    merged_bboxes = np.concatenate((np.array(padded_bboxes), cutmix_bboxes), axis=0)
    merged_labels = np.concatenate((np.array(padded_labels), cutmix_labels), axis=0)
    merged_numbers = np.concatenate((numbers, cutmix_numbers), axis=0)

    # 합쳐진 데이터셋을 딕셔너리 형태로 반환
    merged_dataset = {
        'images': merged_images,
        'bboxes': merged_bboxes,
        'class': merged_labels,
        'numbers': merged_numbers
    }

    return merged_dataset

datasets = apply_cutmix(dataset, max_people=5, iou_threshold=0.05, mode='all', new_image_size=(128, 96), target_size=1000)
```

```python

import numpy as np
import random
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.patches as patches

images, numbers, bboxes = datasets['images'], datasets['numbers'], datasets['bboxes']
print(len(numbers))
# Filter dataset for numbers 4
max_label_length = 5
labels = []
for num in numbers:
    cls = [1] * num if num != 0 else [0]
    cls += [0] * (max_label_length - len(cls))
    labels.append(cls)

non_zero_indices = np.where(numbers > 4)[0]

images_filtered = images[non_zero_indices]
bboxes_filtered = bboxes[non_zero_indices]
numbers_filters = numbers[non_zero_indices]



# images_filtered = images_filtered[0:100]
# bboxes_filtered = bboxes_filtered[0:100]
# labels_filtered = labels_filtered[0:100]
# numbers_filters = numbers_filters[0:100]

print(bboxes_filtered)

print(len(numbers_filters))

dataset = {
    'images': images_filtered,
    'bboxes': bboxes_filtered,
    'numbers': numbers_filters
}


# Load TFLite model
tflite_model_path = 'ObjectDetection/model_v10/tflite/model_v10_quant_small.tflite'
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

class_names = ['person']  # List of class names



def calculate_iou(box1, box2):
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)
    
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    
    union_area = box1_area + box2_area - intersection_area
    
    iou = intersection_area / union_area if union_area != 0 else 0
    
    return iou

def draw_bboxes(image, true_boxes, pred_boxes, ap_score):
    fig, ax = plt.subplots(1, figsize=(10, 10))
    ax.imshow(image)

    # Draw true bounding boxes
    for box in true_boxes:
        x1, y1, x2, y2 = box[:4]
        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

    # Draw predicted bounding boxes
    for box in pred_boxes:
        x1, y1, x2, y2, score = box
        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='b', facecolor='none')
        ax.add_patch(rect)
        ax.text(x1, y1, f'Score: {score:.2f}', fontsize=10, color='b', bbox=dict(facecolor='white', edgecolor='none', pad=2.0))

    ax.text(5, 5, f'AP: {ap_score:.4f}', fontsize=15, color='g', bbox=dict(facecolor='white', edgecolor='none', pad=2.0))

    plt.axis('off')
    plt.show()

def apply_nms(predictions, iou_threshold=0.3, score_threshold=0.5):
    boxes = predictions[:, :4]
    scores = tf.nn.sigmoid(predictions[:, 4])  # Apply sigmoid to get the scores in range [0, 1]
    selected_indices = tf.image.non_max_suppression(
        boxes, scores, max_output_size=30, iou_threshold=iou_threshold, score_threshold=score_threshold)
    selected_boxes = tf.gather(predictions, selected_indices)
    return selected_boxes.numpy()

def iou(box1, box2):
    x1, y1, x2, y2 = box1[:4]
    x3, y3, x4, y4 = box2[:4]
    x_overlap = max(0, min(x2, x4) - max(x1, x3))
    y_overlap = max(0, min(y2, y4) - max(y1, y3))
    intersection = x_overlap * y_overlap
    area1 = (x2 - x1) * (y2 - y1)
    area2 = (x4 - x3) * (y4 - y3)
    union = area1 + area2 - intersection
    return intersection / union if union != 0 else 0

def calculate_precision_recall(true_boxes, pred_boxes, iou_threshold=0.5):
    true_positives = np.zeros(len(pred_boxes))
    false_positives = np.zeros(len(pred_boxes))
    detected = []
    for i, pred_box in enumerate(pred_boxes):
        best_iou = 0
        best_idx = -1
        for j, true_box in enumerate(true_boxes):
            current_iou = iou(pred_box, true_box)
            if current_iou > best_iou:
                best_iou = current_iou
                best_idx = j
        if best_iou >= iou_threshold and best_idx not in detected:
            true_positives[i] = 1
            detected.append(best_idx)
        else:
            false_positives[i] = 1
    false_negatives = len(true_boxes) - len(detected)
    precision = np.sum(true_positives) / (np.sum(true_positives) + np.sum(false_positives)) if np.sum(true_positives) + np.sum(false_positives) > 0 else 0
    recall = np.sum(true_positives) / (np.sum(true_positives) + false_negatives) if np.sum(true_positives) + false_negatives > 0 else 0
    return precision, recall

def calculate_ap(precision, recall):
    precision = np.concatenate(([0.], precision, [0.]))
    recall = np.concatenate(([0.], recall, [1.]))
    for i in range(len(precision) - 1, 0, -1):
        precision[i - 1] = np.maximum(precision[i - 1], precision[i])
    indices = np.where(recall[1:] != recall[:-1])[0]
    ap = np.sum((recall[indices + 1] - recall[indices]) * precision[indices + 1])
    return ap

def compute_map(dataset, interpreter, iou_thresholds=[0.5]):
    aps = []
    image_ap_scores = []
    for img, true_boxes in zip(dataset['images'], dataset['bboxes']):
        img_expanded = tf.expand_dims(img, axis=0)
        img_expanded = tf.cast(img_expanded, tf.float32)
        img_expanded = img_expanded / 255.0  # Adjusting to the range [0, 1]
        img_expanded = (img_expanded - 0.5) * 2.0  # Normalizing to [-1, 1]
        interpreter.set_tensor(input_details[0]['index'], img_expanded)
        interpreter.invoke()
        predictions = interpreter.get_tensor(output_details[0]['index'])[0]
        pred_boxes = apply_nms(predictions, iou_threshold=iou_thresholds[0], score_threshold=0.5)

        precision_list = []
        recall_list = []
        precision, recall = calculate_precision_recall(true_boxes, pred_boxes, iou_thresholds[0])
        if precision != 0 and recall != 0:
            precision_list.append(precision)
            recall_list.append(recall)

        precision = np.array(precision_list)
        recall = np.array(recall_list)
        ap = calculate_ap(precision, recall)
        aps.append(ap)

        image_ap_scores.append((img, true_boxes, pred_boxes, ap))
        if ap < 0.8:
            print("90% under : ", ap)
            draw_bboxes(img, true_boxes, pred_boxes, ap)  # Draw bounding boxes and show AP score

    map_score = np.mean(aps)

    # Find the image with the lowest AP score
    image_ap_scores = sorted(image_ap_scores, key=lambda x: x[3])
    lowest_ap_image, lowest_ap_true_boxes, lowest_ap_pred_boxes, lowest_ap_score = image_ap_scores[0]

    print(f'Lowest AP Score: {lowest_ap_score:.4f}')
    # draw_bboxes(lowest_ap_image, lowest_ap_true_boxes, lowest_ap_pred_boxes, lowest_ap_score)

    return map_score

# Assuming dataset and interpreter are already defined and set up
iou_thresholds = [0.3]
map_score = compute_map(dataset, interpreter, iou_thresholds=iou_thresholds)
print(f'mAP: {map_score:.4f}')
```

```python
'''
Lowest AP Score: 0.8000
mAP: 0.9914
'''



'''
MobileNet SSD
Used Flash : 471.87KiB
Used Ram : 502.13KiB
Total MAC: 71,504,135
Inference Time: 468.276 ms
mAP: 52.93

YoloV2
Used Flash : 276.73KiB
Used Ram : 449.17KiB
Total MAC: 110,052,238
Inference Time: 559.481ms
mAP: 94.05 % 

My Model

Used Flask : 159.15 KiB 
Used Ram : 312.00KiB   
Total MAC: 43,506,197
Inference Time: 303.948 ms
mAP: 96.21


# 둘 다 사용 : Final mAP: 0.9621

# 625/625 [==============================] - 43s 68ms/step - loss: 0.3961 - F1Score: 0.8646 - Precision: 0.8691 - Recall: 0.8601 - val_loss: 0.4130 - val_F1Score: 0.8610 - val_Precision: 0.8670 - val_Recall: 0.8551 - lr: 5.8594e-07

  
  

# HardNegativeMining 만 사용 : Final mAP: 0.8967

# 625/625 [==============================] - 44s 68ms/step - loss: 0.9663 - F1Score: 0.8535 - Precision: 0.7984 - Recall: 0.9167 - val_loss: 0.9937 - val_F1Score: 0.8511 - val_Precision: 0.7989 - val_Recall: 0.9106 - lr: 9.3750e-06

  
  

# HardPositiveMining 만 사용 : Final mAP: 0.9247

# 625/625 [==============================] - 41s 65ms/step - loss: 0.3441 - F1Score: 0.8614 - Precision: 0.8714 - Recall: 0.8515 - val_loss: 0.3604 - val_F1Score: 0.8580 - val_Precision: 0.8726 - val_Recall: 0.8439 - lr: 4.6875e-06

  
  

# 둘 다 사용 안함 : Final mAP: 0.8147

# 625/625 [==============================] - 41s 65ms/step - loss: 0.3692 - F1Score: 0.8562 - Precision: 0.8074 - Recall: 0.9114 - val_loss: 0.3882 - val_F1Score: 0.8517 - val_Precision: 0.8029 - val_Recall: 0.9069 - lr: 2.3438e-06
'''
```

**SDNet** (Split Denoise Network)

**SRNet** (Split Reduce Noise Network)

**DSSNet** (Dual Split Suppress Network)

MSPNet

