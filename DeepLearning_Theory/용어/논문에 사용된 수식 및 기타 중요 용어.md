



# Convolutional Layer (Conv2D)

- **Output Height**:
$$OH = \frac{{IH + 2 \times P - KH}}{S} + 1$$
  
- **Output Width**:
$$OW = \frac{{IW + 2 \times P - KW}}{S} + 1$$

  - $IH$ : Input Height
  - $IW$ : Input Width
  - $P$ : Padding
  - $KH$ : Kernel Height
  - $KW$ : Kernel Width
  - $S$ : Stride
---
# Transposed Convolutional Layer (Conv2DTranspose):

- **Output Height**:
$$OH = S \times (IH-1) + KH - 2 \times P$$

- **Output Width**:
$$OW = S \times (IW-1) + KW - 2 \times P$$

  - $IH$ : Input Height
  - $IW$ : Input Width
  - $P$ : Padding
  - $KH$ : Kernel Height
  - $KW$ : Kernel Width
  - $S$ : Stride

**참고**: 위의 공식에서 패딩(P) 값은 'same' 또는 'valid'에 따라 달라집니다.
- 'same': $P = \frac{KH - 1}{2}$ (일반적으로 커널 크기가 홀수일 때)
- 'valid': $P = 0$
---



# WaveNet


### 오디오 파형의 결합 확률(joint probability)

- 이 수식은 주어진 데이터 시퀀스 $\bf{x}$에 대한 결합 확률 $p(\bf{x})$를 표현한 것입니다. 
- 여기서 $x= \{\bf{x_1,x_2,…,x_T} \}$ 는 길이 $T$의 시퀀스입니다.
$$\begin{equation} p(\bf{x})=\prod_{t=1}^{T} p(x_t|x_1,\dots,x_{t-1}) \end{equation}$$
- 왼쪽에 있는 $p(\bf{x})$는 전체 시퀀스 $\bf{x}$의 결합 확률을 나타냅니다.
    
- 오른쪽에 있는 $\prod_{t=1}^{T}$​는 시퀀스의 모든 원소에 대한 곱셈을 나타내는 기호입니다. 이 기호는 $t=1$에서 $T$까지의 범위에 대한 곱셈을 의미합니다.
    
- $\bf p(x_t|x_1,\dots,x_{t-1})$는 조건부 확률을 나타냅니다. 이 확률은 시퀀스의 $t$번째 원소 $x_t$​의 확률을 
	$x1​,x2​,…,x_{t-1}$​이 주어졌을 때 계산한 것입니다. 다시 말해, 이 조건부 확률은 이전의 모든 원소들이 주어졌을 때 $x_t$가 발생할 확률을 나타냅니다.
    
이 수식은 주로 시퀀스 모델링, 특히 자연어 처리나 시계열 예측과 같은 분야에서 사용됩니다. 예를 들어, 언어 모델은 주어진 단어 시퀀스를 바탕으로 다음 단어를 예측하는데 이러한 조건부 확률을 사용합니다.

---


- 주어진 수식
$p(\mathbf{x})=\prod_{t=1}^{T} p(x_t|x_1,\dots,x_{t-1})$

에서, 각 항의 조건부 확률 $p(x_t|x_1,\dots,x_{t-1})$는 "지금까지의 사건 $x_1, x_2, \dots, x_{t-1}$이 주어졌을 때, 다음 사건 $x_t$가 발생할 확률"을 나타냅니다.

따라서 조건부 확률의 기준이 되는 사건(또는 정보)는 $x_1, x_2, \dots, x_{t-1}$입니다. 이전에 발생한 모든 사건들이 조건이 되어, 그 조건 아래에서 $x_t$가 발생할 확률을 나타냅니다.


- 각 조건부 확률 $p(x_t|x_1,\dots,x_{t-1}$) 내에서 $x_1, x_2, \dots, x_{t-1}$는 곱해지는 것이 아닙니다. 이들은 단순히 "조건"으로 주어진 사건들입니다.

예를 들어:
- \(t=1\)일 때: $p(x_1)$ (첫 번째 사건이 발생할 확률)
- (t=2)일 때: $p(x_2|x_1)$ (첫 번째 사건 $x_1$이 발생했을 때 두 번째 사건 $x_2$가 발생할 확률)
- \(t=3\)일 때: $p(x_3|x_1, x_2)$ (첫 두 사건 $x_1, x_2$가 발생했을 때 세 번째 사건 $x_3$가 발생할 확률)

... 이런 식으로 계속됩니다.

$\prod_{t=1}^{T}$는 이러한 모든 조건부 확률들을 곱하는 것을 의미합니다. $x_1$부터 $x_{t-1}$까지의 사건들 자체를 곱하는 것이 아닙니다.


---
- **설명** :
상상해보세요. 우리는 연속된 이야기를 만들고 있습니다. 각 단계에서 다음 단어나 사건을 예측하려고 합니다.

1. 첫 번째 단계에서는 아무런 정보가 없으므로, 첫 번째 사건(또는 단어)이 무엇일지 예측합니다.
2. 두 번째 단계에서는 첫 번째 사건(또는 단어)의 정보만을 사용하여 두 번째 사건을 예측합니다.
3. 세 번째 단계에서는 첫 번째와 두 번째 사건의 정보를 사용하여 세 번째 사건을 예측합니다.
... 이런 식으로 계속됩니다.

이 수식은 이러한 연속적인 예측 과정을 수학적으로 표현한 것입니다. 

사실상, 우리는 이야기의 각 단계에서 이전에 발생한 모든 사건들을 기반으로 다음 사건을 예측하려고 합니다. 그리고 이 모든 예측들의 확률을 곱하여 전체 이야기가 발생할 확률을 계산합니다.

예를 들면, "나는 학교에 갔다"라는 문장을 만들 때:
1. "나는" 이후에 "학교에"가 올 확률
2. "나는 학교에" 이후에 "갔다"가 올 확률

이 두 확률을 곱하면 "나는 학교에 갔다"라는 문장이 완성될 확률이 계산됩니다.

이 수식은 이러한 연속적인 예측 과정을 나타내는 것입니다!


---
### 조건부 확률 

조건부 확률은 두 사건 \( A \)와 \( B \)가 주어졌을 때, 사건 \( A \)가 발생했다는 조건 하에 사건 \( B \)가 발생할 확률을 의미합니다. 조건부 확률은 다음과 같이 표기됩니다:

$$P(B|A) = \frac{P(A \cap B)}{P(A)}$$

여기서:

- $P(B|A)$는 사건 $A$가 발생했을 때 사건 $B$가 발생할 확률을 의미합니다.

- $P(A \cap B)$는 사건 $A$와 $B$가 동시에 발생할 확률을 의미합니다.
	- 예를 들어, 주사위 던지기를 생각해봅시다:
	- 사건 AA: 주사위의 눈금이 짝수가 나오는 경우 (2, 4, 6)
	- 사건 BB: 주사위의 눈금이 4 이상 나오는 경우 (4, 5, 6)

	- 이 때, $A∩B$는 두 사건이 동시에 발생하는 경우를 나타내므로, 주사위의 눈금이 4와 6인 경우입니다. 따라서, $P(A∩B)$는 주사위의 눈금이 4 또는 6이 나올 확률을 의미하게 됩니다.

	- 일반적으로, $P(A∩B)$는 "사건 A와 사건 B가 동시에 일어날 확률"을 의미합니다.


- $P(A)$는 사건 $A$가 발생할 확률을 의미합니다.
	- $P(A)$는 "사건 A가 발생했다는 조건"을 나타내는 역할

- "사건 AA가 발생했다는 조건 하"에서 사건 BB의 확률을 정규화하는 역할

조건부 확률의 개념은 다양한 확률적 문제나 시나리오에서 중요한 역할을 합니다. 예를 들어, 의학적 진단에서 어떤 질병을 가진 환자가 특정 증상을 보일 확률, 혹은 자연어 처리에서 주어진 문장 또는 단어들이 주어졌을 때 다음 단어가 나타날 확률 등 다양한 분야에서 활용됩니다.


---
- **설명** :
상상해보세요. 우리는 초콜릿 상자를 가지고 있습니다. 이 상자에는 빨간 초콜릿과 파란 초콜릿이 들어 있습니다. 그리고 이 중 일부는 속이 크림으로 차 있는 특별한 초콜릿입니다.

1. 사건 \( A \): 초콜릿이 빨간색인 경우
2. 사건 \( B \): 초콜릿 속이 크림으로 차 있는 경우

이제, 조건부 확률 $P(B|A)$는 "빨간 초콜릿을 선택했을 때 그 초콜릿 속이 크림으로 차 있을 확률"입니다.

이 확률을 계산하려면:

- 분자에서는 빨간 초콜릿 중에 크림으로 찬 초콜릿이 몇 개인지 알아야 합니다. 이것이 $P(A \cap B)$입니다. 예를 들어, 10개의 빨간 초콜릿 중 3개가 크림으로 찬 경우, $P(A \cap B)$는 3/10 또는 0.3입니다.
  
- 분모에서는 전체 초콜릿 중 빨간 초콜릿이 몇 개인지 알아야 합니다. 이것이 $P(A$)입니다. 예를 들어, 50개의 초콜릿 중 10개가 빨간색이면, $P(A$)는 10/50 또는 0.2입니다.

이제 조건부 확률을 계산하면:

$P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{0.3}{0.2} = 1.5$

하지만 확률은 1을 초과할 수 없기 때문에 이 계산은 실제로는 불가능한 예시입니다. 그렇지만 이 설명은 초등학생이 조건부 확률의 개념을 이해하는 데 도움이 될 것입니다. 실제 상황에서는 $P(B|A$)의 값이 0과 1 사이에 있어야 합니다.





---
# 곱셈

곱셈은 확률론에서 특정한 사건들이 동시에 발생할 확률, 즉 결합 확률을 나타낼 때 주로 사용됩니다. 곱셈의 의미와 딥러닝에서의 사용에 대해 좀 더 구체적으로 설명하겠습니다.

1. **확률론에서의 곱셈의 의의**:
   - 독립적인 사건들의 경우: 두 사건 $A$와 $B$가 독립적일 때, 두 사건이 동시에 발생할 확률은 각 사건이 발생할 확률의 곱입니다. 즉, $P(A \cap B) = P(A) \times P(B)$.
   - 조건부 확률에서: 사건 $A$가 주어졌을 때 사건 $B$가 발생할 확률은 $P(B|A)$. 이를 결합 확률로 확장하면 $P(A \cap B) = P(A) \times P(B|A)$로 표현됩니다.

2. **딥러닝에서의 곱셈**:
   - **연쇄 법칙**: 딥러닝에서의 곱셈은 종종 확률의 연쇄 법칙을 표현할 때 사용됩니다. 예를 들어, 시퀀스 데이터나 시계열 데이터에서 과거의 정보로부터 미래를 예측할 때 조건부 확률들의 곱을 사용합니다.
   - **손실 함수**: 딥러닝 모델을 훈련할 때, 데이터 포인트들의 손실(또는 오차)을 계산하는 데 사용되는 확률값들의 곱이 포함될 수 있습니다. 로그 가능도를 최대화하는 것은 각 데이터 포인트에서의 확률들의 곱을 최대화하는 것과 관련이 있습니다.
   - **정규화**: 확률 값들을 곱하면 그 결과는 0과 1 사이의 값이 됩니다. 이는 확률 분포를 정규화하는 데 도움이 됩니다.

요약하면, 곱셈은 여러 사건이나 확률 값들이 동시에 얼마나 자주 발생하는지를 측정하는 데 사용됩니다. 딥러닝에서도 이러한 개념이 다양한 방식으로 활용됩니다.

---
곱셈을 사용하는 주된 이유는 전체 시퀀스 $\mathbf{x}$의 결합 확률을 구하기 위해서입니다. 

시퀀스의 각 원소 (또는 사건)가 독립적으로 발생한다고 가정할 경우, 그 시퀀스 전체가 발생할 확률은 각 원소가 발생할 확률들의 곱으로 나타낼 수 있습니다. 그러나, 여기서 우리는 각 원소 (또는 사건)가 이전 원소들에 의존한다고 가정하고 있습니다. 따라서 각 원소의 발생 확률은 조건부 확률로 표현됩니다.

예를 들어, 단어들로 이루어진 문장을 생각해봅시다. "나는 학교에 갔다"라는 문장이 있을 때, "갔다"라는 단어가 나올 확률은 "나는 학교에"라는 이전 단어들에 의존합니다. 따라서, 이 문장 전체가 주어질 확률은 각 단어가 주어질 조건부 확률들의 곱으로 표현됩니다.

이러한 연속적인 사건들의 결합 확률을 구할 때, 조건부 확률들을 곱하는 방식을 사용합니다. 이것이 $\prod_{t=1}^{T}$ 기호를 사용하는 주된 이유입니다.





---
# 로그 

로그(logarithm)는 수학적 개념으로, 어떤 수를 다른 수의 지수로 표현하는 방법입니다. 로그의 기본 아이디어는 "어떤 수를 몇 번 곱해야 다른 수가 될까?"라는 질문에 답하는 것입니다.

**기본 예시**:
10을 2번 곱하면 100이 되죠? 이걸 로그로 표현하면, "10을 몇 번 곱해야 100이 될까?"라는 질문에 답하는 것과 같습니다. 그 답은 2입니다. 그래서, 밑이 10인 로그에서 100의 값은 2입니다.

$$\log_{10} 100 = 2$$




이해를 돕기 위해 좀 더 간단한 예시를 들어보겠습니다.

**예시**:
상상해보세요, 우리가 거대한 타워를 짓고 싶어요. 그리고 각 층을 쌓을 때마다 타워의 높이가 2배로 늘어난다고 합시다. 만약 1층을 쌓으면 타워의 높이는 2m, 2층을 쌓으면 4m, 3층을 쌓으면 8m가 됩니다.

이제 질문! 타워의 높이가 8m가 되려면 몇 층을 쌓아야 할까요? 답은 3층이죠. 이런 상황에서 로그는 "타워의ㄴ 높이가 8m가 되려면 몇 층을 쌓아야 하는가?"라는 질문에 답하는 것과 같습니다.





**로그의 사용 이유**:
1. **값의 범위 축소**: 딥러닝에서는 데이터의 값이 너무 크면 학습이 어렵습니다. 로그를 사용하면 큰 값을 작게 만들어 학습을 도와줍니다.
2. **곱셈을 덧셈으로 변환**: 로그의 특성 상, 두 수의 곱셈은 로그에서 덧셈으로 바뀝니다. 이는 계산을 단순화하는데 도움이 됩니다.
3. **비율 감지**: 로그는 비율이나 백분율 변화를 감지하는 데 유용합니다. 예를 들어, 경제나 음악에서 로그 스케일이 사용되기도 합니다.

**결론**:
로그는 큰 수를 작게 만들거나, 복잡한 연산을 간단하게 만드는 데 도움을 줍니다. 딥러닝에서도 이러한 이유로 로그를 자주 사용하게 됩니다.







---
### Residual and Skip Connections

컴퓨터가 정보를 처리할 때, 중간 중간에 정보를 건너뛰는 특별한 방법을 'skip connection'이라고 해요. 이 방법은 'Resnet'이라는 기술에서 처음 제안되었고, 이 논문에서도 그 방법을 사용합니다.


#### - Residual :
"Residual"은 남아있는 또는 잔여의 것을 의미하는 단어입니다. 딥러닝, 특히 심층 신경망에서 "Residual"이 주로 언급될 때, 이는 "Residual Connection" 또는 "Residual Block"과 관련이 있습니다. 이 개념은 Microsoft Research에서 발표된 "Deep Residual Learning for Image Recognition" 논문에서 처음 소개되었습니다.

**Residual Connection (또는 Skip Connection)의 핵심 아이디어**:
- 심층 신경망은 훈련하기 어려운데, 이는 네트워크가 깊어질수록 그래디언트(gradient)가 소실되거나 폭발하는 문제 때문입니다.
- Residual Connection은 이 문제를 해결하기 위한 방법 중 하나로, 입력을 몇 레이어를 건너뛰고 직접 출력에 더하는 형태로 도입되었습니다.
- 이렇게 함으로써, 신경망은 "잔여" 정보를 학습하게 되어, 심층 네트워크도 효과적으로 훈련할 수 있게 되었습니다.

**간단한 예시**:
고려해보실 상황: 어떤 신경망 레이어의 입력이 \( X \)이고, 이 레이어가 학습해야 할 변환을 \( F(X) \)라고 합시다. 일반적인 신경망에서는 \( F(X) \)를 직접 학습하려고 하지만, Residual Network에서는 \( F(X) + X \)를 출력으로 합니다. 여기서 \( F(X) \)는 "잔여" 변환을 의미합니다. 

이 구조의 핵심적인 장점은, 심층 네트워크에서도 각 레이어가 학습해야 할 변환을 훨씬 쉽게 만들어 준다는 것입니다. 

따라서, "Residual"이란 단어는 네트워크가 이전 레이어의 입력과 현재 레이어의 출력 사이의 "잔여" 차이 또는 변환을 학습하게 된다는 개념을 나타냅니다.



#### - Residual == Skip connection but  Skip connection != Residual

"Residual Connection"과 "Skip Connection"은 많은 상황에서 서로 바꿔 사용될 수 있지만, 원래의 의미나 맥락에 따라 약간의 차이가 있습니다.

1. **Residual Connection**:
   - "Residual"은 잔여나 남은 것을 의미합니다. ResNet 아키텍처에서는 각 레이어가 입력과 출력 사이의 잔여 차이나 변환을 학습하도록 설계되었습니다.
   - Residual Connection에서는 입력을 해당 레이어의 출력에 직접 더하는 방식으로 도입됩니다. 즉, \( \text{output} = \text{input} + F(\text{input}) \)과 같은 형태로 표현됩니다.

2. **Skip Connection**:
   - "Skip"은 건너뛴다는 의미입니다. 따라서, Skip Connection은 입력 데이터를 몇 개의 레이어를 건너뛰고 다음 레이어의 출력에 직접 연결하는 구조를 의미합니다.
   - Skip Connection은 네트워크의 어떤 부분에서도 사용될 수 있으며, Residual Connection이라는 특별한 형태의 Skip Connection 중 하나라고 볼 수 있습니다.

**결론**: 
- "Residual Connection"은 "Skip Connection"의 한 형태입니다.
- 모든 "Residual Connection"은 "Skip Connection"입니다. 하지만, 모든 "Skip Connection"이 "Residual Connection"인 것은 아닙니다.
- 그러나 실제로 많은 문헌이나 논의에서 이 두 용어는 서로 바꿔 사용되므로, 문맥을 잘 파악하여 이해하는 것이 중요합니다.
