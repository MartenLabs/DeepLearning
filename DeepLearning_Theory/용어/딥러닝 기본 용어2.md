

### Decision Tree
- 의사결정 트리는 classification과 regression 모두 가능한 지도학습 모델 중 하나
- 스무고개와 같이 질문을 하나씩 던져 정답을 맞춰가며 학습하는 알고리즘으로 비교적 예측 과정을 이해하기 쉽다
- 특정 기준(질문)에 따라 데이터를 구분하는 모델을 결정 트리 모델이라고 하며 한번의 분기 때마다 변수 영역을 두개로 구분한다
- Decision Tree에서 질문이나 정답을 담은 상자를 Node라고 하며 맨 처음 분류 기준(첫 질문)을 Root Node라고 하고, 맨 마지막 노드를 Terminal Node 혹은 Leaf Node라고 한다


### validation set
- 하이퍼파라미터 튜닝을 위해 모델을 평가할 때, test set을 사용하지 않기 위해 train set에서 다시 떼어 낸 data set
	- train set : 약 60% 
	- validation set : 약 20%
	- test set : 약 20%


### cross validation
- train set을 <U>여러 fold로 나눈 다음</U> 한 fold가 validation set의 역할을 하고 나머지 fold에서는 모델을 훈련한
	- 이렇게 모든 폴드에 대해 검증 점수를 얻어 평균하는 방법으로 교차 검증을 이용하면 검증 점수가 안정적이며, 훈련에 더 많은 데이터를 사용할 수 있다


### Grid Search
- 하이퍼파라미터 탐색을 자동화해 주는 도구
- 모델에게 가장 적합한 하이퍼파라미터를 찾을 수 있게 해준다
- Grid Search는 모델 하이퍼파라미터에 넣을 수 있는 값들을 순차적으로 입력한 뒤 가장 높은 성능을 보이는 하이퍼파라미터들을 찾는 탐색 방법


### Random Search
- Grid Search와 동일한 방식으로 사용하지만, 모든 조합을 다 시도하지 않고 각 반복마다 임의의 값을 대입해 지정한 횟수만큼만 평가
- 연속적인 매개변수 값을 탐색할 때 유용하다


### structured data / unstructured data
- 특정 구조로 이루어진 데이터를 structured data(csv나 database 등)라 하고, 반면 정형화되기 어려운 사진이나 음악 등을 unstructured data라 함


### ensemble learning
- 앙상블 학습은 강력한 하나의 모델을 사용하는대신 약한 모델 여러개를 조합하여 더 정확한 예측에 도움을 주는 방식
- 앙상블 학습 유형 
	- Voting : 여러개의 모델이 투표를 통해 최종 예측 결과를 결정하는 방식
		- Voting 방식
			- Hard Voting : 다수의 모델이 예측한 결과값을 최종 결과로 선정	
			- Soft Voting : 모든 분류기가 예측한 값의 결정 확률 평균을 구한 뒤 가장 확률이 높은 값을 최종 결과로 선정 
	- Bagging
		- 데이터 샘플링을 통해 모델을 학습시키고 결과를 집계하는 방법
		- 모두 같은 유형의 알고리즘 기반의 모델 사용
		- Overfitting 방지에 효과적
		- 대표적인 bagging방식 : random forest algorithm
	- Boosting 
		- 여러개의 모델이 순차적으로 학습을 진행
		- 이전 모델에서 예측이 틀린 데이터에 대해 올바르게 예측 할 수 있도록 다음 모델에게 weight를 부여하면서 학습과 예측을 진행
		- 계속해서 모델에게 weight를 boosting하며 학습을 진행
		- 예측 성능이 뛰어나 ensemble 학습을 주도
		- 대표적인 boosting 모듈: XGBoost, LightGBM
		- 보통 boosting 방식은 bagging에 비해 성능이 좋지만, 속도가 느리고 overfitting이 발생할 가능성이 존재하므로 상황에 따라 적절히 사용해야 함


### Random Forest
- 대표적인 decision tree 기반의 앙상블 학습 방법
- 안정적인 성능 덕분에 널리 사용
- bootstrap sample(data sampling)을 사용하고 랜덤하게 일부 특성을 선택하여 tree를 만드는 것이 특징 


### bootstrap sample
- data set에서 중복을 허용하여 data를 sampling하는 방식


### extra trees
- random forest와 비슷하게 동작하며  decision tree를 사용하여 ensemble 모델을 만들지만 bootstrap sample을 사용하지 않는 대신 random하게 node를 분할하여 overfitting을 감소시킴


### gradient boosting
- 깊이가 얕은  decision tree를 사용하여 이전 tree의 오차를 보완하는 방식으로 ensemble하는 방법, 깊이가 얕은 결정 트리를 사용하기 때문에  overfitting에 강하고 일반적으로 높은 generalization성능을 기대할 수 있음


### Histogram-based Gradient Boosting
- gradient boosting의 속도를 개선한 것으로 overfitting을 잘 억제하며 gradient boosting보다 조금 더 높은 성능을 제공
- 안정적인 결과와 높은 성능으로 매우 인기가 높다


### histogram
- 값이 발생한 빈도를 그래프로 표시한 것으로 보통 x축이 값의 구간(계급)이고, y축은 발생 빈도(도수)


### clustering
- 비슷한 sample끼리 그룹으로 모으는 작업으로 대표적인 비지도 학습 작업 중 하나


### K-means Algorithm
- 처음에 랜덤하게 cluster 중심을 정하여 cluster를 만들고 그 다음 cluster의 중심을 이동하여 다시 cluster를 결정하는 식으로 반복해서 최적을 cluster를 구성하는 알고리즘


### inertia
- k-means algorithm은 cluster 중심과 cluster에 속한 sample사이의 거리를 잴 수 있는데 이 거리의 제곱 합을 이너셔라고 함, 즉 cluster의 sample이 얼마나 가깝게 있는지를 나타내는 값


### dimensionality reduction
- 데이터의 특성(feature)은 다양한 측면을 보여 줄 수 있기에 feature가 많을수록 더 나은 결과를 가질 것이라고 예상할 수 있지만 실제로는 많은 feature를 가질수록 모델의 성능이 떨어진다
- feature의 수가 증하가는 것은 데이터의 수가 증가하는 것이 아닌 데이터를 표현하는 공간(차원)이 커지는 것을 말한다
- 차원 축소: 고차원의 데이터로 부터 저차원의 데이터로 변환하는 방법
	- Feature selection   : 주요 feature를 선택하는 과정
	- Feature extraction : 원래의 feature를 조합하여 데이터를 가장 잘 표현할 수 있는 중요한 성분들을 가진 새로운 feature를 추출하는 것을 발한다.
	
- 즉 데이터를 가장 잘 나타내는 일부 특성을 선택하여 데이터 크기를 줄이고 지도 학습 모델의 성능을 향상시킬 수 있는 방법 


### principal component analysis (PCA)
- 차원 축소 알고리즘 중 feature extraction 방법 중 하나로 데이터에서 가장 분산이 큰 방향을 찾는 방법이며 이런 방향을 주성분이라 한다
- 원본 데이터를 주성분에 투영하여 새로운 특성을 만들 수 있다


### one-hot encoding
- target값을 해당 클래스만 1이고 나머지는 모두 0인 배열로 만드는 것, 다중 분류에서 cross entropy loss function을 사용하여면 0, 1, 2와 같이 정수로 된 타깃값을 one-hot encoding으로 변환해야 한다
- ex) 1 = 001,      2 = 010,        3 = 100


### hidden layer
- 입력층과 출력층 사이에 있는 모든 layer를 은닉층이라 부름


### deep neural network
- 2개 이상의 hidden layer를 포함한 신경망


### optimizer
- 신경망의 가중치와 절편을 학습하기 위한 알고리즘 또는 방법
- 딥러닝 분야에서 최적화란 손실함수(Loss Function)값을 최소화하는 파라미터를 구성하는 과정
- 학습 데이터를 입력하여 네트워크 구조를 거쳐 예측값 (y^)을 얻게되는데 이 예측값과 실제 답(y)과의 차이를 비교하는 함수가 손실함수
- 즉 모델이 에측한 값과 실제 값의 차이를 최소화 하는 네트워크 구조의 파라미터를 찾는 과정이 최적화
- 대표적으로 SGD, RMSprop, Adam등이 있으며 Adam을 가장 많이 사용


### adaptive learning rate
- 모델이 최적점에 가까이 갈수록 안정적으로 수렴하도록 학습률을 낮추도록 조정하는 방법
- 이런 방식들을 학습률 매개변수를 튜닝하는 수고를 덜 수 있는 것이 장점


### dropout
- 훈련 과정에서 층에 있는 일부 뉴런을 랜덤하게 제거(drop)해서(뉴런의 출력을 무조건 0으로 만들어) 과적합을 방지
- 특정한 feature만을 과도하게 집중하여 학습함으로써 발생할 수 있는 overfitting을 방지하기 위해 사용


### convolution
- input에 weight를 곱하고 bias를 더하는 선형 계산이지만 입력 데이터 전체에 weight를 적용하는 것이 아닌 일부에 weight를 곱함 
- 이미지가 들어왔을 때 특징을 뽑아내는 과정
![Alt text](https://velog.velcdn.com/images/seongguk/post/fe454740-904a-4d30-8be4-23652a483f9a/1_1okwhewf5KCtIPaFib4XaA.gif)


### Convolution Layer
- 전체가 아닌 일부분만 빨간색 테두리(필터 혹은 커널)를 통해 가중치와 입력값을 곱한 값에 활성화 함수를 취하여 특징을 뽑는 과정.
![Alt text](https://velog.velcdn.com/images/seongguk/post/c31cb135-0601-4b1d-aa48-7291ee540ac9/img.gif)


### filter
- neuron = kernel = filter


### feature map
- 입력으로부터 kernel을 사용하여 합성곱(Convolution)연산을 통해 나온 결과
- 목적
	- 각각의 특징들을 패턴으로 읽어내는 것이 목적.
	- 말 그대로 특징을 잡아내는 맵
- $N \times N$ 입력 이미지와 $F \times F$ 필터일 경우, feature map의 크기 $O \times O$ 는 다음과 같이 결정된다. $$ O = {{(N+2P-F)}\over S} + 1 \quad (P : padding size, S : stride)$$
![Alt text](https://velog.velcdn.com/images/seongguk/post/ffc1e9c6-e5cb-408c-b1f7-779e373fbc32/image.png)


###  Kernel Size
- kernel size는 convolution의 시야(view)를 결정 
- 보통 2D에서 3x3 pixel로 사용


### padding / same padding
- 입력 배열의 주위를 가상의 원소(보통 0)로 채우는 것을 padding이라 하며 convolution network에서는 same padding을 많이 사용
- 샘플 테두리를 어떻게 조절할지 결정
- Padding된 Convolution은 input과 동일한 output차원을 유지하는 반면, 패딩되지 않은
- Convolution은 커널이 1보다 큰 경우 테두리의 일부를 제거 가능


### valid padding
- 패딩 없이 순수한 입력 배열에서만 합성곱을 하여 feature map을 만드는 경우이며 이때 feature map의 크기가 줄어듦


### stride 
- convolution layer에서 filter가 입력 위를 이동하는 크기로 기본 stride는 1픽셀, 즉 1칸씩 이동
- 이미지를 횡단할 때 커널의 스텝 사이즈를 결정
- 기본값은 1이지만 Max Pooling과 비슷하게 이미지를 다운샘플링 하기 위해 Stride를 2로 사용.
![](https://velog.velcdn.com/images/seongguk/post/70901c25-2ecd-4a3b-b042-e7e70c74f1c0/image.png)


### pooling
- convolution layer에서 만든 feature map의 가로세로 크기를 줄이는 역할을 수행하지만 feature map의 갯수는 줄지 않음 
- 역할
	- 차례대로 처리되는 데이터의 크기를 줄인다. 이 과정으로 모델 전체 매개변수의 수를 줄일 수 있다. 즉 데이터의 공간적인 특성을 유지하면서 크기를 줄여주는 층이며, 특정 위치에서 큰 역할을 하거나, 전체를 대변할 수 있는 특징을 추출 가능 하다
- 거치는 이유
	- 더 높은 정확도를 위해서는 필터가 많아야 하는데, 필터가 늘어날 수록 Feature Map이 늘어난다. 이는 딥러닝 모델의 Dimension이 늘어난다는 것이고, High Dimension 모델은 그만큼 파라미터의 수 또한 늘어난다. 이는 Over fitting의 문제점 뿐만 아니라, 모델의 사이즈와 레이턴시에도 큰 영향을 끼친다.
- Max pooling
	- ![Alt text](https://velog.velcdn.com/images/seongguk/post/adafefcc-5241-4fd3-976b-ef973be1fc7e/full_padding_no_strides.gif)

- Average Pooling
	- ![Alt text](https://velog.velcdn.com/images/seongguk/post/5290d7d2-7088-4139-9aa9-4cfd782c154f/%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C.gif)



### sequential data(순차 데이터)
- 텍스트나 시계열 데이터와 같이 순서에 의미가 있는 데이터
- 예를 들어 I am a boy는 말이 되지만 boy am a I는 말이 되지 않음


### time series data(시계열 데이터)
- 일정한 시간 간격으로 기록된 데이터 (주식, 날씨 등)


### feedforward neural network, FFNN
- 입력 데이터의 흐름이 앞으로만 전달되는 신경망 
- Fully Connected(Dense) Layer와 Convolution Layer모두 FFNN에 속함


#### recurrent neural network, RNN
- Fully connected layer와 거의 비슷
- 순차 데이터에 잘 맞는 인공신경망의 한 종류로 순차 데이터를 처리하기 위해 고안된 순환 층을 1개 이상 사용한 신경망 


### cell
- RNN에서는 특별히 layer를 cell이라 부르며 한 cell에는 여러 개의 neuron이 있지만 dense layer와 달리 neuron을 모두 표시하지 않고 하나의 cell로 layer를 표현 


### hidden state
- RNN에서는 cell의 출력을 은닉 상태라 부름
- 은닉 상태는 다음 cell로 전달 될 뿐만 아니라 cell이 다음 time step의 data를 처리할 때 재사용됨


### corpus(말뭉치)
- 자연어 처리 분야에서는 train data를 종종 corpus라 부름 


### token
- 공백으로 구분되는 문자열 또는 단어를 토큰이라 부름 
- I am a boy 는 4 token


### word embedding 
- RNN에서 text를 처리할 때 즐겨 사용하는 방법으로 입력으로 정수 데이터를 받아 메모리를 훨씬 효율적으로 사용할 수 있음 


### Long Short-Term Memory(LSTM)
- 단기 기억을 오래 기억하기 위해 고안된 순환층
- 입력 게이트, 삭제 게이트, 출력 게이트 역할을 하는 작은 셀이 포함 
- 감성 분석, 언어 모델링, 음성 인식, 비디오 분석, 예측 등에 널리 활용


### cell state
- LSTM cell은 hidden state외에 cell state를 출력
- cell state는 다음 cell로 전달되지 않으며 현재 cell에만 순환


### Gated Recurrent Unit(GRU)
- LSTM보다 weight가 적기 때문에 계산량이 적지만 LSTM 못지않게 좋은 성능을 냄













