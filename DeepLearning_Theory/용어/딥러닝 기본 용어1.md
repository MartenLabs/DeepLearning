
 
### binary classification 
- 머신러닝에서 여러 개의 종류(혹은 클래스) 중 하나를 구별해 내는 문제를 classification 이라 부르며 2개의 종류(클래스) 중 하나를 고르는 문제를 이진 분류 라 함


### feature
- 데이터를 표현하는 특징 
- ex) 생선의 특징인 길이와 무게를 특성이라 함


### KNN (K-Nearest Neighbors Algorithm)
- 가장 간단한 머신러닝 알고리즘 중 하나로 어떤 규칙을 찾기보다는 인접한 샘플을 기반으로 예측을 수행함


### training
- 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정 또는 모델에 데이터를 전달하여 규칙을 학습하는 과정 


### supervised learning
- 지도학습은 입력(데이터)과 target(정답)으로 이루어진 훈련 데이터가 필요하며 새로운 데이터를 예측하는 데 활용함


### unsupervised learning
- target data 없이 입력 데이터만 있을 때 사용. 이런 종류의 알고리즘은 정답을 사용하지 않으므로 무언가를 맞힐 수가 없는 대신 데이터를 잘 파악하거나 변형하는 데 도움을 줌 


### training data
- supervised learning의 경우 필요한 입력(데이터)와 target(정답)을 합쳐 놓은 것


### train set, test set
- 모델을 훈련할 때는 train set을 사용하고 평가는 test set으로 함. test set은 전체 데이터에서 20~30%


### sampling bias
- train set 과 test set에 샘플이 고르게 섞여 있지 않을 때 나타나며 sampling bias가 있음. 제대로 된 지도학습 모델을 만들 수 없음


### data preprocessing
- 머신러닝 모델에 훈련 데이터를 주입하기 전 가공하는 단계로 특성값을 일정한 기준으로 맞추어 주는 작업. 데이터를 표현하는 기준이 다르면 알고리즘을 올바르게 예측할 수 없음 


### broadcasting
- 조건을 만족하면 모양이 다른 배열 간의 연산을 가능하게 해 주는 기능 


### regression
- 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제 


### coefficient of determination(결정계수 $R^2$)
- 회귀 모델이서 예측의 적합도를 0과 1사이의 값으로 계산한 것으로 1에 가까울수록 완벽
$$R^2 = 1 - {(target - predict)^2\over {(target - avg)^2}} $$

### overfitting vs underfitting
- overfitting은 모델의 train set 점수가 test set 점수보다 훨씬 높을 경우를 의미
- underfitting은 반대로 모델의 train set와 test set 점수가 모두 동일하게 낮거나 test set 성능이 오히려 높을 경우를 의미



### linear regression
- 널리 사용되는 대표적인 regression 알고리즘으로 특성이 하나인 경우 어떤 직선을 학습하는 알고리즘


### Weight(가중치 || 중요도)
- 처음 들어오는 데이터(입력층)에서 다음 노드로 넘어갈때 모두 같은 값이면 계속 같은 값이 나옴으로 각기 다르게 곱해야 한다는 것이 가중치(Weight).
- 데이터를 각기 다른 비중으로 다음 hidden layers로 전달시키기 위해 weight, 중요도를 다르게 설정


### Bias(편향 || 성향)
- 하나의 뉴런에서 activation function을 거쳐 최종적으로 출력되는 값을 조절하는 역할
- 임계점을 얼마나 쉽게 넘을지 말지를 조절 


### Activation Function
- 뇌의 뉴런도 하나의 뉴런에서 다른 뉴런으로 신호를 전달할 때 어떤 임계점을 경계로 출력값에 큰 변화가 있는 것으로 추정 
- 인공 신경망은 디지털 세계이긴 하지만 뇌의 구조를 모방하므로 임계점을 설정하고 출력값에 변화를 주는 함수를 이용
- Sigmoid, ReLU, Softmax, tanh 등이 있음


### Activation Value 
$$a = g((\vec{x})^T \cdot \vec{w} + b)$$
$a$ : activation value    $g$ : activation function     $x$ : input vector     $w$: weight     $b$: bias


### Matrix Multiplication
- if $X = (N \times l_i)$ ,   $W = (l_i \times l_1)$,  $b=(1 \times l_1)$
- then $A = (N \times l_1)$
- $l_1$ : Neural 갯수,  $l_i$: input 갯수


### polynomial regression(다항 회귀)
- 다항식을 사용하여 특성과 타깃 사이의 관계를 나타낸 선형 회귀


### multiple regression(다중 회귀)
- 여러 개의 특성을 사용한 선형 회귀


### ridge regression
- 규제가 있는 선형 회귀 모델 중 하나로 모델 객체를 만들 때 alpha 매개변수로 규제의 강조를 조절
- alpha값이 크면 규제 강도가 세지므로 계수 값을 더 줄이고 조금 더 underfitting되도록 유도하여 overfitting을 완화


### lasso regression
- 또 다른 규제가 있는 선형 회귀 모델로 alpha 매개변수로 규제의 강도를 조절 
- ridge와 달리 계수 값을 아예 0으로 만들 수도 있음


### hyperparameter
- 머신러닝 모델이 학습할 수 없고 사람이 지정하는 파라미터


### multi-class classification
- target data에 2개 이상의 class가 포함된 문제 


### logistic regression
- 선형 방정식을 사용한 분류 알고리즘으로 선형 회귀와 달리 sigmoid 함수나 softmax 함수를 사용해 클래스 확률을 출력 


### sigmoid function
- sigmoid 함수 또는 logistic 함수라 부르며 선형 방정식의 출력을 0과 1사이의 값으로 압축하며 이진 분류를 위해 사용. 이진 분류일 경우 시그모이드 함수의 출력이 0.5보다 크면 양성 클래스 0.5보다 작으면 음성 클래스로 판단 
- 이진 분류 출력층에 주로 사용
$$\sigma(x) =  {1 \over 1 + e^{-x}}$$



### tanh function
- sigmoid함수 보다 Gradient Vanishing 증상이 더 적으며 학습 효율성이 뛰어남 
- 은닉층에서 sigmoid함수 역할을 하는 레이어를 쌓고자 한다면, tanh를 사용하는 것이 효과적
- Gradient Vanishing 문제는 여전히 존재

| 범위 | 중앙값 | 미분 최댓값 |
| :----: | :------: | :-----------: |
| -1 ~ 1    |    0    |    1         |
$$\tanh(x)$$


### ReLU
- 양수면 자기 자신을 반환하고, 음수면 0을 반환
- 보통 은닉층에서 가장 많이 사용되는 함수
- 출력값의 범위가 넓고, 양수인 경우 자기 자신을 그대로 반환하기 때문에 심층 신경망인 딥러닝 에서 sigmoid 함수를 activation function으로 사용해 발생한 문제였던 Gradient Vanishing 문제가 발생하지 않는다.
$$\max(0,x)$$



### softmax
- 주로 출력층에 사용되는 함수
- 다중 클래스 분류 모델을 만들 때 사용
- 결과를 확률로 해석할 수 있게 변환해주는 함수로 각 class에 속할 확률을 추정
$$ y_k = \frac{e^{a_k}}{\sum_{i=1}^{n}e^{a_i}} $$
$$softmax(z) = [\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}},\ \ \ \frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}},\ \ \  \frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}}] = [p_1, p_2, p_3]$$



### Stochastic Gradient Descent(확률적 경사 하강법)
- train set 에서 random하게 하나의 sample을 선택하여 loss function의 경사를 따라 최적의 모델을 찾는 알고리즘 


### epoch
- 인공 신경망에서 전체 data set에 대해 한 번 학습을 완료한 상태를 의미
- 1epoch은 학습에서 train data를 모두 소진했을 때의 횟수에 해당


### batch
- 데이터를 1개씩 입력받아 총 10만번 연산을 진행하는 것보다 한번에 큰 묶음으로 데이터를 입력받아(Batch) n번의 연산을 진행하는 것이 더 빠르다 
- 수치 계산 라이브러리 대부분이 큰 배열을 효율적으로 처리할 수 있도록 고도로 최적화되어 있기 때문에 Batch(묶음)로 데이터를 입력받아 학습시키는 것이 속도 측면에서 효율적 



### Mini-batch
- 거대한 양의 데이터를 한꺼번에 학습하지 않고 단위 별로 쪼개서 하는 것이 minibatch
	- 데이터를 하나씩 학습시키는 경우
		- 장점
			- 신경망을 한번 학습시키는데 소요되는 시간이 매우 짧다
		- 단점
			- GPU병렬 처리를 사용할 수 없다
			- 오차를 줄이기 위해 사용하는 Loss Function에서 최적의 parameter 를 설정하는데 상당히 많이 헤맬 수 있다
	- 전체 데이터를 입력하는 경우
		- 장점
			- 한번에 여러개의 데이터에 대해 신경망을 학습시킬 수 있으므로 오차를 줄일 수 있는 cost function의 최적의 parameter를 하나씩 학습하는것보다 빠르게 알아낼 수 있다
		- 단점
			- 전체 데이터를 학습시키기 떄문에 신경망을 한 번 학습시키는데 소요되는 시간이 매우 길다.
- 위 문제점과 장점을 적절히 섞은 것이 mini batch
- minibatch는 SGD(Stochastic Gradient Descent: 확률적 경사 하강법)와 배치를 섞은 것
- 전체 데이터를 N등분하여 각각의 train data를 batch 방식으로 학습
- 최대한 신경망을 한 번 학습시키는데(iteration) 걸리는 시간을 줄이며 전체 데이터를 반영할 수 있게 되며 효율적으로 하드웨어를 활용 가능 



### minibatch gradient descent
- 1개가 아닌 여러 개의 샘플을 사용해 경사 하강법을 수행하는 방법으로 실전에서 많이 상요


### batch gradient descent
- 한 번에 전체 샘플을 사용하는 방법으로 전체 데이터를 사용하므로 가장 안정적인 방법이지만 그만큼 컴퓨터 자원을 많이 사용.
- 또한 어떤 경우는 데이터가 너무 많아 한 번에 전체 데이터를 모두 처리할 수 없을지도 모름


###  loss function(cost function)
- 모델을 통한 예측 값이 실제 값과 얼마나 유사한지 판단하는 기준
- 예측값과 실제 값의 차이를 loss라 하며 이 loss를 줄이는 방향으로  학습 진행


### entropy
- 불확실성(uncertainty)에 대한 척도


### logistic loss function(binary cross entropy)
- 양성 클래스(target = 1)일 때 손실은 $-log(예측 확률)$ 로 계산하며, 1의 확률이 1에서 멀어질수록 손실은 아주 큰 양수가 됨
- 음성 클래스(target = 0)일 때 손실은 $-log(1-예측 확률)$ 로 계산
- 이 예측 확률이 0에서 멀어질수록 손실은 아주 큰 양수가 됨
$$BCE = \begin{cases} -log(\hat y), \; where \; y = 1 \\
-log(1-\hat y), \; where \; y = 0 \end{cases}$$


### cross-entropy loss function
- 다중 분류에서 사용하는 손실 함수


### Kullback-Leibler Divergence(쿨백-라이블러 발산)
- 두 확률분포 $p(y)$, $q(y)$ 의 분포모양이 얼마나 다른지를 숫자로 계산한 값
- p와 q가 완전히 같으면 0이 된다
- 표기법$$D_{KL}(p||q) = E[logp(x) - logq(x)]$$
 - KL Divergence는 거리(distance)가 아니라 확률분포 $q$가 기준확률분포 $p$와 얼마나 다른지를 나타내는 값이므로 두 확률 분포의 위치가 달라지면 일반적으로 값이 달라진다


### Jenson-Shannon divergence
- KL Divergence를 거리의 개념(Distance Metric)으로 쓸 수 있도록 하는 개념
- 두 확률간의 분포 차이(거리)를 알 수 있다
- 표기법$$JSD(p||q) = {1\over2}KL(p||M) + {1\over2}KL(q||M)$$ $$where, M = {1\over2}(p+q)$$


### hinge loss
- support vector machine이라 불리는 또 다른 머신러닝 알고리즘을 위한 손실 함수로 널리 사용하는 머신러닝 알고리즘 중 하나
- SGDClassifier가 여러 종류의 손실 함수를 loss 매개변수에 지정하여 다양한 머신러닝 알고리즘을 지원









