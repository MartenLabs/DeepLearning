
|                    |                                      |
| ------------------ | ------------------------------------ |
| 태그               | Generative Model                     |
| Description        | LSGAN                                |
| Journal/Conference | ICLR                                 |
| Link               | https://arxiv.org/abs/1611.04076   |
| Year               | 2016                                 |
| 원문 링크          | https://arxiv.org/pdf/1611.04076.pdf |



### LSGAN 요약 (Chat-GPT)

LS-GANs은 GAN의 변형 모델 중 하나로, 원래의 Adversarial Loss 대신 최소 제곱 손실 함수를 사용합니다. LS-GANs의 목표는 실제 이미지와 구분할 수 없는 고품질 이미지를 생성하는 것입니다. LS-GANs는 전통적인 GAN에서 발생하는 모드 붕괴와 훈련 중 불안정성과 같은 몇 가지 문제를 해결하기 위해 도입되었습니다.

LS-GANs의 구조는 생성자와 판별자 두 가지 주요 구성 요소로 구성됩니다. 생성자는 임의의 노이즈 벡터를 입력으로 받아 이미지를 생성합니다. 판별자는 이미지를 입력으로 받아 이미지가 실제인지 가짜인지를 나타내는 점수를 출력합니다. 생성자와 판별자는 적대적인 방식으로 훈련되며, 생성자는 판별자가 실제 이미지와 구별할 수 없는 이미지를 생성하도록 노력하고, 판별자는 이미지가 실제인지 가짜인지를 올바르게 식별하도록 노력합니다.

LS-GANs에서 생성자와 판별자를 훈련시키기 위해 사용되는 손실 함수는 원래의 Adversarial Loss 대신 최소 제곱 손실 함수를 기반으로 합니다. 생성자 손실은 생성된 이미지와 대상 이미지 간의 평균 제곱 오차로 정의되며, 판별자 손실은 판별자의 예측과 대상 레이블 (실제 또는 가짜) 간의 평균 제곱 오차로 정의됩니다. 이 손실 함수는 훈련 과정을 안정화하고, 전통적인 GAN에서 자주 발생하는 모드 붕괴를 피할 수 있도록 도와줍니다.

결론적으로, LS-GANs은 최소 제곱 손실 함수를 사용하여 고품질 이미지를 생성하는 GAN의 변형 모델입니다. LS-GANs의 구조는 생성자와 판별자로 구성되며, 적대적인 방식으로 최소 제곱 손실 함수를 사용하여 훈련됩니다. 최소 제곱 손실 함수의 사용은 훈련 과정을 안정화하고 모드 붕괴를 방지하여 고품질 이미지 생성에 이바지합니다.



# 리뷰

# Abstract

Generative Adversarial Networks (GANs)과 함께한 unsupervised learning은 큰 성공이 증명되었다. 보통의 GAN은 sigmoid cross entropy loss function과 함께한 classifier로 discriminator를 가정한다. 하지만, 우리는 이 loss function이 학습 과정에서 vanishing gradient 문제로 이끌 수 있음을 발견했다. 이러한 문제를 극복하기 위해서, 우리는 본 논문에서 discriminator를 위한 least square loss function을 적용하는 Least Square Generative Adversarial Networks (LSGANs)를 제안한다. 우리는 LSGAN의 목적함수를 최소화하는 것이 Pearson x^2 divergence를 최소화하는 것을 일으킨다는 것을 보여준다. 첫번째로, LSGAN은 보통의 GAN들보다 더 좋은 퀄리티의 이미지를 생성할 수 있다. 두번째로, LSGAN은 학습 과정에서 더 안정적으로 수행한다. 우리는 LSUN과 CIFAR10 데이터셋에서 LSGAN을 평가했고 실험 결과는 보통의 GAN에서 생성된 것들 보다 LSQGAN에서 생성된 것이 더 좋은 퀄리티임을 보인다. 우리는 또한 LSGAN의 안정성을 묘사하기 위해서 LSGAN과 보통의 GAN 사이를 비교하는 실험을 수행한다.


# 1. Introduction

딥러닝은 심오한 개혁을 시작했고, 이미지 분류, 객체 검출, segmenation과 같은 많은 실제 세상의 문제에 적용 되어왔다. 이러한 문제들은 학습 과정에서 많은 레이블이 있는 데이터를 제공하는 지도 학습의 범위에 들어간다. 지도 학습과 비교하여, 생성 모델과 같은 비지도 학습은 딥러닝으로부터 제한적인 영향을 얻는다. RBM, DBM, VAE와 같은 몇가지 deep 생성 모델이 제안되었음에도 불구하고, 이러한 모델은 이러한 모델들의 효율성을 제한하는 다루기 힘든 함수의 어려움이나 다루기 힘든 추론의 어려움에 직면한다.

최근에, Generative Adversarial Networks (GANs)는 비지도 학습의 문제에서 인상깊은 성능을 증명했다. 다루기 힘든 함수나 추론을 위한 대략적인 방법을 적용하는 다른 deep 생성 모델과 달리, GAN은 근사가 필요 없고 미분가능한 네트워크를 통해 end-to-end로 학습될 수 있다. GAN의 기본적인 아이디어는 discriminator와 generator를 동시에 학습하는 것이다: discriminator는 실제 샘플과 생성된 샘플 사이를 구분하는 것을 목표로 하고; generator는 최대한 실제와 가깝게 가짜의 샘플들을 생성하는 것을 시도하고, discriminator가 가짜 샘플이 실제 데이터로부터 왔다고 믿게 만든다. 지금까지, 많은 연구들은 GAN이 이미지 생성, 이미지 초해상화, 준지도 학습과 같은 다양한 문제에서 중요한 역할을 할 수 있음을 보인다.

이미지 생성에서 GAN의 대단한 진보에도 불구하고, GAN에 의해 생성된 이미지의 퀄리티는 아직 몇몇의 실제적인 문제에 대해서 제한적이다. 보통의 GAN은 discriminator에 sigmoid cross entropy loss function을 적용한다. 우리는 이러한 loss function이 결정 경계의 맞는 부분에 있고 하지만 아직도 실제 데이터로부터는 먼 가짜 샘플을 사용하여 generator를 업데이트 할 때 vanishing gradient 문제를 일으킬 것이라고 주장한다. Figrure 1 (b)는 discriminator가 샘플들이 모두 실제 데이터로부터 왔다고 믿게 만들면서 generator를 업데이트 할 때 가짜 샘플을 사용할 때, 결정 경계의 맞는 부분에 있기 때문에 이것은 거의 에러를 만들지 않는다. 하지만, 이러한 샘플들이 아직도 실제 데이터로부터 멀고 우리는 이것을 실제 데이터 근처로 넣고 싶어한다. 이러한 관찰에 의해서 우리는 discriminator에 least square loss function을 적용하는 Least Square Generative Adversarial Network (LSGANs)를 제안한다. 아이디어는 간단하지만 강력하다: least square loss function은 가짜 샘플을 결정 경계로 움직이게 할 수 있고, least square loss function은 샘플들을 결정 경계의 맞는 부분에 긴 길을 따라 놓여지도록 만들기 때문이다. Figure 1(c)에서 보여지듯이, least square loss function은 가짜 샘플에 패널티를 부과하고 이것이 심지어 정확하게 분류 됬음에도 불구하고 결정 경계로 이것들을 밀어 넣는다. 이러한 특성을 기반으로, LSGAN은 더 실제 데이터에 가까운 샘플들을 생성할 수 있다.

LSGAN의 또 다른 이점은 학습 과정에서 안정성이 증가된 것이다. 일반적으로 말하면, GAN의 훈련은 GAN 학습의 불안정성 때문에 실제로 어려운 문제이다. 최근에, 몇몇의 논문들은 GAN 학습의 불안정성은 부분적으로 목적 함수에 의해 발생된다고 지적했다. 구체적으로, 보통의 GAN의 목적함수를 최소화 하는 것은 vanishing gradient를 겪게 하고, 이것은 generator를 업데이트 하는 것을 어렵게 한다. LSGAN은 결정 경계로의 그들의 거리에 기반하여 샘플들에 패널티를 부과하므로 이러한 문제를 경감시키고, generator를 업데이트 하기위해 더 gradient를 생성한다 최근에 Arjovsky et al은 batch normalization을 제외함으로써 GAN 학습의 안정성을 평가한 방법을 제안했다. 안정성을 평가하기 위해 이 방법을 따르고, 우리는 LSGAN이 batch normalization 없이 상대적으로 좋은 상태로 수렴할 수 있음을 발견했다.

본 논문의 기여는 다음과 같이 요약될 수 있다:

-   우리는 discriminator에 least square loss function을 적용하는 LSGAN을 제안한다. 우리는 LSGAN의 목적함수를 최소화하는 것은 Pearson x^2 divergence를 최소화하는 것임을 보인다.
-   우리는 LSGAN을 LSUN과 CIFAR10 데이터셋에서 평가했고 실험 결과는 LSGAN이 보통의 GAN 보다 더 실제적인 이미지르 생성할 수 있다는 것을 증명한다.
-   우리는 conditional LSGAN을 중국어 생성에 적용했다. 우리는 3740개 클래스의 중국어 손글씨 데이터셋에서 평가했다. 제안된 모델은 읽을 수 있는 중국어를 생성할 수 있다.

본 논문의 나머지는 다음을 따른다. Section 2는 generative adversarial network의 관련 연구를 짧게 리뷰한다. 제안 방법은 Section 3에 소개되었고, 실험 결과는 Section 4에서 보여진다. 마지막으로, 결론은 Section 5에서 나온다.

![[LSGAN_1.png]]


# 2. Related Work

Generative Adversarial Networks (GANs)는 Gootfellow et al에 의해 제안 되었고, GAN의 이론을 게임 이론 시나리오에 기반하여 설명한다. 비지도 문제를 위한 강력한 가능성을 보여주면서, GAN은 이미지 생성, 이미지 초해상화, text to image synthesis, image to image translation과 같은 많은 구체적인 문제들에 적용되어왔다. 전통적인 content loss와 adversarial loss를 결합함으로써, super-resolution generative adversarial networks는 이미지 초해상화 문제에서 state-of-the-art 성능을 얻었다. Reed et al은 conditional GAN 기반으로 text 설명이 주어졌을 때 이미지를 합성하는 모델을 제안했다. Isola et al은 또한 하나의 표현에서 또 다른 표현으로 이미지를 전이하기 위해서 conditional GAN을 사용했다. 비지도 학습 문제에 더불어, GAN은 준지도 학습의 문제에서도 잠재력을 보여준다. Salimans et al은 준지도 학습을 위한 GAN 기반의 프레임워크를 제안했고, discriminator는 실제 데이터로부터의 입력 이미지의 확률을 출력할 뿐만 아니라 각각의 클래스에 속할 가능성도 출력한다.

GAN의 대단한 성공에도 불구하고 생성된 이미지의 퀄리티를 높이는 것은 아직도 도전적인 문제이다. 많은 연구들은 GAN을 위한 이미지 퀄리티를 증가시키기 위해 제안하고 있다. Radford et el은 첯번째로 GAN 아키텍쳐에 컨볼루셔널 레이어를 도입했고, deep convolutional generative adversarial entwork (DCGAN)이라는 네트워크 아키텍쳐를 제안한다. Denton et al은 Laplacian pyramid of generative adversarial networks (LAPGANs)라는 또 다른 프레임워크를 제안했다. 그들은 저해상도의 이미지로부터 시작해서 고해상도의 이미지를 생성하기 위해 Laplacian pytramid를 구성했다. 더구나, Salimans et al은 더 좋은 수렴을 위해 feature matching이라 불리는 기법을 제안했다. 이 아이디어는 생성된 샘플이 discriminator의 중간 레이어에서 mean square error를 최소화하므로써 실제 데이터의 통계를 매치하게 만드는 것이다.

GAN의 또 다른 중요한 이슈는 학습의 안정성이다. 많은 연구들은 GAN의 목적함수를 분석함으로써 이 문제를 해결하기 위해 제안되었다. discriminator를 energy function으로 봄으로써, [33]은 GAN의 학습의 안정성을 증가시키기 위해서 auto-encoder 아키텍쳐를 사용했다. generator와 discriminator를 더 균형있게 만들기 위해서, Metz et al은 generator를 증진시키기 위해 unrolled 목적함수를 만들었다. Che et al은 reconstruction 모듈을 통합하고 더 안정적인 gradient를 얻기 위해서 실제 샘플과 재구성된 샘플들 사이의 거리를 사용한다. Nowozin et al은 Jensen-Shannon divergence와 관련이 되어있는 원래의 GAN의 목적함수를 divergence estiatmation의 특별한 경우임을 지적했고, 이것을 임의의 f-divergence로 일반화했다. Arjovsky et al은 이것을 두가지 분포의 4개의 다른 divergence 또는 distance의 특성으로 분석했고 Wasserstein distance가 Jenson-Shannon divernce보다 좋다고 결론을 지었다. Qi는 실제 샘플이 가짜 샘플보다 더 작은 loss를 가진다는 가정 아래의 loss function인 Loss-Sensitive GAN을 제안했고 이 loss function은 거의 대부분에서 non-vanishing gradient를 가진다.


# 3. Method

본 section에서는, 우리는 첫번째로 GAN의 공식을 리뷰한다. 다음, 우리는 Section 3.2에서 LSGAN의 이점을 제시한다. 마지막으로, LSGAN의 두개의 모델 아키텍쳐는 3.3에서 소개된다.

## 3.1. Generative Adversarial Networks

GAN의 학습 과정은 discriminator D와 generator G를 동시에 학습하는 것이다. G의 타겟은 데이터 x로부터 분포 p_g를 학습하는 것이다. G는 uniform 또는 Gaussian 분포 p_z(z)로 부터 입력 변수 z를 샘플링하기 시작하고, 그 다음 입력 변수 z를 미분가능한 네트워크를 통해서 데이터 공간 G(z;theta_g)로 맵핑한다. 다른 한편으로 D는 이미지가 학습 데이터로부턴인지 또는 G로부터인지 인식하는 것을 목표로 하눈 분류기 D(x;theta_d)이다. GAN의 목적함수는 다음과 같이 형성된다.
![[LSGAN_2.png]]


## 3.2. Least Squares Generative Adversarial Networks

discriminator를 분류기로써 봄으로써, 보통의 GAN은 sigmoid cross entropy loss function을 적용한다. Section 1에서 언급했듯이, generator를 업데이트할 때, 이 loss function은 결정 경계의 맞는 부분에 있는 샘플들에 대해 vanishing gradient 문제가 발생시킬 것이고 아직도 실제의 데이터분포로부터 멀 것이다. 이 문제를 해결하기 위해서, 우리는 Least Squares Generative Adversarial Networks (LSGANs)를 제안한다. 우리는 discriminator를 위해 a-b 코딩 방법을 사용하고, a와 b는 각각 가짜 데이터와 실제 데이터를 위한 레이블이다. LSGAN의 목적함수는 다음과 같이 정의된다:
![[LSGAN_3.png]]
c는 D가 가짜 샘플을 믿도록 G가 원하는 값으로 정의한다.


### 3.2.1 Benifits of LSGANs

LSGAN의 이점은 두가지로부터 유도될 수 있다. 첫번째로, 결정 경계의 맞는 부분에서 길게 놓여져 잇는 샘플들을 위한 loss가 거의 없는 보통의 GAN과 달리, LSGAN은 이러한 샘플들이 맞게 구분되어 있음에도 불구하고 패널티를 부과한다. 우리가 generator를 업데이트 할 때, discriminator의 파라미터들은 고정된다 결과적으로 패널티를 부과하는 것은 generator가 결정 경계로 샘플들을 생성하도록 만든다. 다른 한편으로, 결정 경계는 성공적인 GAN 학습을 위해 실제 데이터의 매니폴드에 걸쳐있어야 한다. 그렇지 않으면 학습 과정은 포화될 것이다 따라서 생성된 샘플들을 결정 경계로 움직이게 하는 것은 실제 데이터의 매니폴드에 더 가깝게 만드는 것을 이끈다.

두번째로, 결정 경계의 긴 방향에 놓여있는 샘플들에 패널티를 부과하는 것은 generator를 업데이트 할 때 더 많은 gradient를 생성할 수 있고, vanishing gradient 문제를 완화시킨다. 이것은 LSGAN이 학습 과정에서 더 안정적으로 수행하도록 한다. 이러한 이점은 또 다른 관점에서도 유도될 수 있다: Figure 2에서 보여진데로, least square loss function은 한 부분에서 평평하고, sigmoid cross entropy loss function은 x가 상대적으로 클 때 포화된다.
![[LSGAN_4.png]]


### 3.2.2 Relation to Pearson x^2 Divergence

원래의 GAN 논문에서는, 저자들은 Equation 1을 Jensen-Shannon divergnce를 최소화하는 것으로 만든다.
![[LSGAN_5.png]]

우리는 LSGAN과 f-divergence 사이의 관계를 탐험한다 Equation 2의 확장은 다음과 같이 따른다.
![[LSGAN_6.png]]

고정된 G의 최적의 discriminator D는 다음과 같이 유도된다.
![[LSGAN_7.png]]

우리는 Equation 4의 V_LSGAN(G)를 재형성 할 수 있다.
![[LSGAN_8.png]]

![[LSGAN_9.png]]


### 3.2.3 Parameters Selection

Equation 2에서 value a,b,c를 정의하기 위한 하나의 방법은 b-c=1, b-a=2의 조건을 만족시키는 것이다. 예를 들어, a=-1, b=1, c=0의 셋팅을 따르면 다음과 같은 목적함수를 얻을 수 있다.
![[LSGAN_10.png]]

또 다른 방법은 c=b로 셋팅함으로써 생성하는 샘플들을 최대한 실제로 만드는 것이다.
![[LSGAN_11.png]]



## 3.3. Model Archiectures

VGG model에 의해 동기부여된 우리가 설계한 첫번째 모델은 Figure 3에서 보여진다. [25]에서의 아키첵쳐와 비교하여, 두개의 stride=1 deconvolutional layers는 top two deconvolutional layer 후에 추가된다. discriminator의 아키텍쳐는 least square loss function의 사용을 제외하고 [25]에서의 것과 동일하다. DCGAN을 따르면, ReLU activation과 LeakyReLU activation은 각각 generator와 discriminator에 사용되었다.

우리가 설계한 두번째 모델은 예를 들어 중국어 같은 많은 클래스를 가진 과제를 위한 것이다. 중국어를 위해, 우리는 여러개의 클래스 에서 GAN을 학습하는 것은 읽을 수 있는 문자를 생성하는데 가능하지 않다는 것을 발견했다. 이유는 입력은 여러개의 클래스가 있지만, 출력은 하나의 클래스를 갖기 때문이다. [9]에서 말한데로, 입력과 출력간의 결정적인 관계가 있어야 한다. 이러한 문제를 푸는 방법 중 하나는 conditional GAN을 사용하는 것이고 레이블 정보를 조건화 하는 것은 입력과 출력 사이의 결정적인 관계를 생성하기 때문이다. 하지만, 수만개의 클래스의 one-hot encoding 레이블 벡터에서 직접적으로 조건화하는 것은 메모리 비용과 계산량 측면에서 불가능하다. 우리는 레이블 벡터의 차원을 줄이기 위해서 선형 맵핑 레이어를 사용한다. generator를 위해, 레이블 벡터는 노이즈 입력 레이어로 concatenated 된다. discriminator를 위해, 레이블 벡터는 모든 convolutional layer와 fully-connected layer에 concatenate 된다. concatenated된 레이어들은 실험적으로 결정된다.

![[LSGAN_12.png]]


# 4. Experiments

이번 섹션에서는, 우리는 처음으로 데이터셋의 디테일과 구현을 제시한다. 그 다음, 우리는 LSGAN에 대한 정성적 평가와 정량적 평가를 제시한다. 그 다음 우리는 두개의 비교 실험에 의해 LSGAN과 보통의 GAN 사이의 안정성을 비교한다. 마지막으로, 우리는 3740개의 클래스를 포함하는 중국어 손글씨 데이터셋에서 LSGAN을 평가한다.

## 4.1. Datasets and Implementation Details

우리는 LSGAN을 3가지의 데이터셋에서 평가한다: LSUN, CIFAR10, HWDB1.0이다. 우리의 제안 방법의 구현은 Tensorflow를 사용하여 구현된 DCGAN의 공개 버전을 기반으로 했다. LSUN을 위해, leaning rate는 0.001로 설정했다. CIFAR10과 HWDB1.0을 위해서는 learning rate를 0.0002로 했다. DCGAN을 따라, Adam 옵티마이져의 beta1은 0.5로 설정했다.

## 4.2. Qualitative Evaluation

우리는 LSGAN과 DCGAN을 LSUN bedroom 데이터셋에서 같은 네트워크 아키텍쳐와 같은 해상도로 학습했다. 두 방법에 의해 생성된 이미지는 Figure 4에서 제시된다. DCGAN에 의해 생성된 이미지와 비교하여, LSGAN에 의해 생성된 이미지의 질감 디테일은 더 절묘하고 LSGAN에 의해 생성된 이미지가 더 날카롭게 보인다.

우리는 또한 LSGAN을 church, dining room, kitchen, conference room을 포함하는 네개의 장면 데이터셋에서 학습했다. 네개의 장면 데이터셋에서 학습된 LSGAN의 결과는 Figure 5에서 보여진다.

![[LSGAN_13.png]]
![[LSGAN_14.png]]
![[LSGAN_15.png]]



## 4.3. Quantitative Evaluation

### 4.3.1 Inception Score on CIFAR10

우리는 LSGAN과 DCGAN을 CIFAR10에서 같은 네트워크 아키텍쳐로 학습했고 inception score를 계산하기 위해 50,000개의 이미지를 랜덤하게 생성하기 위한 모델을 사용했다. LSGAN과 DCGAN에서 계산된 inception score는 Table1에 주어진다. 우리는 다른 학습된 모델을 따라서 inception score가 다양하다는 것을 관찰했고, Table 1에서 보여진 inception score는 LSGAN과 DCGAN 둘다에서 10개의 학습된 모델의 평균이다. 이러한 inception score의 정량적인 결과는 LSGAN이 DCGAN의 성능과 견줄만하다는 것을 보여준다.
![[LSGAN_16.png]]



### 4.3.2. Human Subjective Study

LSGAN의 성능을 더 평가하기 위해서, 우리는 같은 네트워크의 LSGAN과 DCGAN으로부터 침대 이미지를 생성하는 것을 사용해서 사람의 주관적인 스터디를 진행했다. 우리는 하나의 이미지는 LSGAN으로부터 오고 다른 하나는 DCGAN으로 부터 오는 이미지의 쌍을 랜덤하게 구성했따. 우리는 Amazon Mechanical Turk anotator들에게 어떤 이미지가 더 실제같이 보이는지 판단하도록 물었다. 총 4000개의 투표에서, DCGAN은 43.6%를 얻었고 LSGAN은 56.4%를 얻었다. LSGAN이 DCGAN보다 12.8% 더 많이 투표가 됬다.

## 4.4. Stability Comparison

섹션 3.2.1에서 말한데로, LSGAN의 하나의 이점 중 하나는 향상된 안정성이다. 여기서 우리는 LSGAN과 보통의 GAN 사이의 안정성을 비교하기 위해서 두개의 비교실험을 보여준다.

하나는 [2]에서의 비교 방법을 따르는 것이다. [25]에서 제시된 네트워크 아키텍쳐에 기반해서, 두개의 아키텍쳐는 안정성을 비교하기 위해 설계되었다. 첫번째 것은 generator에 batch normalization을 제외하는 것이고, 두번째 것은 generator와 discriminator 모두에 batch normalization을 제외하는 것이다. [2]에서 지적한데로, 옵티마이저를 선택하는 것은 모델의 성능에 중요하다. 따라서 우리는 Adam과 RMSProp의 두개의 옵티마이져를 가지고 대개의 아키텍쳐를 평가했다. 요약하면, 우리는 4개의 학습 셋팅을 따른다. (1) BN_G with Adam, (2) BN_G with RMSProp, (3) BN_GD with Adam, (4) BN_GD with RMSProp

우리는 보통의 GAN과 LSGAN을 사용하여 LSUN bedroom 데이터셋에서 우의 모델들을 학습했고 4개의 주요한 관찰을 따른다. 첫번째로, BN_G with ADAM에서, LSGAN이 상대적으로 좋은 퀄리티의 이미지를 생성하기 위한 기회가 있다. 우리는 10번 테스트 했고, 이 중에 5번은 상대적으로 좋은 퀄리티의 이미지를 생성했다. 하지만 보통의 GAN에서는, 우리는 성공적인 학습을 전혀 보지 못했다. 보통의 GAN은 심각한 정도의 mode collapse를 겪는다. LSGAN과 보통의 GAN에서 생성된 이미지는 Figure 6에서 보여진다. LSGAN은 어느정도의 model collapse를 가진 보통의 GAN보다 더 높은 퀄리티의 이미지를 생성한다. 세번째로, LSGAN과 보통의 GAN은 BN_G with RMSProp과 BN_GD with Adam에서 비슷한 성능을 보였다. 구체적으로 BN_G with RMSProp에서는, LSGAN과 보통의 GAN 둘다 상대적으로 좋은 이미지를 생성할 수 있다. BN_GD with Adam에서는, 둘다 약간의 mode collapse가 있다. 마지막으로, RMSProp은 보통의 GAN에서 BN_G RMSProp에서는 상대적으로 좋은 이미지를 만들지만 Adam과 함께하면 학습이 실패하기 때문에 Adam보다 더 안정적으로 수행한다.

또 다른 실험은 [19]에서 설계된 Gaussian mixture distribution 데이터셋에서 평가하는 것이다. 우리는 LSGAN과 보통의 GAN을 generator와 discriminator 둘다 3개의 fully connected layer를 가지고 있는 간단한 네트워크 아키텍쳐를 사용하여 2D mixture of 8 Gaussian 데이터셋에서 학습했다. Figure 7은 Gaussian kernel density estimation의 동적인 결과를 보여준다. 우리는 보통의 GAN이 step 15k에서 시작해서 mode collapse를 겪는 것을 볼 수 있다. 이것들은 데이터 분포의 하나의 유효한 mode 주위의 샘플을 생성한다. 하지만 LSGAN은 Gaussian mixture distribution을 성공적으로 학습한다.

![[LSGAN_17.png]]


### 4.4.1 Suggestions in Practice

학습하기 어려운 과제를 위한 LSGAN의 학습 과정 중에, 우리는 LSGAN이 처음 여러개의 학습 epoch에서 좋은 퀄리티의 이미지를 생성하도록 학습하지만, 마지막에는 종종 mode collapse을 겪는다는 것을 발견했다. LSGAN이 마지막에 mode collapse를 겪음에도 불구하고, 우리는 학습 과정의 중간에서 여전히 좋은 모델을 선택할 수 있다. 우리는 또한 학습 과정의 중간에서 LSGAN에 의해 생성된 이미지의 퀄리티가 좋고 나쁨 사이를 이동할 수 있다는 것을 발견했다. 위의 두가지 발견에 기반하여, 우리는 매 수천가지 또는 수백가지 이터레이션에서 생성된 이미지의 기록을 유지하고 이미지 퀄리티를 체크하면서 모델을 선택하는 것을 제안한다.

## 4.5 Handwritten Chinese Characters

우리는 또한 conditional LSGAN 모델을 3740개의 클래스를 포함하는 중국어 손글씨 데이터셋에서 학습했다. LSGAN은 성공적으로 읽을 수 있는 중국어를 생성하도록 학습했고, 몇개의 랜덤하게 선택된 문자들이 Figure 8에서 보여진다. 우리는 Figure 8로부터 두개의 주요한 관찰을 얻었다. 첫번째로, LSGAN에 의해 생성된 문자들은 읽을 수 있다는 것이다. 두번째로, 우리는 data augmentation과 같이 더 나아간 어플리케이션에 사용될 수 있는 레이블 벡터를 통해 생성된 이미지의 정확한 레이블을 얻을 수 있다.

![[LSGAN_18.png]]


# 5. Conclusions and Future Work

본 논문에서, 우리는 Least Squares Generative Adversarial Networks (LSGANs)을 제안한다. 실험 결과는 LSGAN이 보통의 GAN보다 더 높은 퀄리티의 이미지를 생성한다는 것을 보여준다. 안정성을 평가하기 위한 두개의 비교 실험이 수행되었고 결과는 LSGAN이 보통의 GAN보다 더 안정적으로 수행한다는 것을 증명한다. 더구나, 우리는 3740개의 클래스를 가진 중국어 손글씨 데이터셋에서 평가하며 중국어 생성을 위한 conditional LSGAN 모델을 제안한다. 발견들에 기반하여, 우리는 미래에 LSGAN을 ImageNet과 같은 복잡한 데이터셋으로 확장하기를 희망한다. 생성된 샘플을 결정 경계에 밀어넣는 대신에, 생성된 샘플을 실제 데이터로 직접 밀어 넣는 방법을 설계하는 것도 우리의 가치있는 미래 연구가 될 것이다.




---

# 정리


![[LSGAN_19.png]]
- 손실함수로 바이너리 크로스 엔트로피 로스를 사용 
- 바이너리 크로스 엔트로피를 사용해 minmax게임을 풀었다 
- 바이너리 크로스 엔트로피 로스는 입력에 대한 예측의 유사도
	- (얼마나 진짜와 유사한가, 유사할수 있도록 얼마를 조절하면 되는가 에 대한 정보)


![[LSGAN_20.png]]
- 코드상에서 바이너리 크로스 엔트로피라 명시 
- Discriminator와 Adversarial (G가 노이즈를 받아 G에서 나온 출력을 D가 받는 일련의 과정을 묶은 것) 둘 다 바이너리 크로스 엔트로피로 손실을 측정한 후 Optimization을 수행(Gradient를 최적화 한다)



![[LSGAN_21.png]]
- LSGAN은 바이너리 크로스 엔트로피를 사용하는 것이 아닌 Least Square를 사용



![[LSGAN_22.png]]
- 기존 GAN에서 Sigmoid cross entropy loss(= binary cross entropy loss) 를 사용 
- 입력에 대해 예측을 수행했을 때 얼마나 진짜와 비슷한가
- 그런데 이 BCE를 사용하면 Gradient vanishing문제 발생



![[LSGAN_23.png]]
- 위 그래프가 BCE Error를 썼을 때 양상 
- 빨간 동그라미가 진짜 데이터 분포 
- 파란색 + 가 G가 만들어낸 Fake sample들

- 마젠타 : fake sample인데 결정경계를 기준으로 아래가 진짜 위가 가짜로 판단 
	- D입장에서는 마젠타 색상이 결정경계 아래에 있기 때문에 가짜지만 진짜라고 판단
	- 즉 이미 진짜라고 판단 했기 때문에 G 업데이트에 도움을 주지 않는다 

- 하지만 실제로는 Real 과 가깝게 이동을 할 필요가 있음 
- 이런 필요가 있음에도 불구하고 업데이트에 이미 진짜로 판단 중이기 때문에 도움을 주지 않는다 (Gradient Vanishing)



![[LSGAN_24.png]]
- LSGAN의 해결법 
	- 손실함수를 Least square로 바꿈 
	- MSE와 비슷하게 실제와 가짜의 차이를 거리로 측정 

- 이 방법으로 가짜 데이터의 거리를 측정 하고 그 거리를 낮추는 방향으로 학습 



![[LSGAN_25.png]]
- 설명을 그림으로 나타낸 것
- 빨간색 선이 결정경계 
	- 가짜 데이터가 멀리 떨어져 있을 수록 패널티 부여 
	- 손실함수는 이 패널티를 줄이기 위해 결정경계에 가깝게 데이터를 이동

- 결국 real data분포에 더 가깝게 fake data 생성 



![[LSGAN_26.png]]
- sigmoid cross entropy loss funciton과 least squares loss function 비교 
	- sigmoid cross entropy loss funciton은 어느 시점 이상으로 가게되면 0이 되어 버림 
	- 반면 least squares loss function는 딱 한점에서만 0이고 전부 Loss값 존재 그래서 최적화 할 때 한점으로 수렴할 수 있어서 안정적으로 학습할 수 있다 



![[LSGAN_27.png]]
- 식으로 보면 기존의 GAN은 위 식으로 minmax게임을 해결
- LSGAN은 약간 거리라는 개념이 들어가도록 변경해서 해결 



![[LSGAN_28.png]]
- a 가 fake label
- b 가 real label 

- 진짜 데이터에 대해서는 real label에 대해서 다루고 가짜 데이터에 대해서는 fake label에 대해서 다뤄 오차를 최소화하는게 판별기의 목적

- 생성기는 real data에 대한 정보는 필요가 없고 자기가 만드는 fake data에 대한 정보만 있으면 되서 아래처럼 생김

- 여기서 a를 그냥 fake label이라고 말하고 있지만 c는 생성기 입장에서 진짜라고 믿게 하고싶은 fake 라는 요소가 있어서 기호를 다르게 표시 



![[LSGAN_29.png]]
- D가 최소가 되기 위해서는 D가  판별한 값과 real data의 오차가 적어야 함 
- 또한 G가 생성한 가짜 이미지를 D가 판별 했을 때 a와 가까워 져야 함



![[LSGAN_30.png]]
- G는 진짜를 흉내내는 fake들 에 가깝게 가서 손실을 최소화 해야 함 





![[LSGAN_31.png]]
- DCGAN 해상도도 낮고 많이 부자연스러운 요소가 많음 

- LSGAN은 상대적으로 더 선명하고 DCGAN보다 더 나은 방 이미지를 생성함 

- 왜냐하면 DCGAN은 이미 진짜로 판단된 fake이미지에 대해서는 업데이트 할 수 있는 정보를 얻을 수 없지만 LSGAN은 진짜로 분류된 fake 이미지에 대해서도 결정경계에 더 가깝게 만들어 주기 위해, 진짜 분포와 비슷하게 만들기 위해 손실을 설계했기 때문에 진짜에 가까운 데이터를 생성하게 됨



![[LSGAN_32.png]]
- LSGAN이 생성한 다양한 이미지들
















  








