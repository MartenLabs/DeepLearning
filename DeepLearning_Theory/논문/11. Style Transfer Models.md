
Image to IR by style transfer

# 1. Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization (AdaIN) - 2017
https://arxiv.org/abs/1703.06868


https://lifeignite.tistory.com/46


# Abstract

Gatys는 content image를 다른 이미지의 스타일로 rendering하는 알고리즘을 개발했다. 그리고 이를 style transfer라고 부른다. 하지만, **gatys의 framework는 느린 반복적 최적화 과정을 요구**하기 때문에 현실적인 적용이 힘들었다. 이에 FFNN을 통해서 빠른 속도로 이에 근접한 성능을 내는 Neural style transfer 알고리즘이 제안되었다. 하지만, **이러한 speed의 증가는 style의 종류를 한정**시키고, **각각의 새로운 스타일에 독립적으로 adapt 시키기 어려웠다**. 이에 우리는 간단하지만, 효율적인 독립적인 실시간 style transfer를 제안한다. 우리 방식의 **key는 AdaIN layer인데, 이 adaIN 레이어는 contents feature의 mean과 variance를 style features와 align**한다. 이 방식은 **매우 빠른 속도의 추론을 가능하게하며**, 동시에 pre-defined된 style-set의 제약을 없앴다. 추가적으로 우리의 접근법은 content-style trade-off, style interpolation, color & spatial controls 등의 유연한 user control을 하나의 FFNN을 통해 가능하게 한다.

# Introduction

이미지는 style과 contents로 어느정도 분리할 수 있다. 때문에 이미지의 contents를 유지한체 style을 바꿀수 있는데, 이를 우린 style transfer라고 부른다.

**대충 gatys 방식의 한계 - 느림**

**대충 기존 FFNN 방식의 한계 - 스타일이 한정됨**

우리의 접근방식은 새로운 스타일은 실시간으로 독립적인 transfer를 수행할 수 있다. gatys 방식(최적화기반)의 유연성과 feed-forward 방식 (FFNN 방식)와 유사한 속도를 결합해서. 우리의 방식은 Instance Normalization (IN) 방식에서 movitation을 받았는데, IN방식은 NN style transfer상에서 놀랍도록 효율적이다.

instance noramlization 의 성공을 설명하기 위해서, 우리는 새로운 해석을 제안하는데, **그것은IN이 feature statistics를 정규화함으로서 style normalization을 수행한다는 것이다. 이는 feature statistics를 정규화 하는것이 style information을 유도할 수 있다는 기존의 연구들로 비롯된 것이다**. 이러한 우리의 해석에 motivation을 얻어서, 우리는 IN을 간단하게 확장한 AdaIN을 제안한다.

**AdaIN은 Contents input과 Style input이 주어졌을 때, 간단하게 content input의 mean과 variance를 style input의 mean과 variance와 match되도록 조정한다**. 전체적인 실험해서, `Through experiments, we find AdaIN effectively combines the content of the former and the style latter by transferring feature statistics.` 디코더 네트워크는 AdaIN output을 image space로 inverting 함으로서 마지막 stylized image를 생성하는 법을 배운다.

우리의 방식은 input을 독립적인 새로운 스타일로 변환하는 유연성을 희생하지 않고도 Gatys방식보다 3배이상 빠르다. 그리고 유저컨트롤이가능하다.




# Related Work

## Style Transfer

### 초창기의 style transfer

- Style transfer는 non-photo-realistic rendering으로부터 비롯되었으며, 이것은 texture synthesis와 transfer와 연관이 깊다.
    - 이러한 이전 접근 방식에는 **linear filter response상에서의 histogram matching방식이나 non-parametric sampling**등이 포함된다.
    - 이러한 방식들은 일반적으로 **low-level statistics에 의존하며, 자주 semantic structure를 잡아내는데 실패**했다.

### feature statistics를 이요한 style transfer

- 하지만 Gatys는 DNN의 Convolutional layers 상에서 feature statistics를 매칭함으로서 굉장히 인상깊은 style transfer 결과를 최초로 발표했다.
- 최근에는 몇가지 개선점들이 발표되었는데,
    - Li와 wand는 `local patterns를 찾기 위한 (enforce)` deep feature space상에서의 MRF framework를 제안했다.
    - gatys는 color preservation, spatial location, scale of style transfer를 조절할 수 있는 새로운 방법을 제안했다.
        - Controlling Perceptual Factors in Neural Style Transfer - CVPR 2017 인데 별로 주목은 못받은듯?
    - Ruder는 시간상의 제약을 도입함으로서 video style transfer의 퀄리티를 향상시킬 수 있는 방법을 제안했다.
- 초창기의 방식 (gatys 2016)은 느려서 **On-device processing** 은 실용적인 사용이 어려웠다. 공통적인 제 2의 방식은 optimization process를 FFNN으로 대체하는 것인데, 이는 Gatys와 같은 object function을 최소화 하는 방식이다. 이러한 FFNN방식은 최적화 방식 보다 훨신 빠른 속도를 보여줘 이를 real-time application에서 사용 가능한 정도 수준까지 끌어올렸다.
    - Wang et al. 은 multi-resolution architecture를 제안했다.
    - **Ulyanov는 IN이다. (생성된 smaple의 quality와 diversity를 개선시키는 방법을 제안)**
- 하지만 이러한 Style Transfer방식은 각각의 network들이 특정 fixed style에 tied되어있다는 한계점을 가지고 있었다.
- 이를 해결하기 위해(to address this problem), **Dumoulin et al은 CIN을 제안해서 하나의 네트워크로 여러개의 스타일을 제공**할 수 있게 되었찌만 여전히 32개라는 encoding된 몇몇개만 제공 가능했기 때문에 한계점이있다.
- 매우 최근에는 **Chen and Schmidt 이 FF방식의 arbitrary style transfer를 제안**했는데, 이는 style swap layer를 이용한 것이다. content와 style 이미지의 feature activation이 주어졌을 때, style swap layer는 content feature를 가장 가깝게 매칭된 style feature로 p**atch-by-patch방식으로 대체**한다. 하지만 이방식은 굉장히 **높은 computational bottleneck**을만든다. 거의 512 x 512 인풋 이미지에 대하여 style swap을 수행하는데만 95%에 이르는 계산을 사용한다. 우리의 AdaIN은 이방식에 비해서 거의 수배에서 수십배 빠르다.

### Style transfer의 Loss Function

- Gatys는 feature상에서 Gram matrix를 이용하여 second-order statistics를 매칭시키는 Loss를 사용하였다.
- 다른 효과적인 Loss function들도 많이 제안되었는데, MRF loss, adversarial loss, histogram loss, CORAL loss, MMD loss, distance between channel-wise mean and variance. 등이 있다.
- 뭐 근데 이런애들은 결국 하나의 공통적인 목표가 있는데 그건 바로 **style image와 synthesized image상의 어떤 feature statistics를 match시키는 것을 목표**로한다.





## GAN

- GAN도 cross-domain image generation을 통해서 style transfer를수행할 수 있다.

# 3. Background

## 3.1 Batch Normalization





## 3.2 Instance Normalization

기존의 feed-forward stylization method에서는 각각의 Convolutional layer상에서 BN layer가 포함되어 있다. 놀랍게도 Ulyanov 아저씨가 BN을 IN으로 바꾸기만 했서 높은 성능 향상을 달성했다.

IN(x)=γ(x−μ(x)σ(x))+β

IN은 다음과 같이 생겼는데, BN과 마찬가지로 γ,β는 학습되어야 하는 파라미터다. 식자체는 BN과 똑같지만 다른점은 바로 group by c 뿐만 아니라 group by (b, c)해서 정규화 한다는 것이다.







# 2. Photorealistic Style Transfer via Wavelet Transforms (Wavelet Style Transfer) -2019
https://arxiv.org/abs/1903.09760



















