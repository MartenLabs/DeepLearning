
https://arxiv.org/abs/1703.06868



# Abstract

Gatys는 content image를 다른 이미지의 스타일로 rendering하는 알고리즘을 개발했다. 그리고 이를 style transfer라고 부른다. 하지만, **gatys의 framework는 느린 반복적 최적화 과정을 요구**하기 때문에 현실적인 적용이 힘들었다. 이에 FFNN을 통해서 빠른 속도로 이에 근접한 성능을 내는 Neural style transfer 알고리즘이 제안되었다. 하지만, **이러한 speed의 증가는 style의 종류를 한정**시키고, **각각의 새로운 스타일에 독립적으로 adapt 시키기 어려웠다**. 이에 우리는 간단하지만, 효율적인 독립적인 실시간 style transfer를 제안한다. 우리 방식의 **key는 AdaIN layer인데, 이 adaIN 레이어는 contents feature의 mean과 variance를 style features와 align**한다. 이 방식은 **매우 빠른 속도의 추론을 가능하게하며**, 동시에 pre-defined된 style-set의 제약을 없앴다. 추가적으로 우리의 접근법은 content-style trade-off, style interpolation, color & spatial controls 등의 유연한 user control을 하나의 FFNN을 통해 가능하게 한다.

# Introduction

이미지는 style과 contents로 어느정도 분리할 수 있다. 때문에 이미지의 contents를 유지한체 style을 바꿀수 있는데, 이를 우린 style transfer라고 부른다.

**대충 gatys 방식의 한계 - 느림**

**대충 기존 FFNN 방식의 한계 - 스타일이 한정됨**

우리의 접근방식은 새로운 스타일은 실시간으로 독립적인 transfer를 수행할 수 있다. gatys 방식(최적화기반)의 유연성과 feed-forward 방식 (FFNN 방식)와 유사한 속도를 결합해서. 우리의 방식은 Instance Normalization (IN) 방식에서 movitation을 받았는데, IN방식은 NN style transfer상에서 놀랍도록 효율적이다.

instance noramlization 의 성공을 설명하기 위해서, 우리는 새로운 해석을 제안하는데, **그것은IN이 feature statistics를 정규화함으로서 style normalization을 수행한다는 것이다. 이는 feature statistics를 정규화 하는것이 style information을 유도할 수 있다는 기존의 연구들로 비롯된 것이다**. 이러한 우리의 해석에 motivation을 얻어서, 우리는 IN을 간단하게 확장한 AdaIN을 제안한다.

**AdaIN은 Contents input과 Style input이 주어졌을 때, 간단하게 content input의 mean과 variance를 style input의 mean과 variance와 match되도록 조정한다**. 전체적인 실험해서, `Through experiments, we find AdaIN effectively combines the content of the former and the style latter by transferring feature statistics.` 디코더 네트워크는 AdaIN output을 image space로 inverting 함으로서 마지막 stylized image를 생성하는 법을 배운다.

우리의 방식은 input을 독립적인 새로운 스타일로 변환하는 유연성을 희생하지 않고도 Gatys방식보다 3배이상 빠르다. 그리고 유저컨트롤이가능하다.




# Related Work

## Style Transfer

### 초창기의 style transfer

- Style transfer는 non-photo-realistic rendering으로부터 비롯되었으며, 이것은 texture synthesis와 transfer와 연관이 깊다.
    - 이러한 이전 접근 방식에는 **linear filter response상에서의 histogram matching방식이나 non-parametric sampling**등이 포함된다.
    - 이러한 방식들은 일반적으로 **low-level statistics에 의존하며, 자주 semantic structure를 잡아내는데 실패**했다.

### feature statistics를 이요한 style transfer

- 하지만 Gatys는 DNN의 Convolutional layers 상에서 feature statistics를 매칭함으로서 굉장히 인상깊은 style transfer 결과를 최초로 발표했다.
- 최근에는 몇가지 개선점들이 발표되었는데,
    - Li와 wand는 `local patterns를 찾기 위한 (enforce)` deep feature space상에서의 MRF framework를 제안했다.
    - gatys는 color preservation, spatial location, scale of style transfer를 조절할 수 있는 새로운 방법을 제안했다.
        - Controlling Perceptual Factors in Neural Style Transfer - CVPR 2017 인데 별로 주목은 못받은듯?
    - Ruder는 시간상의 제약을 도입함으로서 video style transfer의 퀄리티를 향상시킬 수 있는 방법을 제안했다.
- 초창기의 방식 (gatys 2016)은 느려서 **On-device processing** 은 실용적인 사용이 어려웠다. 공통적인 제 2의 방식은 optimization process를 FFNN으로 대체하는 것인데, 이는 Gatys와 같은 object function을 최소화 하는 방식이다. 이러한 FFNN방식은 최적화 방식 보다 훨신 빠른 속도를 보여줘 이를 real-time application에서 사용 가능한 정도 수준까지 끌어올렸다.
    - Wang et al. 은 multi-resolution architecture를 제안했다.
    - **Ulyanov는 IN이다. (생성된 smaple의 quality와 diversity를 개선시키는 방법을 제안)**
- 하지만 이러한 Style Transfer방식은 각각의 network들이 특정 fixed style에 tied되어있다는 한계점을 가지고 있었다.
- 이를 해결하기 위해(to address this problem), **Dumoulin et al은 CIN을 제안해서 하나의 네트워크로 여러개의 스타일을 제공**할 수 있게 되었찌만 여전히 32개라는 encoding된 몇몇개만 제공 가능했기 때문에 한계점이있다.
- 매우 최근에는 **Chen and Schmidt 이 FF방식의 arbitrary style transfer를 제안**했는데, 이는 style swap layer를 이용한 것이다. content와 style 이미지의 feature activation이 주어졌을 때, style swap layer는 content feature를 가장 가깝게 매칭된 style feature로 p**atch-by-patch방식으로 대체**한다. 하지만 이방식은 굉장히 **높은 computational bottleneck**을만든다. 거의 512 x 512 인풋 이미지에 대하여 style swap을 수행하는데만 95%에 이르는 계산을 사용한다. 우리의 AdaIN은 이방식에 비해서 거의 수배에서 수십배 빠르다.

### Style transfer의 Loss Function

- Gatys는 feature상에서 Gram matrix를 이용하여 second-order statistics를 매칭시키는 Loss를 사용하였다.
- 다른 효과적인 Loss function들도 많이 제안되었는데, MRF loss, adversarial loss, histogram loss, CORAL loss, MMD loss, distance between channel-wise mean and variance. 등이 있다.
- 뭐 근데 이런애들은 결국 하나의 공통적인 목표가 있는데 그건 바로 **style image와 synthesized image상의 어떤 feature statistics를 match시키는 것을 목표**로한다.





## GAN

- GAN도 cross-domain image generation을 통해서 style transfer를수행할 수 있다.

# 3. Background

## 3.1 Batch Normalization

$$BN(x) = \gamma \bigg({ x - \mu(x) \over \sigma(x) }\bigg) + \beta $$
$$ \displaystyle \mu_c (x) = {1 \over NHW} \sum_{n=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{nchw}$$
$$\displaystyle \sigma_c(x) = \sqrt{{1 \over NHW} \sum_{n=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_{nchw} - \mu_c(x))^2 + \epsilon}$$



## 3.2 Instance Normalization

기존의 feed-forward stylization method에서는 각각의 Convolutional layer상에서 BN layer가 포함되어 있다. 놀랍게도 Ulyanov 아저씨가 BN을 IN으로 바꾸기만 했서 높은 성능 향상을 달성했다.

$$IN(x) = \gamma \bigg({x - \mu(x)\over \sigma(x)}\bigg) + \beta$$

IN은 다음과 같이 생겼는데, BN과 마찬가지로 γ,β는 학습되어야 하는 파라미터다. 식자체는 BN과 똑같지만 다른점은 바로 group by c 뿐만 아니라 group by (b, c)해서 정규화 한다는 것이다.


$$\displaystyle \mu_{nc}(x) = {1 \over HW } \sum_{h=1}^H \sum_{w=1}^W x_{nchw}$$

$$\displaystyle \sigma_{nc}(x) = \sqrt {{1 \over HW} \sum_{h=1} ^H \sum_{w=1} ^W} (x_{nchw} - \mu_{nc}(x))^2 + \epsilon$$

**BN은 σn이었는데 IN은 σnc다. 또한 BN과의 가장 큰 차이점은 바로 BN은 batch와 inference시에 사용하는 statistics가 다르지만, IN은 바뀌지 않는다.**

![](../../Data/논문_AdaIN/AdaIN_1.png)



## 3.3 Conditional Instance Normalization

affine parameter인 γ,β 하나만 학습시키는대신에, 각각의 style s에 따라서 서로다른 파라미터인 γs,βs를 학습시키는 CIN이 제안되었다.
$$CIN(x;s) = \gamma ^s \bigg({x - \mu(x) \over \sigma(x)}\bigg) + \beta^s$$

트레이닝시에 style image를 각각의 index s로 묶고, 랜덤하게 선택해서 style s에 따라서 파라미터를 학습시켰다. (set S는 실험에서 32개로) Content image는 CIN layer상에서 γs와 βs를 사용하는 style transfer network에 의해서 처리된다. 놀랍게도, **네트워크는 같은 convolutional parameters를 사용하며 다른 affine parameters를 IN layer에서 사용함으로서 완벽하게 다른 스타일을 생성**해낼 수 있었다.

이는 기존의 CIN이 없는 일반적인 네트워크를 사용할 경우와 비교해서 CIN 레이어는 추가적으로 2FS개의 파라미터를 사용한다. 여기서 F: 총 feature map의 개수며, S: 총 스타일의 개수이다. 따라서 style 의 개수가 증가할 때 **선형적으로 파라미터의 개수가 증가**하며, 스타일의 개수가 큰 모델의 경우 굉장히 challenging하게 된다. 또한, **이러한 방식은 독립적인 새로운 스타일에 대해서 트레이닝 없이 적용할 수 없다**.

# 4. Interpreting Instance Normalization

비록 IN의 큰 성과에도 불구하고, 왜 style transfer가 잘 작동하는지에 관해서는 여전히 알수 없다. **IN의 성공은 content image의 contrast에 invariance하게 IN 동작하기 때문이라고 말한다**. 하지만 IN은 feature space에서 존재하고, 그러므로 이것은 **픽셀 상에서 간단한 contrast normalization보다는 큰 영향을 줄 수 있을 것이다**. 아마도 더 놀라운 것은 IN의 affine parameters가 output images의 style을 완벽하게 바꿀 수 있다는 사실이다.

DNN의 feature statistics는 이미지의 style을 capture할 수 있다고 알려져 있다. Gatys도 second-order statistics를 optimization의 objective로 삼았고 Li et al.도 다른 statistics를 매칭하는 방식을 보여줬다. 이러한 관찰에 모티브를 받아서, 우리는 **instance normalization이 mean과 variance라고 이름붙여진 feature statistics를 normalizing함으로서 style normalization의 형태를 수행한다고 주장**한다. 비록 DNN은 image descriptor로서 사용되지만, 우리는 generator network의 feature statistics가 생성된 이미지의 style을 컨트롤한다고 믿는다.

![](../../Data/논문_AdaIN/AdaIN_2.png)


우리는 IN 모델과 BN모델을 각각 학습시켜서 style loss를 비교해봤다. 우리는 이 때 (a) 원본 이미지, (b) contrast normalized image, (c) pre-trained 모델을 활용하여 style normalized를 수행한 image에 대해서 학습을 수행했다. 놀랍게도 IN에 의한 개선이 contrast에 대해서 정규화한 이미지에 대해서 꽤 차이가 났따. 하지만 style을 normalized한 실험에서는 IN과 BN의 차이가 크게 나지 않았다. 우리의 결과는 IN이 style normalization의 한종류로 동작한다고 주장한다.

BN는 single image의 feature statistics가 아니라 sample의 배치의 feature statistics를 정규화 하기 때문에, 직관적으로 배치상의 서로 다른 스타일들을 하나의 스타일 근처로 정규화 시킨다고 이해할 수 있다. 이는 성능을 하락시킬 수 있다.

이러한 이해는 여러 연구들에 공통적으로 적용시킬 수 있으며 CIN도 마찬가지로 쉽게 이해할 수 있다.



# 5. Adaptive Instance Normalization

만약IN이 인풋을 하나의 단일 스타일, 특히 affine parameters에 의해 구체화되는 하나의 스타일로 정규화 한다면, adaptive affine transformations를 활용하여 독립적으로 주어진 스타일을 적용하는 것도 가능하지 않을까?

이에 따라 우리는 IN을 간단하게 확장한 Adaptive InstnaceNormalization (AdaIN)을 제안한다. AdaIN은 Content input x 와 style input y를 받았을 때, 간단하게 channel-wise mean and varinece of x를 channel-wise mean and varinece of y로 alighn한다. BN, IN, CIN과달리 AdaIN은 Affine parameter가 없다! 대신 affine parameter를 adaptively 계산한다.

$$AdaIN(x, y) = \sigma(y) \bigg({x - \mu(x) \over \sigma(x)}\bigg) + \mu(y)$$

여기서 우리는 간단하게 content input을 σ(y)와 μ(y)로 조정한다. IN과 유사하게, 이러한 statistics는 spatial locations을 across하여 계산된다.

직관적으로, 우리가 feature channel을 특정한 스타일의 brushstrokes 을 detect하자고 하자. 이러한 stock는 이 feature를 위한 high average activation 를 생성할 것이다. AdaIN에 의해서 생성된 output은 contents image의 spatial structure를 유지하면서 이 feature를 위한 같은 high average activation를 가지게 될 것이다. 이러한 brushstrokes feature는 Fedd-forward decoder를 통해 iamge space로 inverted될 것이다. 이러한 feature channel의 variance는 더 미묘한 style imnformation을 encode할 수 있을 것이고, 이것은 또한 AdaIN output으로 전달되고, 최종적인 output image에 전달될 것이다.

정리하자면, AdaIN은 channel-wise mean and variance라는 feature statistics를 transfering함으로서 feature space상에서 style trasnfer를 수행한다.
![](../../Data/논문_AdaIN/AdaIN_3.png)


style transfer algorithm의 overview. 우리는 style, contents image를 인코딩하기위해 fixed VGG-19네트워크의 첫 몇개의 레이어를 사용하였다. AdaIN layer는 style trasnfer를 feature space 상에서 수행한다. decoder는 adaIN output을 image spaces상으로 invert하는 방법을 learning한다. 우리는 같은 VGG encoder를사용해 content loss와 style loss를 구했다.




# 6. Experimental Setup

## 6.1 Architecture

우리의 style transfer network T는 content image c와 arbitrary style image s를 input으로 받아서, content image에 style image를 합성한다. 우리는 간단한 encoder-decoder architecture를 사용했으며, encode f는pre-trained VGG-19의 앞 몇개 부분(upto relu4_1)까지 사용했다. feature 이미지 상에서 content와 style image를 인코딩한 후에 우리는 두 인코딩 된 feature map을 AdaIN 레이어에게 주고, mean과 variance를 맞춘다.

$$t = AdaIN(f(c), f(s))$$
랜덤하게 초기화된 디코더 g는 t를 image space로 보내는 방법을 트레인하며 스타일이 입혀진 이미지 T(c,s)를 생성한다.

$$T(c, s) = g(t)$$


### Architecture Detail

- checker-board effect를 감소시키기 위하여 encoder의 pooling layer를 nearest up-sampling 방식으로 교체.
- f와 g에서 모두 reflection padding을 사용했다.
- - IN레이어는 각각의 셈플들을 하나의 스타일로 정규화하고 BN은 셈플들의 배치를 하나의 스타일로 centered시킨다. 두 normalization방식은 모두 원하지 않는방식인데, 왜냐하면 우리는 decoder가 굉장히 다른 스타일로 이미지를 생성하기를 위하기 때문이다.
    - 따라서 우리는 normalization을 사용하지로 결정했고, IN/BN이 모두 정말로 performance를 떨어트리는지 보여주겠다.
    - decoder에서 normalization방식을 고르는 것이 중요했는데, 결론은 no normalization이 제일 좋았다.
- MS-COCO를 컨텐츠 이미지로, WIkiArt 데이터셋을 스타일이미지로 나머지는 baseline network 셋팅을 따라했다. (어 키페이퍼가 patch-by-patch 였네?...) 대충 8만개쯤 된다.
- adam optimizer사용 batch size는 8로 content-style image pair로
- 전처리: 두 이미지를 aspect ratio를 유지한체 512로 사이즈를 키웠고, 여기서 256 by 256로 crop한다. 우리의 네트워크는 fully convolutional 이기 때문에, **어떤 사이즈의 이미지가 온다고해도 적용 가능하다.** 

### Loss

우리는 다른 네트워크와 유사하게 다음과같은 로스를 사용한다.

$$\mathcal{L} = \mathcal{L}c + \lambda \mathcal{L}_s$$
content loss는 target feature와 output image의 feature의 Euclidean distance를 구했다. 우리는 일반적으로 사용되는 content image의 feature response를 사용하는 대신에 AdaIN output t.를 content target으로 삼았다. 이게 조금더 빨리 convergence가 이루어졌기 때문이다.

$$\mathcal{L}_c = |f(g(t)) - t|2$$

우리의 AdaIN 레이어는 오직 style features의 mean과 standard deviation를 transfer하기 때문에 우리의 style loss는 오직 이러한 statistics를 match시켜야한다. 우리는 gram matrix loss가 유사한 결과물을 생성한다는 사실을 알았지만, 우리는 IN statistics를 match 시켰는데, 왜냐하면 이게 개념상 더 깔끔하기 대문이다. 이 스타일 로스는 Li et al.에 의해서 발견된 스타일로스이다.

$$\displaystyle \mathcal{L}_s = \sum_{i=1} ^ \mathcal{L} |\mu(\phi_i(g(t))) - \mu(\phi_i(s))|_2 + \sum_{i=1} ^ \mathcal{L} | \sigma(\phi_i(g(t))) - \sigma(\phi_i(s))|_2$$
$\phi_i$.는 VGG-19의 i번째 레이어이다. 스타일로스에서 사용한 레이어는 `relu1 1, relu2 1, relu3 1, relu4 1` 이다.



# Result

## 7.1. Comparison with other methods

## Speed

![](../../Data/논문_AdaIN/AdaIN_4.png)



## Quality

![](../../Data/논문_AdaIN/AdaIN_5.png)



## 7.2. Additional experiments

## 7.3. Runtime controls

![](../../Data/논문_AdaIN/AdaIN_6.png)
![](../../Data/논문_AdaIN/AdaIN_7.png)
![](../../Data/논문_AdaIN/AdaIN_8.png)














