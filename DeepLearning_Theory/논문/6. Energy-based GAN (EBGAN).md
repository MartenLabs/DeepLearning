|                    |                                      |
| ------------------ | ------------------------------------ |
| 태그               | Generative Model                     |
| Description        | EBGAN                                |
| Journal/Conference | ICLR                                 |
| Link               | https://arxiv.org/abs/1609.03126 |
| Year               | 2017                                 |
| 원문 링크          | https://arxiv.org/pdf/1609.03126.pdf|


### Introduction
본 논문에서 제시하는 바는 Discriminator를 일종의 Energy Function으로 보자는것이다.

여기서 [Energy Function]이란, 데이터를 받았을 때 해당 데이터가 [Data Manifold]에 가까우면 낮은 값을 내놓고 Data Manifold에서 멀다면 높은 값을 내놓는 함수를 의미한다. 여기서 내놓는 값을 Energy 라고 한다. 

이와 같은 Energy Function 역할을 하는 Discriminator를 구성하기 위해 MSE Loss를 내놓는 Auto Encoder 구조를 제안한다.(real data에 가까우면 낮은 MSE를 내놓음 -> reconstruction을 잘한다)


### EBGAN Model 

- #### Objective Functional 
$(1)\;\;\;\;\;\;\;\;\;\;\;\;\;L_D(x, z) \; = \; D(x) + [m - D(G(z))]^+$
$(2)\;\;\;\;\;\;\;\;\;\;\;\;\; L_G(z) \; = \; D(G(z))$

Discriminator의 output은 Data Manifold에 가까울수록 작은 값을 내놓는 Energy 값이다.
그래서 Discriminator를 훈련시킬때 Fake Data에 대해서는 높은 값을 내놓도록 하고 Real Data에 대해서는 작은 값을 내놓도록 학습시켜야한다.

이런 Energy 기반 모델의 loss로 [margin loss]를 제안한다.



### Optimality of the solution
논문에서 이 부분은 Vanila GAN 논문에서 제시한 것과 같이 이러한 방법이 Nash Equilibrium으로 이끈다는 것을 증명한다 



### Using Auto-Encoders 
![](../../Data/논문_EBGAN/EBG_2.png)

Discriminator가 Energy Function의 역할을 하기 위해서 Auto Encoder 형식과 MSE Loss를 output으로 내놓는 Architecture를 제안한다. (MSE Loss를 Energy로 보면 된다)

###### TMI
- 최근엔 Fast GAN이나 PatchGAN과 같이 Discriminator의 output이 단일 값이 아닌 경우가 존재하긴 하지만, 예전의 GAN 구조에서는 단일 값을 내놓는 형식이 흔한 구조
- 당시 상황에서 단일 값 (0 or 1)을 내놓는 standard GAN과 달리 모든 pixel을 비교하는 reconstruction based output은 다양한 direction의 gradient를 내놓을 수 있어서 더 큰 batch size를 사용할 수 있었다고 한다 



### Connection to the Regularized Auto-Encoders
오토인코더는 입력 데이터를 재구성하는 방식으로 훈련되는 신경망 모델이다. 그러나 오토인코더의 훈련에서 자주 발생하는 문제 중 하나는 모든 공간에서 입력 데이터를 완벽하게 재구성하는, 즉 완전한 항등 함수처럼 동작하는 경향이 있어서 데이터의 특징을 충분히 학습하지 못하는 경우이다. 이러한 문제를 피하기 위해서는 데이터 공간의 외부 영역에는 높은 에너지 값을 할당해야한다.

EBGAN은 이와 유사한 개념을 도입하여 훈련한다. 즉, EBGAN은 GAN 프레임워크이기 때문에 생성된 가짜 데이터에 대해 높은 에너지 값을 갖도록 훈련한다. 이렇게 함으로써 생성자를 일종의 정규화(regularization)로 간주할 수 있다.

이러한 접근 방식에서, 생성자는 생성된 가짜 데이터를 주어진 판별자에게 진짜 데이터로 오인하게끔 만들려고 노력합니다. 판별자는 진짜 데이터와 가짜 데이터를 구분하도록 최선을 다합니다. 따라서 생성자가 완벽하게 입력 데이터를 재구성하는 것을 피하고, 판별자가 데이터의 특징을 식별하는 데 도움이 되도록 에너지 값을 조정한다.
이와 같은 방식으로, EBGAN은 생성자를 정규화 메커니즘으로 활용하여 입력 데이터의 특징을 더욱 잘 포착할 수 있도록 돕는다.


### Repelling Regularizer 

본 논문에서는 Auto Encoder에 잘 어울리는 "Repelling Regularizer"를 제안한다. 

![](../../Data/논문_EBGAN/EBG_3.png)

Repelling Regularizer(또는 Repelling Loss)는 생성된 가짜 데이터가 서로 다른 특징을 갖도록 유도하는 정규화 기법이다. 이를 통해 생성자가 다양하고 다른 형태의 가짜 데이터를 생성하도록 돕는다.

Repelling Regularizer는 생성된 가짜 데이터 포인트 간의 거리를 증가시키는 방향으로 작동한다. 이는 가짜 데이터가 서로 가깝게 군집되는 것을 방지하고, 공간 전체에 걸쳐 더 분산되어 분포하도록 돕는 역할을 한다.

이를 수학적으로 표현하면, Repelling Regularizer는 가짜 데이터 포인트들 사이의 거리를 최대화하기 위해 데이터 포인트 쌍의 거리를 측정하고 해당 거리를 최소화하는 손실 함수를 추가한다. 이 손실 함수는 가짜 데이터 포인트 쌍의 거리 합을 최소화하는 방향으로 최적화된다.

Repelling Regularizer는 생성자의 출력 공간을 탐색하고 다양한 가짜 데이터를 생성하는 데 도움이 된다. 이를 통해 생성자는 다양한 형태와 특징을 갖는 가짜 데이터를 생성하여 더욱 다양하고 풍부한 데이터 분포를 모델링할 수 있다.

즉, Repelling Regularizer는 생성된 가짜 데이터의 다양성을 증가시키고, 훈련 데이터의 다양성과 비슷한 분포를 학습할 수 있도록 도와준다. 이는 생성자의 품질을 향상시키고 더 현실적이고 다양한 가짜 데이터를 생성할 수 있게 해준다.


두 가짜 데이터 포인트 $x_i$와 $x_j$ 사이의 거리를 $d(x_i, x_j)$로 나타내고 이 거리를 최소화하는 손실 함수로 Repelling Regularizer를 사용하게 되면 손실 함수는 다음과 같이 정의됩니다:

$L_{\text{repel}} = \frac{1}{N(N-1)} \sum_{i=1}^{N} \sum_{j\neq i}^{N} d(x_i, x_j)$

여기서 $N$은 가짜 데이터 포인트의 수

Repelling Regularizer의 값이 작아지면 가짜 데이터 포인트들 사이의 거리가 멀어지게 된다. 이를 통해 가짜 데이터 포인트들이 서로 멀리 떨어지도록 유도하여 분포의 다양성을 증가시킨다.

$d(x_i, x_j)$는 일반적으로 유클리디안 거리 또는 맨하탄 거리와 같은 거리 측정 방법을 사용하여 계산된다. 이 거리 함수는 데이터 포인트 사이의 유사성을 측정하는 척도로 사용된다.

따라서 Repelling Regularizer를 통해 생성자는 다양한 가짜 데이터를 생성하고, 가짜 데이터 포인트들이 서로 다른 형태와 특징을 갖도록 유도한다. 이는 생성자가 보다 다양한 데이터 분포를 모델링하고 훈련 데이터의 다양성을 따라갈 수 있도록 도와준다.














### 용어 

- Data Manifold 
데이터 공간에는 '데이터 매니폴드(Data Manifold)'라는 개념이 있습니다. 이는 고차원 데이터 공간에서 실제 데이터가 존재하는 저차원 부분공간을 의미합니다. 예를 들어, 모든 가능한 RGB 이미지를 생각해보면, 이들 중 실제로 유의미한 이미지는 그 공간의 매우 작은 부분에 위치하게 됩니다. 이렇게 데이터가 주로 존재하는 공간을 '데이터 매니폴드'라고 부릅니다.


- Energy Function
이 함수는 데이터 매니폴드에서의 각 점들이 얼마나 '자연스러운' 데이터를 나타내는지를 측정하는 것으로 볼 수 있습니다. 이 함수는 주어진 데이터가 데이터 매니폴드에 가까울수록 낮은 값을, 매니폴드에서 멀수록 높은 값을 출력합니다. 즉, 이는 데이터의 '에너지'를 측정하는 것으로, 이 값이 낮다는 것은 그 데이터가 모델이 표현하려는 데이터 분포에 가깝다는 것을 의미합니다. 반대로, 에너지 값이 높다는 것은 그 데이터가 모델이 표현하려는 데이터 분포에서 멀리 떨어져 있다는 것을 의미합니다.


- Energy-based Model
에너지 기반 모델(Energy-based Model, EBM)은 이 에너지 함수를 최소화하는 방향으로 학습을 진행합니다. EBM에서의 학습 목표는 데이터의 에너지를 낮추고 (즉, 데이터 매니폴드에 가깝게 만드는 것), 또한 데이터 매니폴드 외부의 에너지를 높이는 것입니다. 이를 통해 모델은 실제 데이터 분포를 더 잘 표현할 수 있게 됩니다.


- Margin loss
판별자를 학습시키는 데 사용되는 특별한 손실 함수입니다. 이는 판별자가 실제 데이터와 생성된 데이터를 잘 구별할 수 있도록 돕습니다. 
기본적으로, 마진 손실은 실제 데이터에 대한 에너지와 생성된 데이터에 대한 에너지 사이의 '마진'을 최대화하려는 목표를 가지고 있습니다. 이는 실제 데이터에 대한 에너지는 가능한 한 낮게 유지하면서, 동시에 생성된 데이터에 대한 에너지는 가능한 한 높게 유지하려는 것을 의미합니다.
수학적으로, 마진 손실은 다음과 같이 표현될 수 있습니다:
L = D(x) + max(0, m - D(G(z)))
여기서 D(x)는 실제 데이터 x에 대한 판별자의 출력(에너지)이고, D(G(z))는 생성된 데이터 G(z)에 대한 판별자의 출력입니다. m은 마진 파라미터로, 이 값은 실제 데이터와 생성된 데이터 사이의 에너지 차이에 대한 기대치를 설정합니다.
이 마진 손실 함수는 판별자가 실제 데이터와 생성된 데이터를 잘 구별하도록 돕는데, 실제 데이터에 대한 판별자의 에너지를 낮추고, 생성된 데이터에 대한 에너지를 높이는 방향으로 판별자를 학습시킵니다.


- Nash Equilibrium (내쉬 균형)
생성자와 판별자 간의 상호 작용에서 최적의 상태를 의미

EBGAN에서의 Nash 균형은 다음 조건을 만족하는 상태입니다
1.  생성자가 주어진 판별자에 대해 최적의 가짜 데이터를 생성하고,
2.  판별자가 주어진 생성자에 대해 최적의 구분을 수행합니다.
즉, 생성자는 판별자가 가짜 데이터를 진짜로 오인하도록 최선의 노력을 기울이며, 판별자는 진짜 데이터와 가짜 데이터를 올바르게 구분하기 위해 최선을 다해야 합니다. 이러한 상황에서 생성자와 판별자는 더 이상 개선할 수 없는 최적의 상태에 도달하게 되고, 이것이 Nash 균형입니다.
Nash 균형은 EBGAN에서 안정적인 학습을 도와주며, 생성자와 판별자 간의 균형을 유지하는 중요한 요소입니다.

