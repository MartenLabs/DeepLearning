# (PGGAN)
https://happy-jihye.github.io/gan/gan-5/


### 1. Introduction

Autoregressive models(ex. PixelCNN), VAEs, GANs 등 많은 생성모델들이 있다. 본 논문은 이 중에서도 GAN의 architecture를 사용한 논문이다. 

- Autoregressive models : sharp images, slow to evaluate, no latent space 
- VAE : fast to train, blurry images
- GANs: sharp images, low resolution, limited variation, unstable training

#### GAN 
GAN은 서로 경쟁하는 두 네트워크인 generator와 discriminator로 구성되어 있다. generator는 실제 데이터를 모방하는 새로운 데이터를 생성하고, discriminator는 이 데이터가 실제인지 아닌지를 판별한다. 두 네트워크는 지속적으로 서로를 개선시키면서 학습한다. 최종적으로, GAN은 실제와 구별하기 어려운 고품질의 데이터를 생성할 수 있게 된다. 

GAN에서는 generator를 잘 학습시키는 것이 중요하다.(discriminator는 학습중에만 사용되고 이후에는 버러진다.)


#### Challenge
GAN에는 해결해야 할 문제점들이 있다.
1. generated distribution과 training distribution들이 겹치는 부분(overlap)이 적다면, 이 분포들간의 거리를 측정할 때 gradient는 random한 방향을 가리킬 수 있다. 
    - original GAN에서는 Jensen-Shannon Divergence를 distance metric으로 사용했다면, 최근에는 least squares나 Wasserstein distance등의 metric을 사용해서 모델을 안정화 시켰다.

2. mode collapse: generated distribution이 실제 데이터의 분포를 모두 커버하지 못하고 다양성을 잃어버리는 현상을 뜻한다. G는 그저 loss만을 줄이려고 학습을 하기 때문에 전체 데이터 분포를 찾지 못하게 되고, 결국에는 하나의 mode에만 강하게 몰리게 되는 경우이다. 
   - 예를 들어, MNIST에서 G가 특정 숫자만을 생성하게되는 경우가 이게 속한다. 

3. High-resolution의 image를 생성할 수록, 가짜 이미지라고 판별하기 쉬워진다. 

4. High-resolution의 이미지를 만들기 위해서는 memory constraint 때문에 더 작은 minibatch를 사용해야하고, training stability 역시 떨어진다. 

$\star$ 따라서 이러한 문제점들을 해결하기 위해 PGGAN에서는 Generator와 Discriminator를 점진적으로 학습시킨다. 즉, 만들기 쉬운 low-resolution부터 시작하여 새로운 layer를 조금씩 추가하고 higher-resolution의 detail들을 생성한다. 




### 2. Progressive Growing of GANs
![](../../Data/논문_PGGAN/pggan1.gif)
위의 그림처럼 PGGAN은 low-resolution의 image에서 시작하여 점차 layer를 추가하면서 high-resolution을 학습하게 된다. 또한, discriminator는 generator와 대칭의 형태를 이루고 있으며 모든 layer들을 학습할 수 있다. 


> 처음에는 large scale(low frequency)의 정보들을 학습하고, 점차 fine scale(higher frequency)의 정보들을 학습하게 된다. 



![](../../Data/논문_PGGAN/pggan2.png)
이러한 Progressive training은 몇가지 장점이 있다. 

1. Stable: low-resolution의 이미지를 학습하면 class information도 적고 mode도 몇없기 때문에 안정적이다.
2. Reduced training time: PGGAN은 lower resolution에서부터 비교하여 학습을 하기 때문에 학습속도가 2~6배 빨라진다. 



#### Fading in higher resolution layers 
![](../../Data/논문_PGGAN/pggan3.png)

G와 D의 resolution을 upsampling 할 때, PGGAN은 새로운 layer에 fade in 하는 방식을 사용한다. 

- **(a)**: 이 단계에서는 생성자(G)와 판별자(D)가 16x16 이미지를 처리한다. 생성자는 랜덤 노이즈를 받아 이미지를 생성하고, 판별자는 이 이미지가 실제 이미지인지 아닌지 판별한다. 이미지는 `toRGB` 레이어를 통해 RGB 색상으로 표현되며, 판별자는 `fromRGB` 레이어를 통해 RGB 이미지를 피처 벡터로 변환한다.
    
- **(b)**: 이 단계는 해상도가 16x16에서 32x32로 증가하는 과정을 나타낸다. 이 때 새로운 레이어들은 점차적으로 부드럽게 페이드인된다. 해상도가 높아진 레이어들은 가중치 $\alpha$가 0에서 1로 선형적으로 증가하는 잔차 블록처럼 처리된다. 생성자의 출력은 두 가지 해상도를 결합하고, 판별자의 입력은 실제 이미지의 두 가지 해상도를 보간한다. 이는 훈련 과정 동안 생성자와 판별자가 더 높은 해상도의 이미지를 처리하는 방법을 점차적으로 배울 수 있게 한다.
    
- **(c)**: 이 단계에서는 생성자와 판별자가 32x32 이미지를 처리한다. 이 단계는 (a) 단계와 비슷하지만, 이미지의 해상도가 더 높아진다. 생성자는 이제 더 높은 해상도의 이미지를 생성하고, 판별자는 더 높은 해상도의 이미지를 판별한다.




### 3. Increasing Variation using Minibatch Standard Deviation

PGGAN에서는 mode collapsing을 해결하기 위한 한가지 방법인 Mini-batch discrimination의 방식을 사용한다. mini-batch 별로 생성이미지와 실제 이미지 사이의 거리 합의 차이를 목적함수에 추가하는 것이다. 

- 이 값을 discriminator의 어디에나 추가해도 되지만, 보통은 맨 뒤에 추가하곤 한다
- 이 방식 외에 repelling regularizer를 사용할 수도 있다. 




### 4.  Normalization in Generator and Discriminator 

GAN에서는 G와 D가 경쟁을 할 때 signal magnitude(신호의 크기)가 커지기 쉽다. 따라서 보통은 batch normalization을 하곤 한다. 그런데 PGGAN에서는 signal magnitude을 할 때 이러한 현상이 나타나지 않기 때문에 parameter를 학습시키기 위한 방식으로 다른 접근 방식을 사용한다. 




### 4.1 Equalized Learning Rate 

batch size가 큰 일반 GAN의 경우 batch norm을 사용해도 문제가 없지만, PGGAN에서는 high-resolution의 이미지를 생성해야하기 때문에 작은 사이즈의 batch를 사용하게 되고 그렇기 때문에 initialization이 굉장히 중요해진다. 

본 논문에서는 모든 layer의 learning speed가 같도록 equalized learning rate의 방식을 사용한다. 
gradient의 학습 속도가 parameter와 무관하도록 standard deviation으로 gradient를 normalize하는 방식이다. (weight를 N(0, 1)의 정규 분포에서 initialization한 후, runtime시에 scaling 해준다.)





### 4.2 Pixelwise Feature Vector Normalization in Generator

D와 G가 경쟁을 하면서 크기가 control이 잘 안되는 경우를 대비하여 PGGAN에서는 convolution layer 후 generator에서 각 pixel 별로 normalization을 해준다. 

$$b_{x, y}=a_{x, y} / \sqrt{\frac{1}{N} \sum_{j=0}^{N-1}\left(a_{x, y}^{j}\right)^{2}+\epsilon}$$




### 5. Experiments 
![](../../Data/논문_PGGAN/pggan4.png)
![](../../Data/논문_PGGAN/pggan5.png)

다른 GAN들과 비교했을 때 고해상도 이미지가 잘 출력된다.

![](../../Data/논문_PGGAN/pggan6.png)

학습속도 역시 매우 빠르다.





  

