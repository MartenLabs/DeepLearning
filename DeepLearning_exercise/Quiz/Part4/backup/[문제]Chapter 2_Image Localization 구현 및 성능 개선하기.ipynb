{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Image Localization 구현 및 성능 개선하기.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5cwKFxzsBkf0"},"source":["# 주제: CNN을 이용한 Image Localization 구현하기"]},{"cell_type":"markdown","metadata":{"id":"eTnghmuFBrEQ"},"source":["## 실습 가이드\n","    1. 데이터를 다운로드하여 Colab에 불러옵니다.\n","    2. 필요한 라이브러리는 import 가능합니다.\n","    3. 코드는 위에서부터 아래로 순서대로 실행합니다.\n","\n","## 데이터 소개\n","    - 이번 주제는 Oxford-IIIT Pet dataset을 사용합니다.\n","    - Oxford-IIIT Pet dataset은 37종의 개, 고양이로 구성된 dataset입니다.\n","    - 파일은 하나의 압축파일로 구성되며 압축파일은 다음과 같이 구성되어 있습니다.\n","    - 모든 데이터의 이름은 class_name.(확장자)로 구성되어 있으며 class_name의 첫 알파벳이 대문자이면 '고양이', 소문자이면 '개'입니다.\n","\n","    1. images\n","      - image 데이터들이 포함되어 있음\n","      - 확장자는 모두 jpg임\n","      - RGB이미지 외에도 Gray scale이나 RGBA 이미지도 함께 포함되어 있음\n","    2. annotations/xmls\n","      - localization을 위한 bounding-box 위치 정보가 포함되어 있음\n","      - image의 width, height 등 기타 다른 정보도 함께 저장되어 있음\n","      - 확장자는 모두 xml임\n","    3. annotations/trimaps\n","      - segmentation을 위한 segmentation map이 포함되어 있음\n","      - 확장자는 모두 png임\n","      - 이번 실습에서는 사용하지 않음\n","\n","- 원본 데이터 출처: https://www.robots.ox.ac.uk/~vgg/data/pets/\n","\n","## 문제 소개\n","    - 이번 실습에서는 head ROI 정보를 이용하여 개와 고양이의 머리 부분을 찾는(localization) network을 design하고 학습해 보도록 하겠습니다.\n","\n","## 최종 목표\n","    - tfrecord 파일 작성 방법 이해\n","    - pre-trained CNN 활용 방법 이해\n","    - CNN을 활용한 localization model 작성\n","    - multi-task learning을 이용한 성능 향상 방법 습득\n","\n","- 출제자: 이진원 강사"]},{"cell_type":"markdown","metadata":{"id":"QPJ5v4aKFi17"},"source":["## Step 1. 데이터 다운로드 및 전처리"]},{"cell_type":"code","metadata":{"id":"b5m2SlNnHzHv"},"source":["## library를 import 합니다\n","## 추가로 필요한 library가 있으면 추가로 import 해도 좋습니다\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import os\n","import re\n","from PIL import Image\n","import shutil\n","import xml.etree.ElementTree as et\n","from sklearn.model_selection import train_test_split\n","import random\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import gdown\n","\n","print(tf.__version__)\n","print(keras.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vGwb0M49Fi8w"},"source":["### 문제 1. 데이터 불러오기\n","\n","    - data는 아래 url(google drive)에 저장되어 있습니다.(zip 파일)\n","    - gdown library를 이용하여 data를 다운받고, zip파일 압축을 풀어줍니다.\n","    - 압축 파일은 oxford_pet이라는 이름의 directory에 풀도록 합니다."]},{"cell_type":"code","metadata":{"id":"UC4k5nbeBNQ4"},"source":["data_url = \"https://drive.google.com/uc?id=1dIR9ANjUsV9dWa0pS9J0c2KUGMfpIRG0\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9gSPO_hLI8k"},"source":["## data download 받기(gdown.download 사용)\n","##### CODE HERE #####\n","\n","## oxford_pet directory에 압축풀기\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5-3-nDo1Lz_6"},"source":["## 압축이 풀린 directory 확인\n","!ls oxford_pet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bl-VlnWQF4Ew"},"source":["### 문제 2. 이미지 데이터와 bounding box 데이터 갯수 확인\n","\n","    - image data는 images directory에, bounding box 정보는 annotations/xmls directory에 있습니다.\n","    - image data의 확장자는 .jpg이고, bounding box 정보가 있는 파일은 .xml입니다.\n","    - 각각의 directory에서 .jpg파일의 수(n_images) .xmls 파일의 수(n_bboxes)를 확인해봅니다."]},{"cell_type":"code","metadata":{"id":"HOezA6vNR-R4"},"source":["## directory 정보\n","cur_dir = os.getcwd()\n","data_dir = os.path.join(cur_dir, 'oxford_pet')\n","image_dir = os.path.join(data_dir, 'images')\n","bbox_dir = os.path.join(data_dir, 'annotations', 'xmls')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YP8l5Tr1F4Ex"},"source":["## image file 수 확인\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s1mtusxBSBD_"},"source":["## localization을 위한 bounding box 정보가 있는 xml file의 수 확인\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"99U0yCOIF4aA"},"source":["### 문제 3. 데이터 확인하기\n","\n","    - 임의의 image file과 bounding box 정보를 읽어서 화면에 출력해봅시다.\n","    - image file은 numpy array형태로 image라는 변수에 저장합니다.\n","    - bounding box 정보를 위하여, xml.etree.ElementTree library를 활용하여 xml 파일을 parsing합니다.\n","    - bounding box는 matplotlib.patches의 Rectangle을 이용합니다.\n","    - 이를 위하여 필요한 bounding box의 좌측 상단 좌표(x,y)와 width, height는 각각 (rect_x, rect_y)로, rect_w, rect_h로 저장합니다.    "]},{"cell_type":"code","metadata":{"id":"AMmXX7ipF4aB"},"source":["##### CODE HERE #####\n","\n","rect = Rectangle((rect_x, rect_y), rect_w, rect_h, fill=False, color='red')\n","plt.axes().add_patch(rect)\n","plt.imshow(image)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hKobLXuEF4mA"},"source":["### 문제 4. RGB 외의 data 삭제하기\n","\n","    - image 파일 중에는 채널 수가 3이 아닌 즉 RGB가 아닌 형태의 파일들이 존재합니다.\n","    - 채널 수가 3이 아닌 경우에는 따로 처리를 해주어야 하는 불편함이 있으므로 RGB 외의 image는 삭제합니다.\n","    - PIL.Image library에서는 image의 mode를 제공하는데, 이를 이용하여 RGB가 아닌 image를 삭제할 수 있습니다.\n","    - 이때 같은 이름을 갖는 bounding box file도 같이 삭제하도록 합시다. (예: aaa.jpg 파일 삭제 시에 aaa.xml 파일도 같이 삭제)"]},{"cell_type":"code","metadata":{"id":"5dfooty6F4mB"},"source":["## image file들을 읽어서 channel이 3이 아닌 image는 삭제\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WrYpApI4Y-ZN"},"source":["    - 다시 위의 문제 2번에서 확인했던 image file 수와 xml file 수를 확인해봅니다.\n","    - image file은 7378개, xml file은 3685개가 나오는지 확인합니다.\n","    - 나중에 활용하기 위해 image file과 xml file의 file명을 담은 list를 각각 만들어둡니다."]},{"cell_type":"code","metadata":{"id":"ldUWgTJKZRN6"},"source":["## image file 수 확인\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"znjHWEDEZRN6"},"source":["## localization을 위한 bounding box 정보가 있는 xml file의 수 확인\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ryv5pnmjMHVx"},"source":["## Step 2. tfrecord 파일 만들기\n","\n","    - tfrecord file은 protocal buffer 형태의 binary file입니다.\n","    - image와 bounding box정보를 tfrecord file 형태로 저장해보겠습니다.\n","\n","    - tfrecord file을 만드는 방법은 아래 링크를 참고하면 됩니다.\n","https://www.tensorflow.org/tutorials/load_data/tfrecord\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UPbVulAGF4sh"},"source":["### 문제 5. train/validation data 나누기"]},{"cell_type":"markdown","metadata":{"id":"PN6Z32HAZxkl"},"source":["    - tfrecord 파일로 저장할 때 모든 image는 224x224로 resize하여 저장합니다.\n","    - 전체 bounding box file(.xml)수가 3685개 이므로 image file수는 더 많다고 하더라도 이 3685개 data만 활용하도록 하겠습니다.\n","    - 이 중에 training set으로 3000개, validation set으로 나머지 685개를 사용합니다.\n","    - test set은 따로 구성하지 않고, 나중에 인터넷에서 임의의 사진을 down받아서 확인하는 것으로 대신합니다."]},{"cell_type":"code","metadata":{"id":"DJ_qLtqgF4sh"},"source":["## 필요한 상수들\n","IMG_SIZE = 224\n","N_DATA = 3685\n","N_TRAIN = 3000\n","N_VAL = 685"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k-Hk9xL8acUk"},"source":["    - bounding box file 중에 random하게 3000개를 뽑아서 training set, 나머지는 validation set으로 사용합니다.\n","    - bounding box file과 같은 이름(확장자는 .jpg)을 갖는 image file도 똑같이 training set과 validation set으로 나눕니다.\n","    - 이 때 image file과 xml file의 list를 가지고 있는 경우 index를 random으로 3000개, 685개로 뽑아서 활용하면 편합니다."]},{"cell_type":"code","metadata":{"id":"fd5q8lnJbzVc"},"source":["## train/validation data 나누기\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2TismppkF4vZ"},"source":["### 문제 6. tfrecord writer 생성"]},{"cell_type":"markdown","metadata":{"id":"P16CwIi1fwOK"},"source":["    - training과 validation data를 위한 tfrecord file의 경로 및 파일명을 각각 설정합니다.\n","    - 각각의 tfrecord file을 write하기 위한 writer를 생성합니다.(tf.io.TFRecordWriter 사용)"]},{"cell_type":"code","metadata":{"id":"kScVh55RF4vZ"},"source":["## TFRecord 저장할 directory와 file 경로 설정\n","##### CODE HERE(optional) #####\n","\n","tfr_train_dir = ##### CODE HERE #####\n","tfr_val_dir = ##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWc0TPowgbHO"},"source":["## TFRecord writer 생성\n","writer_train = ##### CODE HERE #####\n","writer_val = ##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xecYsYctgk0p"},"source":["    - tfrecord file을 작성할 때 아래 3개의 함수를 사용하여 표준 TensorFlow 유형을 tf.train.Example 호환 tf.train.Feature로 변환합니다. (from TensorFlow hompage)"]},{"cell_type":"code","metadata":{"id":"z1a2o62RglAZ"},"source":["# The following functions can be used to convert a value to a type compatible\n","# with tf.train.Example.\n","\n","def _bytes_feature(value):\n","  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n","  if isinstance(value, type(tf.constant(0))):\n","    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n","  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n","\n","def _float_feature(value):\n","  \"\"\"Returns a float_list from a float / double.\"\"\"\n","  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n","\n","def _int64_feature(value):\n","  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n","  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lEEYIsbjF4yR"},"source":["### 문제 7. training용 tfrecord 파일 작성\n","\n","    - training data에서 image와 bounding box 정보를 하나씩 읽어서 tfrecord writer를 이용하여 tfrecord file에 write합니다.\n","    - 이 때, image는 bytes feature 형태로 저장합니다. (위 3개 함수 중 맨 위 함수 사용)\n","    - bounding box 정보는 bounding box의 center 좌표(x, y)와 width, height 이 4개의 숫자를 각각 floating point로 저장합니다.\n","    - 이 때, x, y, width, height 값을 image의 width와 height로 나누어 모두 0에서 1 사이의 값이 되도록 합니다.\n","    - 추가로 개를 0번 class, 고양이를 1번 class로 하여 class label도 tfrecord 파일에 같이 저장합니다(int type 사용)\n","    - 이 class 정보는 뒤에서 multi-task learning을 활용할 때 사용하게 됩니다.\n","    - 개는 파일명의 첫 알파벳이 소문자이고, 고양이는 대문자이기 때문에 이를 이용하여 구분할 수 있습니다.\n","    - ((중요)) tfrecord file을 저장할 때 feature의 key 값은 다음과 같은 이름을 사용합니다.\n","      - 'image': image\n","      - 'cls_num': 개/고양이 class 번호\n","      - 'x': bounding box의 center x 좌표\n","      - 'y': bounding box의 center y 좌표\n","      - 'w': bounding box의 width\n","      - 'h': bounding box의 height\n","    - ((주의)) training data를 모두 tfrecord file에 write한 후에는 반드시 writer를 close 합니다."]},{"cell_type":"code","metadata":{"id":"5T7cMDdOF4yS"},"source":["for ##### CODE HERE #####\n","  ##### CODE HERE #####\n","  \n","  example = tf.train.Example(features=tf.train.Features(feature={\n","      'image': _bytes_feature(bimage),      \n","      'cls_num': _int64_feature(cls_num),\n","      'x': _float_feature(x),\n","      'y': _float_feature(y),\n","      'w': _float_feature(w),\n","      'h': _float_feature(h)\n","  }))\n","  writer_train.write(example.SerializeToString())\n","\n","writer_train.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMnSg1j-F40p"},"source":["### 문제 8. validation용 tfrecord 파일 작성\n","\n","    - validation data에서 image와 bounding box 정보를 하나씩 읽어서 tfrecord writer를 이용하여 tfrecord file에 write합니다.\n","    - 이 때, image는 bytes feature 형태로 저장합니다. (위 3개 함수 중 맨 위 함수 사용)\n","    - bounding box 정보는 bounding box의 center 좌표(x, y)와 width, height 이 4개의 숫자를 각각 floating point로 저장합니다.\n","    - 이 때, x, y, width, height 값을 image의 width와 height로 나누어 모두 0에서 1 사이의 값이 되도록 합니다.\n","    - 추가로 개를 0번 class, 고양이를 1번 class로 하여 class label도 tfrecord 파일에 같이 저장합니다(int type 사용)\n","    - 이 class 정보는 뒤에서 multi-task learning을 활용할 때 사용하게 됩니다.\n","    - 개는 파일명의 첫 알파벳이 소문자이고, 고양이는 대문자이기 때문에 이를 이용하여 구분할 수 있습니다.\n","    - ((중요)) tfrecord file을 저장할 때 feature의 key 값은 다음과 같은 이름을 사용합니다.\n","      - 'image': image\n","      - 'cls_num': 개/고양이 class 번호\n","      - 'x': bounding box의 center x 좌표\n","      - 'y': bounding box의 center y 좌표\n","      - 'w': bounding box의 width\n","      - 'h': bounding box의 height\n","    - ((주의)) validation data를 모두 tfrecord file에 write한 후에는 반드시 writer를 close 합니다."]},{"cell_type":"code","metadata":{"id":"NRMV7PRvF40p"},"source":["for ##### CODE HERE #####\n","  ##### CODE HERE #####\n","  \n","  example = tf.train.Example(features=tf.train.Features(feature={\n","      'image': _bytes_feature(bimage),\n","      'cls_num': _int64_feature(cls_num),\n","      'x': _float_feature(x),\n","      'y': _float_feature(y),\n","      'w': _float_feature(w),\n","      'h': _float_feature(h)\n","  }))\n","  writer_val.write(example.SerializeToString())\n","\n","writer_val.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVNGGiGfMq5B"},"source":["## Step 3. Image Localization 모델 작성 및 학습\n","\n","    - 저장한 tfrecord 파일을 읽어서, dataset을 구성하고, neural network model을 만들어서 localization을 학습해봅시다.\n","    - 사용할 hyper parameter 값들은 아래와 같이 정해두었으나, 변경 가능합니다."]},{"cell_type":"code","metadata":{"id":"lO_boEDHjyXZ"},"source":["## Hyper Parameters\n","N_EPOCHS = 40\n","N_BATCH = 40\n","N_VAL_BATCH = 137\n","learning_rate = 0.0001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUTYBFciF425"},"source":["### 문제 9. tfrecord 파일을 활용하여 train/validation dataset 만들기(tf.data)\n","\n","    - tfrecord file을 읽어서 tf.data를 활용하여 model에 data를 공급해주기 위한 dataset을 만듭니다.(tf.data.TFRecordDataset 활용)\n","    - 이 때 아래에 작성된 _parse_function을 사용합니다.  \n","    - train dataset의 이름은 train_dataset으로, validation dataset의 이름은 val_dataset으로 합니다.\n"]},{"cell_type":"code","metadata":{"id":"Fkc7afYTF425"},"source":["## tfrecord file을 data로 parsing해주는 function\n","def _parse_function(tfrecord_serialized):\n","    features={'image': tf.io.FixedLenFeature([], tf.string),\n","              'cls_num': tf.io.FixedLenFeature([], tf.int64),              \n","              'x': tf.io.FixedLenFeature([], tf.float32),\n","              'y': tf.io.FixedLenFeature([], tf.float32),\n","              'w': tf.io.FixedLenFeature([], tf.float32),\n","              'h': tf.io.FixedLenFeature([], tf.float32)              \n","             }\n","    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)\n","    \n","    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)    \n","    image = tf.reshape(image, [IMG_SIZE, IMG_SIZE, 3])\n","    image = tf.cast(image, tf.float32)/255.\n","    \n","    x = tf.cast(parsed_features['x'], tf.float32)\n","    y = tf.cast(parsed_features['y'], tf.float32)\n","    w = tf.cast(parsed_features['w'], tf.float32)\n","    h = tf.cast(parsed_features['h'], tf.float32)\n","    gt = tf.stack([x, y, w, h], -1)\n","    \n","    return image, gt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxt5VIfHmvPn"},"source":["## train dataset 만들기\n","train_dataset = ##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yq8f9jIkmvau"},"source":["## validation dataset 만들기\n","val_dataset = ##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DnLFX2t2F45x"},"source":["### 문제 10. dataset 확인"]},{"cell_type":"markdown","metadata":{"id":"D5UMAlBQ5Kz1"},"source":["      - dataset이 제대로 만들어졌는지 확인하기 위해, validation dataset에서 첫번째 data와 label을 가져와서 image와 bounding box를 화면에 출력해봅니다."]},{"cell_type":"code","metadata":{"id":"jTtSTHYbF45x"},"source":["## validation dataset에서 첫번째 image와 bbox를 읽어서 확인\n","for image, gt in val_dataset.take(1):\n","      \n","  '''그림을 그리기 위해서 bbox의 왼쪽 위 꼭지점 좌표를 계산하고, \n","  xmin, ymin, w, h 각각을 image size에 맞게 scaling'''\n","  ##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ClogediF48J"},"source":["### 문제 11. CNN으로 Localization scratch부터 학습하기"]},{"cell_type":"markdown","metadata":{"id":"Jjuv_g-x5r6i"},"source":["    - 간단한 형태의 CNN을 하나 만들고, scratch부터 학습해봅니다.\r\n","    - CNN의 구조는 자유롭게 만들어도 좋습니다.\r\n","    - loss는 MSE를 사용합니다.\r\n","    - optimizer도 자유롭게 선택합니다.\r\n","    - learning rate을 포함하여 필요한 hyper parameter는 조정해도 좋습니다."]},{"cell_type":"code","metadata":{"id":"xQ9zrNT8IhEp"},"source":["# Sequential API를 사용하여 model 구성\r\n","def create_model():\r\n","    ##### CODE HERE #####\r\n","    return model\r\n","\r\n","## Create model, compile & summary\r\n","model = create_model()\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2lXpH1DMIeGY"},"source":["def loss_fn(y_true, y_pred):\r\n","  ##### CODE HERE #####\r\n","\r\n","## learning rate scheduling(필요시), model.compile\r\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoJPsix2GVeQ"},"source":["## Train!\r\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0iMgrSeDNHVY"},"source":["### 문제 12. 학습 결과 확인"]},{"cell_type":"markdown","metadata":{"id":"SJiyqZ2P7UIF"},"source":["    - validation set에서 data를 5개 이상 뽑아서 예측값과 정답을 한꺼번에 출력해서 얼마나 학습이 잘 되었는지 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"TRWJYJaZNHVZ"},"source":["## 예측한 bounding box와 ground truth box를 image에 같이 표시\r\n","## 정답은 빨간색 box, 예측은 파란색 box\r\n","\r\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-DuN-HztNtfi"},"source":["### 문제 13. IoU 계산하기"]},{"cell_type":"markdown","metadata":{"id":"z21Wiw6HJ97c"},"source":["    - validation set에 대하여 average IoU를 계산해봅시다.\r\n","    - 모든 validation set data에 대하여 각각 IoU를 계산하여 average를 계산합니다."]},{"cell_type":"code","metadata":{"id":"QTpcMMW1PBf-"},"source":["## IOU 계산\r\n","avg_iou = 0\r\n","for val_data, val_gt in val_dataset:\r\n","  ##### CODE HERE #####\r\n","print(avg_iou)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5KajdXQNHcA"},"source":["### 문제 14. pre-trained MobileNetV2로 Localization 학습하기"]},{"cell_type":"markdown","metadata":{"id":"kygiBR3JKbzN"},"source":["    - ImageNet data로 pre-training된 MobileNetV2를 불러와서 localization을 학습해봅시다.\r\n","    - Pretrained model은 tensorflow.keras.applications.mobilenet_v2.MobileNetV2를 불러와서 사용합니다.\r\n","    - global average pooling 이후의 layer는 자유롭게 구성해도 좋습니다.\r\n","    - loss는 MSE를 사용하고, optimizer나 learning rate 등은 자유롭게 구성해도 좋습니다."]},{"cell_type":"code","metadata":{"id":"1kuv3OB-NHcC"},"source":["from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.layers import Conv2D, ReLU, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1m7-wrXTnJCP"},"source":["## MobileNet V2의 pretrained model을 load\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z1a82ejfnJE_"},"source":["## localization을 위한 network\n","## mobilenet v2 구조에 fully connected layer 3개를 추가하고 마지막 layer는 4개의 node로 x,y,w,h를 예측하도록 함\n","def mv2_model():\n","  ##### CODE HERE #####\n","  return model\n","\n","model = mv2_model()\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LrjuLRBTnJIX"},"source":["def loss_fn(y_true, y_pred):\n","  ##### CODE HERE #####\n","\n","## learning rate scheduling(필요시), model.compile\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTrX7hrjnOK0"},"source":["## Train!\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7IYYB30NHep"},"source":["### 문제 15. 학습 결과 확인"]},{"cell_type":"markdown","metadata":{"id":"JzHtoaisLmFn"},"source":["    - validation set에서 data를 5개 이상 뽑아서 예측값과 정답을 한꺼번에 출력해서 얼마나 학습이 잘 되었는지 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"mz246PkiNHep"},"source":["## 예측한 bounding box와 ground truth box를 image에 같이 표시\n","## 정답은 빨간색 box, 예측은 파란색 box\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wKc6xYL7N0aB"},"source":["### 문제 16. IoU 확인"]},{"cell_type":"markdown","metadata":{"id":"EFt3o8mtLq9X"},"source":["    - validation set에 대하여 average IoU를 계산해봅시다.\r\n","    - 모든 validation set data에 대하여 각각 IoU를 계산하여 average를 계산합니다."]},{"cell_type":"code","metadata":{"id":"zsEZWeZ4eghg"},"source":["## IOU 계산\r\n","avg_iou = 0\r\n","for val_data, val_gt in val_dataset:\r\n","  ##### CODE HERE #####\r\n","  \r\n","print(avg_iou)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwT_2cKtNeRb"},"source":["## Step 4. Multi-task Learning을 활용하여 성능 개선하기"]},{"cell_type":"markdown","metadata":{"id":"V7rdbJD5NHhN"},"source":["### 문제 17. Classification branch 추가하기"]},{"cell_type":"markdown","metadata":{"id":"BOrp3XBZMNN4"},"source":["    - pre-trained MobileNetV2의 한 쪽에 개/고양이 2개 class를 구분하는 classification branch를 추가해봅시다.\r\n","    - 앞의 경우와 마찬가지로 classification branch도 global average pooling 이후의 network은 자유롭거 구성해도 좋습니다.\r\n","    - classification label을 추가하가 위하여 tfrecord file을 parsing하는 function부터 수정이 필요합니다.\r\n","    - train_dataset, val_dataset도 재생성합니다."]},{"cell_type":"code","metadata":{"id":"f69PWy0GNHhN"},"source":["## tfrecord file을 data로 parsing해주는 function\r\n","def _parse_function(tfrecord_serialized):\r\n","    ##### CODE HERE #####\r\n","    \r\n","    return image, gt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y7MArEvEwgTR"},"source":["## train dataset 만들기\n","train_dataset = ##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LULig_V-wgTT"},"source":["## validation dataset 만들기\n","val_dataset = ##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YstXkyFdoiug"},"source":["def create_multitask_model():\r\n","  ##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZP7bhx8oi6G"},"source":["model = create_multitask_model()\r\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-JhaoPnNHjg"},"source":["### 문제 18. Loss function 만들고 학습하기"]},{"cell_type":"markdown","metadata":{"id":"AaJCegq0QUfI"},"source":["    - localization branch에는 MSE를 그대로 사용하고, classification branch에 cross-entropy loss를 추가해봅시다.\r\n","    - 두 개의 loss를 적당한 비율로 더하여 전체 loss를 구성합니다.\r\n","    - 많은 경우에 classification 보다 localization loss에 가중치를 좀 더 크게주는 것이 좋습니다.\r\n","    - 이전 model들과 마찬가지로 learning rate과 optimizer를 결정하고 model을 학습해봅시다."]},{"cell_type":"code","metadata":{"id":"GpbS9YmiNHjh"},"source":["def loss_fn(y_true, y_pred):\r\n","  ##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eRf0CJUqumcD"},"source":["## learning rate scheduling(필요시), model.compile\r\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2jxDhgOumfj"},"source":["## Train!\r\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lbpTrJjXNHlx"},"source":["### 문제 19. 학습 결과 확인"]},{"cell_type":"markdown","metadata":{"id":"GH5muDSmQ_g4"},"source":["    - validation set에서 data를 5개 이상 뽑아서 예측값과 정답을 한꺼번에 출력해서 얼마나 학습이 잘 되었는지 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"5ckmhG5UNHly"},"source":["## 예측한 bounding box와 ground truth box를 image에 같이 표시\r\n","## 정답은 빨간색 box, 예측은 파란색 box\r\n","##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0IMRpSNENHoQ"},"source":["### 문제 20. IoU 확인"]},{"cell_type":"markdown","metadata":{"id":"-T4HJ8M-RP0C"},"source":["    - validation set에 대하여 average IoU를 계산해봅시다.\r\n","    - 모든 validation set data에 대하여 각각 IoU를 계산하여 average를 계산합니다.\r\n","    - 이전 model들과 결과를 비교하여 classification branch가 성능 향상에 도움이 되었는지 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"7e25oT2PNHoR"},"source":["## IOU 계산\r\n","avg_iou = 0\r\n","for val_data, val_gt in val_dataset:\r\n","  ##### CODE HERE #####\r\n","\r\n","print(avg_iou)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AE8sfbtVNHqh"},"source":["### 문제 21. 새로운 image로 test하기"]},{"cell_type":"markdown","metadata":{"id":"u0RBGnXQRdOO"},"source":["    - 인터넷에서 개나 고양이의 사진을 다운받아서, 학습된 model에 넣고 제대로 localization이 되는지 확인해봅시다.\r\n","    - image size를 224x224로 resize하고 픽셀 값을 225로 나누는 preprocessing은 동일하게 적용합니다.\r\n","    - CNN은 4차원의 입력을 가지므로, 입력 image를 4차원으로 확장하는 과정이 필요합니다.\r\n","    - localization 결과를 화면에 출력하여 직접 확인해봅시다."]},{"cell_type":"code","metadata":{"id":"x2YH9exV9hwF"},"source":["##### CODE HERE #####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXl_xIM24Mgm"},"source":[""],"execution_count":null,"outputs":[]}]}