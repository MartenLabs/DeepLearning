{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.2.0+cu118\n",
      "torchvision: 0.17.0+cu118\n",
      "ignite: 0.4.13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import ignite\n",
    "\n",
    "print(*map(lambda m: \": \".join((m.__name__, m.__version__)), (torch, torchvision, ignite)), sep=\"\\n\")\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\"\n",
    "\n",
    "if 'CUDA_LAUNCH_BLOCKING' in os.environ:\n",
    "    del os.environ['CUDA_LAUNCH_BLOCKING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "import ignite.distributed as idist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_list = [\"4x4\", \"8x8\", \"16x16\", \"32x32\", \"64x64\", \"128x128\", \"256x256\", \"512x512\"]\n",
    "channel_list = [256, 128, 128, 64, 64, 32, 16, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class WSConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * kernel_size ** 2)) ** 0.5\n",
    "\n",
    "        # self.bias.shape: (out_channels)\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, C, H, W) / (batch_size, 1, H, W)\n",
    "        out = x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class UpDownSampling(nn.Module):\n",
    "\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.interpolate(x, scale_factor=self.size, mode=\"nearest\")\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class GeneratorConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, step, scale_size):\n",
    "        super().__init__()\n",
    "        self.up_sampling = UpDownSampling(size=scale_size)\n",
    "\n",
    "        # (C_(step-1), H, W) -> (C_step, H, W)\n",
    "        self.conv1 = WSConv2d(in_channels=channel_list[step-1], out_channels=channel_list[step], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # (C_step, H, W) -> (C_step, H, W)\n",
    "        self.conv2 = WSConv2d(in_channels=channel_list[step], out_channels=channel_list[step], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.pn = PixelNorm()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.scaled = self.up_sampling(x)\n",
    "        \n",
    "        out = self.conv1(self.scaled)\n",
    "        out = self.leakyrelu(out)\n",
    "        out = self.pn(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.leakyrelu(out)\n",
    "        out = self.pn(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, steps):\n",
    "        super().__init__()\n",
    "\n",
    "        self.steps = steps\n",
    "\n",
    "        self.init = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "\n",
    "            # (z_dim, 1, 1) -> (C_0, 4, 4)\n",
    "            nn.ConvTranspose2d(in_channels=channel_list[0], out_channels=channel_list[0], kernel_size=4, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            # (C_0, 4, 4) -> (C_0, 4, 4)\n",
    "            WSConv2d(in_channels=channel_list[0], out_channels=channel_list[0], kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm()\n",
    "        )\n",
    "\n",
    "        self.init_torgb = WSConv2d(in_channels=channel_list[0], out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.prog_blocks = nn.ModuleList([self.init])\n",
    "        self.torgb_layers = nn.ModuleList([self.init_torgb])\n",
    "        \n",
    "        # append blocks that are not init block.\n",
    "        for step in range(1, self.steps+1):\n",
    "            self.prog_blocks.append(GeneratorConvBlock(step, scale_size=2))\n",
    "            self.torgb_layers.append(WSConv2d(in_channels=channel_list[step], out_channels=1, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "\n",
    "    def fade_in(self, alpha, upsampling, generated):\n",
    "        return alpha * generated + (1 - alpha) * upsampling\n",
    "\n",
    "\n",
    "    def forward(self, x, alpha):\n",
    "        out = self.prog_blocks[0](x)\n",
    "\n",
    "        if self.steps == 0:\n",
    "            return self.torgb_layers[0](out)\n",
    "\n",
    "        for step in range(1, self.steps+1):\n",
    "            out = self.prog_blocks[step](out)\n",
    "\n",
    "        upsampling = self.torgb_layers[step-1](self.prog_blocks[step].scaled)\n",
    "        generated = self.torgb_layers[step](out)\n",
    "\n",
    "        return self.fade_in(alpha, upsampling, generated)\n",
    "\n",
    "\n",
    "\n",
    "class DiscriminatorConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, step):\n",
    "        super().__init__()\n",
    "\n",
    "        # (C_step, H, W) -> (C_step, H, W)\n",
    "        self.conv1 = WSConv2d(in_channels=channel_list[step], out_channels=channel_list[step], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # (C_step, H, W) -> (C_(step-1), H, W)\n",
    "        self.conv2 = WSConv2d(in_channels=channel_list[step], out_channels=channel_list[step-1], kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # (C_(step-1), H/2, W/2)\n",
    "        self.downsample = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.leakyrelu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.leakyrelu(out)\n",
    "\n",
    "        out = self.downsample(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MinibatchStd(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # mean of minibatch's std\n",
    "        # (1) -> (batch_size, 1, H, W)\n",
    "        batch_statistics = (torch.std(x, dim=0).mean().repeat(x.size(0), 1, x.size(2), x.size(3)))\n",
    "\n",
    "        # (batch_size, C, H, W) -> (batch_size, C+1, H, W)\n",
    "        return torch.cat((x, batch_statistics), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, steps):\n",
    "        super().__init__()\n",
    "        # progressive growing blocks\n",
    "        self.prog_blocks = nn.ModuleList([])\n",
    "\n",
    "        # fromrgb layers\n",
    "        self.fromrgb_layers = nn.ModuleList([])\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.steps = steps\n",
    "        \n",
    "        # append blocks that are not final block.\n",
    "        for step in range(steps, 0, -1):\n",
    "            self.prog_blocks.append(DiscriminatorConvBlock(step))\n",
    "            self.fromrgb_layers.append(WSConv2d(in_channels=1, out_channels=channel_list[step], kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "        # append final block\n",
    "        self.fromrgb_layers.append(\n",
    "            WSConv2d(in_channels=1, out_channels=channel_list[0], kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        # append final block\n",
    "        self.prog_blocks.append(\n",
    "            nn.Sequential(\n",
    "                MinibatchStd(),\n",
    "                WSConv2d(in_channels=channel_list[0]+1, out_channels=channel_list[0], kernel_size=3, stride=1, padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                WSConv2d(in_channels=channel_list[0], out_channels=channel_list[0], kernel_size=4, stride=1, padding=0),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                WSConv2d(in_channels=channel_list[0], out_channels=1, kernel_size=1, stride=1, padding=0),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # downsample\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    \n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    \n",
    "    def forward(self, x, alpha):\n",
    "        # (3, H, W) -> (C, H, W)\n",
    "        out = self.leakyrelu(self.fromrgb_layers[0](x))\n",
    "\n",
    "        if self.steps == 0: # i.e, image size is 4x4\n",
    "            \n",
    "            # (C, 4, 4) -> (1, 1, 1)\n",
    "            out = self.prog_blocks[-1](out)\n",
    "\n",
    "            # (1, 1, 1) -> (1)\n",
    "            # out.size(0) = batch_size\n",
    "            return out.view(out.size(0), -1)\n",
    "        \n",
    "\n",
    "        downscaled = self.leakyrelu(self.fromrgb_layers[1](self.avgpool(x)))\n",
    "        out = self.prog_blocks[0](out)\n",
    "\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "        \n",
    "        for i in range(1, self.steps+1):\n",
    "            out = self.prog_blocks[i](out)\n",
    "\n",
    "        return out.view(out.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self, directory_list, resolution):\n",
    "        self.directory_list = directory_list\n",
    "        self.resolution = resolution\n",
    "\n",
    "\n",
    "    def image_to_tensor(self, path, res):\n",
    "        img = Image.open(path).convert('L').resize(res)\n",
    "\n",
    "        tensor_img = transforms.ToTensor()(img)\n",
    "        tensor_img = tensor_img.type(torch.float16)\n",
    "\n",
    "        return tensor_img\n",
    "\n",
    "\n",
    "    def dataset_to_tensor(self, directory_path):\n",
    "        files = os.listdir(directory_path)\n",
    "        tensor_dataset = torch.zeros((len(files), 1, *self.resolution)).type(torch.float16)\n",
    "\n",
    "        for i in range(len(files)):\n",
    "            tensor_dataset[i] = self.image_to_tensor(f\"{directory_path}/{files[i]}\", self.resolution)\n",
    "        \n",
    "        return tensor_dataset\n",
    "\n",
    "\n",
    "    def extract_dataset(self):\n",
    "        dataset_pair = []\n",
    "\n",
    "        for directory_path in self.directory_list:\n",
    "            dataset_pair.append(self.dataset_to_tensor(directory_path))\n",
    "\n",
    "        return dataset_pair\n",
    "    \n",
    "\n",
    "def make_gif(paths, save_path, fps=500):\n",
    "    img, *imgs = [Image.open(path) for path in paths]\n",
    "    img.save(fp=save_path, format=\"GIF\", append_images=imgs, save_all=True, duration=fps, loop=1)\n",
    "\n",
    "\n",
    "def merge_test_pred(pred):\n",
    "\n",
    "    test_size = pred.size(0)\n",
    "    \n",
    "    # ex) test_size = 30 -> height = 5, weight = 6\n",
    "    for i in range(int(np.sqrt(test_size)), test_size+1):\n",
    "        if test_size % i == 0:\n",
    "            n_height = max(i, test_size//i)\n",
    "            n_weight = min(i, test_size//i)\n",
    "            break\n",
    "    \n",
    "    image_size = (\n",
    "        1024 - (1024 % n_weight),\n",
    "        1024 - (1024 % n_height)\n",
    "    )\n",
    "\n",
    "    one_image_size = (image_size[0] // n_weight, image_size[1] // n_height)\n",
    "\n",
    "    image = Image.new('RGB', image_size)\n",
    "\n",
    "    for w in range(n_weight):\n",
    "        for h in range(n_height):\n",
    "            img = transforms.ToPILImage()(pred[n_height*w + h])\n",
    "            img = img.resize(one_image_size)\n",
    "\n",
    "            image.paste(img, (one_image_size[0] * w, one_image_size[1] * h))\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for resolution in resolution_list:\n",
    "    resolution_pair = tuple(map(int, resolution.split(\"x\")))\n",
    "\n",
    "    dataset = Dataset(\n",
    "        directory_list=[\"HighResolution/FLIR_ThermalDataset/640x512\"],\n",
    "        resolution=resolution_pair\n",
    "    )\n",
    "\n",
    "    # train_0, train_1, valid_0, valid_1 = dataset.extract_dataset()\n",
    "    train_0 = dataset.extract_dataset()\n",
    "\n",
    "    save_paths = [\n",
    "        f\"HighResolution/torch/{resolution}/train_0.pt\",\n",
    "        # f\"HighResolution/torch/{resolution}/train_1.pt\",\n",
    "        # f\"HighResolution/torch/{resolution}/valid_0.pt\",\n",
    "        # f\"HighResolution/torch/{resolution}/valid_1.pt\"\n",
    "    ]\n",
    "\n",
    "    for path in save_paths:\n",
    "        dir_name = os.path.dirname(path)\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)  # os.makedirs는 중간에 없는 모든 디렉토리도 함께 생성합니다.\n",
    "    \n",
    "    torch.save(train_0, save_paths[0])\n",
    "    # torch.save(train_1, save_paths[1])\n",
    "    # torch.save(valid_0, save_paths[2])\n",
    "    # torch.save(valid_1, save_paths[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples, d_alpha):\n",
    "    # alpha 는 0과 1 사이의 무작위 값으로 구성된 텐서. 이 값은 실제 샘플과 가짜 샘플 사이의 보간(interpolation)에 사용. \n",
    "\t# alpha 의 형태는 실제 샘플의 배치 크기와 동일하게 설정.\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=real_samples.device)\n",
    "    # 실제 샘플과 가짜 샘플 사이의 샘플을 생성\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates, d_alpha)\n",
    "    fake = torch.ones(real_samples.shape[0], 1, device=real_samples.device, requires_grad=False)\n",
    "    # autograd.grad를 사용해 그라디언트를 계산\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    # 그라디언트의 크기를 계산\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brightness_loss(y_true, y_pred):\n",
    "    # y_true와 y_pred 사이의 차이의 제곱을 계산\n",
    "    squared_difference = np.square(y_true - y_pred)\n",
    "    \n",
    "    # 평균 제곱 오차를 계산\n",
    "    mse = np.mean(squared_difference)\n",
    "    \n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "\n",
    "dataset_path = [f\"HighResolution/torch/{i}\" for i in resolution_list]\n",
    "model_state_dict_path = [f\"model_state_dict/{i}\" for i in resolution_list]\n",
    "lambda_gp = 3\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self,\n",
    "                steps: int,\n",
    "                batch_size: int,\n",
    "                device: torch.device,\n",
    "                test_size: int\n",
    "            ):\n",
    "\n",
    "        self.steps = steps\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.test_size = test_size\n",
    "\n",
    "        directory_path = dataset_path[self.steps]\n",
    "\n",
    "        self.trainloader = DataLoader(torch.load(f\"{directory_path}/train_0.pt\"), batch_size=self.batch_size, shuffle=True)\n",
    "        # self.validloader = DataLoader(torch.cat((torch.load(f\"{directory_path}/valid_0.pt\"), torch.load(f\"{directory_path}/valid_1.pt\")), dim=0).type(torch.float32), batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.generator = Generator(steps=self.steps).to(self.device)\n",
    "        self.discriminator = Discriminator(steps=self.steps).to(self.device)\n",
    "\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.generator_optim = Adam(self.generator.parameters(), lr=0.002, betas=(0.5, 0.999))\n",
    "        self.discriminator_optim = Adam(self.discriminator.parameters(), lr=0.002, betas=(0.5, 0.999))\n",
    "\n",
    "        # It will be used for testing.\n",
    "        self.test_z = torch.randn((self.test_size, 256, 1, 1)).to(self.device)\n",
    "\n",
    "        self.load_model()\n",
    "\n",
    "\n",
    "    # def save_model(self):\n",
    "    #     # ------------------------------------------------------------------ generator model ---------------------------------------------------------------------------\n",
    "    #     for i in range(self.steps+1):\n",
    "    #         torch.save(self.generator.prog_blocks[i].state_dict(), f\"{model_state_dict_path[self.steps]}/generator_model/prog_blocks_{i}.pt\")\n",
    "    #         torch.save(self.generator.torgb_layers[i].state_dict(), f\"{model_state_dict_path[self.steps]}/generator_model/torgb_layers_{i}.pt\")\n",
    "\n",
    "    #     # ---------------------------------------------------------------- discriminator model -------------------------------------------------------------------------\n",
    "    #     for i in range(self.steps+1):\n",
    "    #         torch.save(self.discriminator.prog_blocks[i].state_dict(), f\"{model_state_dict_path[self.steps]}/discriminator_model/prog_blocks_{i}.pt\")\n",
    "    #         torch.save(self.discriminator.fromrgb_layers[i].state_dict(), f\"{model_state_dict_path[self.steps]}/discriminator_model/fromrgb_layers_{i}.pt\")\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.generator.eval()\n",
    "        self.discriminator.eval()\n",
    "\n",
    "        # 모델 상태를 저장할 기본 경로\n",
    "        base_path = \"model_state_dict\"\n",
    "\n",
    "        for i in range(self.steps + 1):\n",
    "            # 생성자와 판별자의 상태 저장 경로 설정\n",
    "            gen_path = f\"{base_path}/{resolution_list[self.steps]}/generator_model\"\n",
    "            disc_path = f\"{base_path}/{resolution_list[self.steps]}/discriminator_model\"\n",
    "\n",
    "            # 디렉토리가 없으면 생성\n",
    "            os.makedirs(gen_path, exist_ok=True)\n",
    "            os.makedirs(disc_path, exist_ok=True)\n",
    "\n",
    "            # 생성자 모델 상태 저장\n",
    "            torch.save(self.generator.prog_blocks[i].state_dict(), f\"{gen_path}/prog_blocks_{i}.pt\")\n",
    "            torch.save(self.generator.torgb_layers[i].state_dict(), f\"{gen_path}/torgb_layers_{i}.pt\")\n",
    "\n",
    "            # 판별자 모델 상태 저장\n",
    "            torch.save(self.discriminator.prog_blocks[i].state_dict(), f\"{disc_path}/prog_blocks_{i}.pt\")\n",
    "            torch.save(self.discriminator.fromrgb_layers[i].state_dict(), f\"{disc_path}/fromrgb_layers_{i}.pt\")\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.steps == 0:\n",
    "            return\n",
    "\n",
    "        # ------------------------------------------------------------------ generator model ---------------------------------------------------------------------------\n",
    "        for i in range(self.steps):\n",
    "            self.generator.prog_blocks[i].load_state_dict(torch.load(f\"{model_state_dict_path[self.steps-1]}/generator_model/prog_blocks_{i}.pt\"))\n",
    "            self.generator.torgb_layers[i].load_state_dict(torch.load(f\"{model_state_dict_path[self.steps-1]}/generator_model/torgb_layers_{i}.pt\"))\n",
    "\n",
    "        # ---------------------------------------------------------------- discriminator model -------------------------------------------------------------------------\n",
    "        for i in range(1, self.steps+1):\n",
    "            self.discriminator.prog_blocks[i].load_state_dict(torch.load(f\"{model_state_dict_path[self.steps-1]}/discriminator_model/prog_blocks_{i-1}.pt\"))\n",
    "            self.discriminator.fromrgb_layers[i].load_state_dict(torch.load(f\"{model_state_dict_path[self.steps-1]}/discriminator_model/fromrgb_layers_{i-1}.pt\"))\n",
    "    \n",
    "\n",
    "    def clear_cuda_memory(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.generator.eval()\n",
    "        self.discriminator.eval()\n",
    "\n",
    "        pred = self.generator(self.test_z, alpha=self.alpha)\n",
    "        pred = pred.detach().cpu()\n",
    "\n",
    "        # 이미지를 저장할 경로를 생성합니다.\n",
    "        save_path = f\"./train_log/{resolution_list[self.steps]}\"\n",
    "        os.makedirs(save_path, exist_ok=True)  # 해당 경로에 폴더가 없으면 생성합니다.\n",
    "\n",
    "        test_image = merge_test_pred(pred)\n",
    "        test_image.save(fp=f\"{save_path}/epoch-{epoch}.jpg\")\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        generator_avg_loss = 0\n",
    "        discriminator_avg_loss = 0\n",
    "\n",
    "        for _ in range(len(self.trainloader)):\n",
    "            self.alpha += self.alpha_gap\n",
    "\n",
    "            real_image = next(iter(self.trainloader)).to(self.device)\n",
    "\n",
    "            real_label = torch.full((real_image.size(0), 1), 1).type(torch.float).to(self.device)\n",
    "            fake_label = torch.full((real_image.size(0), 1), 0).type(torch.float).to(self.device)\n",
    "\n",
    "            # ---------------------------------------------------------- discriminator train ------------------------------------------------------------\n",
    "            z = torch.randn(real_image.size(0), 256, 1, 1).to(self.device)\n",
    "\n",
    "            fake_image = self.generator(z, alpha=self.alpha)\n",
    "            \n",
    "            d_fake_pred = self.discriminator(fake_image.detach(), alpha=self.alpha)\n",
    "            d_fake_loss = self.criterion(d_fake_pred, fake_label)\n",
    "            # d_fake_loss_L1 = self.criterionL1(d_fake_pred, fake_label)\n",
    "            # d_fake_loss_L2 = self.criterionL2(d_fake_pred, fake_label)\n",
    "            # d_fake_loss = 0.6 * d_fake_loss_L1 + 0.4 * d_fake_loss_L2\n",
    "\n",
    "\n",
    "            d_real_pred = self.discriminator(real_image, alpha=self.alpha)\n",
    "            d_real_loss = self.criterion(d_real_pred, real_label)\n",
    "            # d_real_loss_L1 = self.criterionL1(d_real_pred, fake_label)\n",
    "            # d_real_loss_L2 = self.criterionL2(d_real_pred, fake_label)\n",
    "            # d_real_loss = 0.6 * d_real_loss_L1 + 0.4 * d_real_loss_L2\n",
    "\n",
    "            gradient_penalty = compute_gradient_penalty(self.discriminator, real_image.data, fake_image.data, d_alpha=self.alpha)\n",
    "            d_loss = d_fake_loss + d_real_loss + lambda_gp * gradient_penalty\n",
    "\n",
    "            self.discriminator_optim.zero_grad()\n",
    "            d_loss.backward()\n",
    "            self.discriminator_optim.step()\n",
    "\n",
    "            discriminator_avg_loss += d_loss.item()\n",
    "\n",
    "            # ---------------------------------------------------------- generator train -----------------------------------------------------------------\n",
    "            z = torch.randn(real_image.size(0), 256, 1, 1).to(self.device)\n",
    "\n",
    "            fake_image = self.generator(z, alpha=self.alpha)\n",
    "\n",
    "            d_fake_pred = self.discriminator(fake_image, alpha=self.alpha)\n",
    "            g_loss = self.criterion(d_fake_pred, real_label)\n",
    "            # g_loss_L1 = self.criterionL1(d_fake_pred, real_label)\n",
    "            # g_loss_L2 = self.criterionL2(d_fake_pred, real_label)\n",
    "            # g_loss = torch.mean(0.6 * g_loss_L1 + 0.4 * g_loss_L2)\n",
    "            self.generator_optim.zero_grad()\n",
    "            g_loss.backward()\n",
    "            self.generator_optim.step()\n",
    "\n",
    "            generator_avg_loss += g_loss.item()\n",
    "\n",
    "\n",
    "            self.clear_cuda_memory()\n",
    "\n",
    "        generator_avg_loss /= len(self.trainloader)\n",
    "        discriminator_avg_loss /= len(self.trainloader)\n",
    "\n",
    "        return generator_avg_loss, discriminator_avg_loss\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, epochs):\n",
    "        train_history = []\n",
    "\n",
    "        self.alpha = 0\n",
    "        self.alpha_gap = 1 / (len(self.trainloader) * (epochs[1] - epochs[0]))\n",
    "\n",
    "        for epoch in range(*epochs):\n",
    "            print(\"-\"*100 + \"\\n\" + f\"Epoch: {epoch}\")\n",
    "\n",
    "            train_history.append(self.train())\n",
    "            print(f\"\\tTrain\\n\\t\\tG Loss: {train_history[-1][0]},\\tD Loss: {train_history[-1][1]}\")\n",
    "\n",
    "            self.test(epoch)\n",
    "    \n",
    "        return train_history\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for steps in range(8):\n",
    "        trainer = Trainer(steps=steps, batch_size=16, device=device, test_size=16)\n",
    "        train_history = trainer.run((0, 30))\n",
    "        trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
