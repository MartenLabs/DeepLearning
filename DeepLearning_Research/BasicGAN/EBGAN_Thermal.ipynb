{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.2.0+cu118\n",
      "torchvision: 0.17.0+cu118\n",
      "ignite: 0.4.13\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import ignite\n",
    "\n",
    "print(*map(lambda m: \": \".join((m.__name__, m.__version__)), (torch, torchvision, ignite)), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from ignite.engine import Engine, Events\n",
    "import ignite.distributed as idist\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger ignite.distributed.launcher.Parallel (WARNING)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignite.utils.manual_seed(999)\n",
    "ignite.utils.setup_logger(name=\"ignite.distributed.auto.auto_dataloader\", level=logging.WARNING)\n",
    "ignite.utils.setup_logger(name=\"ignite.distributed.launcher.Parallel\", level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option():\n",
    "    n_epochs = 180   # 훈련할 에포크 수\n",
    "    batch_size = 9  # 배치의 크기\n",
    "    lr = 0.0002      # Adam 옵티마이저의 학습률\n",
    "    b1 = 0.5         # Adam 옵티마이저의 그래디언트의 일차 모멘텀 감쇠\n",
    "    b2 = 0.999       # Adam 옵티마이저의 그래디언트의 이차 모멘텀 감쇠\n",
    "    n_cpu = 16        # 배치 생성 중에 사용할 CPU 스레드 수\n",
    "    latent_dim = 200 # 잠재 공간의 차원\n",
    "    img_size = 256    # 각 이미지 차원의 크기\n",
    "    channels = 3     # 이미지 채널 수\n",
    "    sample_interval = 500  # 이미지 샘플링 간격\n",
    "\n",
    "opt = Option()\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(opt.img_size),\n",
    "        transforms.CenterCrop(opt.img_size),\n",
    "        # transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "    ]\n",
    ")\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset = dset.ImageFolder(root=\"datasets/HighResolution/FLIR\", transform=data_transform)\n",
    "test_dataset = torch.utils.data.Subset(train_dataset, torch.arange(3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=opt.batch_size, \n",
    "    num_workers=8, \n",
    "    shuffle=True, \n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=opt.batch_size, \n",
    "    num_workers=8, \n",
    "    shuffle=False, \n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=latent_dim, out_channels=1024, \n",
    "                               kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.SELU(True),\n",
    "            # state size. 1024 x 4 x 4\n",
    "\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.SELU(True),\n",
    "            # state size. 512 x 8 x 8\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.SELU(True),\n",
    "            # state size. 256 x 16 x 16\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SELU(True),\n",
    "            # state size. 128 x 32 x 32\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SELU(True),\n",
    "            # state size. 64 x 64 x 64            \n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SELU(True),\n",
    "            # state size. 1 x 128 x 128\n",
    "\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # state size. 1 x 256 x 256\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(opt.latent_dim)\n",
    "generator = generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(generator, (opt.latent_dim, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # 인코더 부분\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 256x256 -> 128x128\n",
    "            nn.SELU(True),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 128x128 -> 64x64\n",
    "            nn.SELU(True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 64x64 -> 32x32\n",
    "            nn.SELU(True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),  # 32x32 -> 16x16\n",
    "            nn.SELU(True),\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),  # 16x16 -> 8x8\n",
    "            nn.SELU(True),\n",
    "        )\n",
    "        \n",
    "        # 임베딩 레이어\n",
    "        self.embedding = nn.Linear(512 * 8 * 8, 2048)\n",
    "        \n",
    "        self.decoder_input_layer = nn.Linear(2048, 512 * 8 * 8)\n",
    "        # 디코더 부분\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),  # 8x8 -> 16x16\n",
    "            nn.SELU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),  # 16x16 -> 32x32\n",
    "            nn.SELU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 32x32 -> 64x64\n",
    "            nn.SELU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 64x64 -> 128x128\n",
    "            nn.SELU(True),\n",
    "            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),  # 128x128 -> 256x256\n",
    "            nn.SELU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        encoded = self.encoder(img)\n",
    "        encoded_flat = encoded.view(encoded.size(0), -1)\n",
    "        embedding = self.embedding(encoded_flat)\n",
    "        decoder_input = self.decoder_input_layer(embedding)\n",
    "        decoder_input = decoder_input.view(encoded.size(0), 512, 8, 8)\n",
    "        decoded = self.decoder(decoder_input)\n",
    "        return decoded, embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator = discriminator.to(device)\n",
    "# summary(discriminator, (1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelwise_loss = nn.MSELoss()\n",
    "\n",
    "pixelwise_loss.to(device=device)\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if device else torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullaway_loss(embeddings):\n",
    "    # 각 임베딩의 노름(norm) 계산\n",
    "    norm = torch.sqrt(torch.sum(embeddings ** 2, -1, keepdim=True))\n",
    "    # 임베딩 정규화\n",
    "    normalized_emb = embeddings / norm\n",
    "    # 정규화된 임베딩 간의 유사도 계산\n",
    "    similarity = torch.matmul(normalized_emb, normalized_emb.transpose(1, 0))\n",
    "    batch_size = embeddings.size(0)\n",
    "    # 유사도 행렬에서 대각선을 제외한 모든 요소의 합을 계산하고, 이를 통해 pull away loss 계산\n",
    "    loss_pt = (torch.sum(similarity) - batch_size) / (batch_size * (batch_size - 1))\n",
    "    return loss_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_pt = 0.1\n",
    "margin = max(1, opt.batch_size / 64.0)\n",
    "\n",
    "def training_step(engine, data):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    real_imgs = data[0].to(device)\n",
    "    b_size = real_imgs.size(0)\n",
    "\n",
    "    optimizer_G.zero_grad()\n",
    "    z = torch.randn(b_size, opt.latent_dim, 1, 1, device=device)\n",
    "    gen_imgs = generator(z)\n",
    "    recon_imgs, img_embeddings = discriminator(gen_imgs)\n",
    "\n",
    "    g_loss = pixelwise_loss(recon_imgs, gen_imgs.detach()) + lambda_pt * pullaway_loss(img_embeddings)\n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "\n",
    "\n",
    "    optimizer_D.zero_grad()\n",
    "    real_recon, _ = discriminator(real_imgs)\n",
    "    fake_recon, _ = discriminator(gen_imgs.detach())\n",
    "    \n",
    "    d_loss_real = pixelwise_loss(real_recon, real_imgs)\n",
    "    d_loss_fake = pixelwise_loss(fake_recon, gen_imgs.detach())\n",
    "\n",
    "    d_loss = d_loss_real\n",
    "    if (margin - d_loss_fake.data).item() > 0:\n",
    "        d_loss += margin - d_loss_fake\n",
    "\n",
    "    d_loss.backward()\n",
    "    optimizer_D.step()\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"Loss_G\" : g_loss.item(),\n",
    "        \"Loss_D\" : d_loss.item(),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Engine(training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.STARTED)\n",
    "def init_weights():\n",
    "    discriminator.apply(weights_init_normal)\n",
    "    generator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def store_losses(engine):\n",
    "    o = engine.state.output\n",
    "    G_losses.append(o[\"Loss_G\"])\n",
    "    D_losses.append(o[\"Loss_D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(10, opt.latent_dim, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=500))\n",
    "def store_images(engine):\n",
    "    with torch.no_grad():\n",
    "        fake = generator(fixed_noise).cpu()\n",
    "    img_list.append(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "\n",
    "\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transforms.ToPILImage()(img)\n",
    "        arr.append(transforms.ToTensor()(pil_img))\n",
    "    return torch.stack(arr)\n",
    "\n",
    "\n",
    "def evaluation_step(engine, batch):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(opt.batch_size, opt.latent_dim, 1, 1, device=device)\n",
    "        generator.eval().to(device=device)\n",
    "        fake_batch = generator(noise)\n",
    "        fake = interpolate(fake_batch)\n",
    "        # print(fake.shape)\n",
    "        real = interpolate(batch[0])\n",
    "        # print(real.shape)\n",
    "        return fake, real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Engine(evaluation_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import FID, InceptionScore\n",
    "fid_metric = FID(device=device)\n",
    "is_metric = InceptionScore(device=device, output_transform=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Engine(evaluation_step)\n",
    "fid_metric.attach(evaluator, \"fid\")\n",
    "is_metric.attach(evaluator, \"is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_values = []\n",
    "is_values = []\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(test_dataloader,max_epochs=1)\n",
    "    metrics = evaluator.state.metrics\n",
    "    fid_score = metrics['fid']\n",
    "    is_score = metrics['is']\n",
    "    fid_values.append(fid_score)\n",
    "    is_values.append(is_score)\n",
    "    print(f\"Epoch [{engine.state.epoch}/{opt.n_epochs}] Metric Scores\")\n",
    "    print(f\"*   FID : {fid_score:4f}\")\n",
    "    print(f\"*    IS : {is_score:4f}\")\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake = generator(fixed_noise).to(device)\n",
    "\n",
    "    epoch_number = engine.state.epoch\n",
    "    image_filename = f\"img/FLIR_EBGAN/{epoch_number}_epoch.png\"\n",
    "    save_image(fake.data[:10], image_filename, nrow=5, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import RunningAverage\n",
    "\n",
    "RunningAverage(output_transform=lambda x: x[\"Loss_G\"]).attach(trainer, 'Loss_G')\n",
    "RunningAverage(output_transform=lambda x: x[\"Loss_D\"]).attach(trainer, 'Loss_D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpuadmin/anaconda3/envs/torch/lib/python3.8/site-packages/ignite/contrib/handlers/tqdm_logger.py:126: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from ignite.contrib.handlers import ProgressBar\n",
    "\n",
    "ProgressBar().attach(trainer, metric_names=['Loss_G','Loss_D'])\n",
    "ProgressBar().attach(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(*args):\n",
    "    trainer.run(train_dataloader, max_epochs=opt.n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "real_batch = next(iter(train_dataloader))\n",
    "print(real_batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpuadmin/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([9, 3, 256, 256])) that is different to the input size (torch.Size([9, 1, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Metric Scores\n",
      "*   FID : 0.464624\n",
      "*    IS : 1.707409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpuadmin/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([9, 3, 256, 256])) that is different to the input size (torch.Size([9, 1, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] Metric Scores\n",
      "*   FID : 0.960117\n",
      "*    IS : 1.564816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] Metric Scores\n",
      "*   FID : 0.769778\n",
      "*    IS : 1.887184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] Metric Scores\n",
      "*   FID : 0.823062\n",
      "*    IS : 1.538081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] Metric Scores\n",
      "*   FID : 0.846349\n",
      "*    IS : 1.529455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] Metric Scores\n",
      "*   FID : 0.705635\n",
      "*    IS : 2.124219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] Metric Scores\n",
      "*   FID : 0.532145\n",
      "*    IS : 2.016595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] Metric Scores\n",
      "*   FID : 0.749553\n",
      "*    IS : 1.874181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] Metric Scores\n",
      "*   FID : 0.857552\n",
      "*    IS : 1.322429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] Metric Scores\n",
      "*   FID : 0.850596\n",
      "*    IS : 1.329325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] Metric Scores\n",
      "*   FID : 0.848965\n",
      "*    IS : 1.307050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100] Metric Scores\n",
      "*   FID : 0.765052\n",
      "*    IS : 1.378920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] Metric Scores\n",
      "*   FID : 0.688591\n",
      "*    IS : 1.385739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100] Metric Scores\n",
      "*   FID : 0.859589\n",
      "*    IS : 1.281952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100] Metric Scores\n",
      "*   FID : 0.915492\n",
      "*    IS : 1.248523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100] Metric Scores\n",
      "*   FID : 0.342982\n",
      "*    IS : 1.865908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100] Metric Scores\n",
      "*   FID : 0.329499\n",
      "*    IS : 1.519030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100] Metric Scores\n",
      "*   FID : 0.477110\n",
      "*    IS : 1.762433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100] Metric Scores\n",
      "*   FID : 0.337249\n",
      "*    IS : 1.770704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100] Metric Scores\n",
      "*   FID : 0.583976\n",
      "*    IS : 1.978966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100] Metric Scores\n",
      "*   FID : 0.504738\n",
      "*    IS : 1.977507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100] Metric Scores\n",
      "*   FID : 0.503136\n",
      "*    IS : 1.913811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100] Metric Scores\n",
      "*   FID : 0.387531\n",
      "*    IS : 2.175328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100] Metric Scores\n",
      "*   FID : 0.552491\n",
      "*    IS : 1.693135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [25/180]: [574/1737]  33%|███▎      , Loss_G=3.72, Loss_D=0.0282 [00:28<00:53]"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Genera or and Discriminator Los s During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\") \n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('EBGAN_training_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "plt.title(\"Evaluation Metric During Training\")\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('IS', color=color)\n",
    "ax1.plot(is_values, color=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('FID', color=color)\n",
    "ax2.plot(fid_values, color=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('EBGAN_evaluation_metric.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:9], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(img_list[-1], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "plt.savefig('EBGAN_real_fake_images_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
