{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import ignite\n",
    "\n",
    "print(*map(lambda m: \": \".join((m.__name__, m.__version__)), (torch, torchvision, ignite)), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "import ignite.distributed as idist\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignite.utils.manual_seed(999)\n",
    "# ignite.utils.setup_logger(name=\"ignite.distributed.auto.auto_dataloader\", level=logging.WARNING)\n",
    "# ignite.utils.setup_logger(name=\"ignite.distributed.launcher.Parallel\", level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option():\n",
    "    n_epochs = 200   # 훈련할 에포크 수\n",
    "    batch_size = 9  # 배치의 크기\n",
    "    lr = 0.0002      # Adam 옵티마이저의 학습률\n",
    "    b1 = 0.5         # Adam 옵티마이저의 그래디언트의 일차 모멘텀 감쇠\n",
    "    b2 = 0.999       # Adam 옵티마이저의 그래디언트의 이차 모멘텀 감쇠\n",
    "    n_cpu = 16        # 배치 생성 중에 사용할 CPU 스레드 수\n",
    "    latent_dim = 150 # 잠재 공간의 차원\n",
    "    img_size = 512    # 각 이미지 차원의 크기\n",
    "    channels = 1     # 이미지 채널 수\n",
    "    sample_interval = 500  # 이미지 샘플링 간격\n",
    "\n",
    "opt = Option()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(opt.img_size),\n",
    "        transforms.CenterCrop(opt.img_size),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5)),\n",
    "    ]\n",
    ")\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset = dset.ImageFolder(root=\"../datasets/HighResolution/FLIR\", transform=data_transform)\n",
    "# test_dataset = torch.utils.data.Subset(train_dataset, torch.arange(3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=opt.batch_size, \n",
    "    num_workers=opt.n_cpu, \n",
    "    shuffle=True, \n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_dataset, \n",
    "#     batch_size=batch_size, \n",
    "#     num_workers=8, \n",
    "#     shuffle=False, \n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(train_dataloader))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:4], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()\n",
    "print(real_batch[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # 512 \n",
    "        self.init_size = opt.img_size // 256 # 2\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=opt.latent_dim, out_features=4096 * self.init_size ** 2),  # 512\n",
    "            )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(4096, 2048, 3, stride=1, padding=1),  \n",
    "            nn.InstanceNorm2d(2048, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # 4, 4\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(2048, 1024, 3, stride=1, padding=1), \n",
    "            nn.InstanceNorm2d(1024, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # 8\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(1024, 512, 3, stride=1, padding=1), \n",
    "            nn.InstanceNorm2d(512, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # 16\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, stride=1, padding=1), \n",
    "            nn.InstanceNorm2d(256, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # 32\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, stride=1, padding=1), \n",
    "            nn.InstanceNorm2d(128, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # 64\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1), \n",
    "            nn.InstanceNorm2d(64, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # 128\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(32, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # 256\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(32, 16, 3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(16, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # 512\n",
    "\n",
    "            nn.Conv2d(16, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 4096, self.init_size, self.init_size) # 4096, 2, 2 \n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netG = netG.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt.latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# summary(netG, input_size=(1, 1, opt.latent_dim), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.InstanceNorm2d(out_filters, True))\n",
    "            return block\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(opt.channels, 8, bn=False), # 256\n",
    "            *discriminator_block(8, 16),  # 128\n",
    "            *discriminator_block(16, 32),  # 64\n",
    "            *discriminator_block(32, 64), # 32\n",
    "            *discriminator_block(64, 128),# 16\n",
    "            *discriminator_block(128, 256), # 8\n",
    "            *discriminator_block(256, 512), # 4\n",
    "        )\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = opt.img_size // 2 ** 7 # 4\n",
    "        self.adv_layer = nn.Linear(512 * ds_size ** 2, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = Discriminator()\n",
    "netD = netD.to(device)\n",
    "# summary(netD, (1, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if device:\n",
    "    generator.to(device=device)\n",
    "    discriminator.to(device=device)\n",
    "    adversarial_loss.to(device=device)\n",
    "\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "Tensor = torch.cuda.FloatTensor if device else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(train_dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False).to(device)\n",
    "        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False).to(device)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor)).to(device)  \n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim)))).to(device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    print(\n",
    "        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "        % (epoch, opt.n_epochs, i, len(train_dataloader), d_loss.item(), g_loss.item())\n",
    "    )\n",
    "    # save_image(gen_imgs.data, \"img/FLIR_LSGAN_MF/%d.png\" % (epoch+1), nrow=5, normalize=True)\n",
    "    \n",
    "    # plt.figure(figsize = (10,10))\n",
    "    # img1 = cv2.imread(\"img/FLIR_LSGAN_MF/%d.png\" % (epoch+1))\n",
    "    # plt.imshow(img1, interpolation='nearest')\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
